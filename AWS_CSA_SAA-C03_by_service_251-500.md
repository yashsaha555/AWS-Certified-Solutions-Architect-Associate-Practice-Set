# AWS SAA-C03 Questions Grouped by AWS Service

## Table of Contents

- [AWS Backup](#aws-backup)
- [AWS Certificate Manager (ACM)](#aws-certificate-manager-acm)
- [AWS Config & CloudTrail](#aws-config-and-cloudtrail)
- [AWS DMS / SCT](#aws-dms-sct)
- [AWS Direct Connect & VPN](#aws-direct-connect-and-vpn)
- [AWS Global Accelerator](#aws-global-accelerator)
- [AWS Glue](#aws-glue)
- [AWS KMS](#aws-kms)
- [AWS Lake Formation](#aws-lake-formation)
- [AWS Lambda](#aws-lambda)
- [AWS Network Firewall](#aws-network-firewall)
- [AWS Organizations / IAM Identity Center (SSO)](#aws-organizations-iam-identity-center-sso)
- [AWS Secrets Manager](#aws-secrets-manager)
- [AWS Snow Family](#aws-snow-family)
- [AWS Systems Manager](#aws-systems-manager)
- [AWS WAF & Shield](#aws-waf-and-shield)
- [Amazon API Gateway](#amazon-api-gateway)
- [Amazon Athena](#amazon-athena)
- [Amazon Aurora](#amazon-aurora)
- [Amazon CloudFront](#amazon-cloudfront)
- [Amazon CloudWatch & EventBridge](#amazon-cloudwatch-and-eventbridge)
- [Amazon DynamoDB](#amazon-dynamodb)
- [Amazon EBS](#amazon-ebs)
- [Amazon EC2 / Auto Scaling](#amazon-ec2-auto-scaling)
- [Amazon EFS](#amazon-efs)
- [Amazon FSx](#amazon-fsx)
- [Amazon Kinesis](#amazon-kinesis)
- [Amazon Macie / Rekognition / Comprehend](#amazon-macie-rekognition-comprehend)
- [Amazon OpenSearch Service](#amazon-opensearch-service)
- [Amazon QuickSight](#amazon-quicksight)
- [Amazon RDS](#amazon-rds)
- [Amazon Redshift](#amazon-redshift)
- [Amazon Route 53](#amazon-route-53)
- [Amazon S3](#amazon-s3)
- [Amazon SNS](#amazon-sns)
- [Amazon SQS](#amazon-sqs)
- [Amazon VPC](#amazon-vpc)
- [Elastic Load Balancing (ALB/NLB/GWLB)](#elastic-load-balancing-albnlbgwlb)
- [General / Architecture](#general-architecture)


## AWS Backup

### Question #252

A solutions architect needs to design a system to store client case files. The files are core company assets and are important. The number of files will grow over time. The files must be simultaneously accessible from multiple application servers that run on Amazon EC2 instances. The solution must have built-in redundancy. Which solution meets these requirements?

- A. Amazon Elastic File System (Amazon EFS)

- B. Amazon Elastic Block Store (Amazon EBS)

- C. Amazon S3 Glacier Deep Archive

- D. AWS Backup

**Correct:** A
**Why:** Amazon EFS provides shared, redundant NFS storage accessible concurrently by multiple EC2 instances.

**Incorrect:**
- B: EBS is block storage attached to a single AZ/instance at a time.
- C: Glacier Deep Archive is archival, not shared storage.
- D: AWS Backup is for backups, not primary shared storage.


---

---

### Question #259

A company is implementing new data retention policies for all databases that run on Amazon RDS DB instances. The company must retain daily backups for a minimum period of 2 years. The backups must be consistent and restorable. Which solution should a solutions architect recommend to meet these requirements?

- A. Create a backup vault in AWS Backup to retain RDS backups. Create a new backup plan with a daily schedule and an expiration period of 2 years after creation. Assign the RDS DB instances to the backup plan.

- B. Configure a backup window for the RDS DB instances for daily snapshots. Assign a snapshot retention policy of 2 years to each RDS DB instance. Use Amazon Data Lifecycle Manager (Amazon DLM) to schedule snapshot deletions.

- C. Configure database transaction logs to be automatically backed up to Amazon CloudWatch Logs with an expiration period of 2 years.

- D. Configure an AWS Database Migration Service (AWS DMS) replication task. Deploy a replication instance, and configure a change data capture (CDC) task to stream database changes to Amazon S3 as the target. Configure S3 Lifecycle policies to delete the snapshots after 2 years.

**Correct:** A
**Why:** AWS Backup can enforce daily backups and retain for 2 years with minimal ops.

**Incorrect:**
- B: DLM is for EBS; RDS snapshots retention isn’t managed that way.
- C: CloudWatch logs/DMS exports are not consistent RDS backups.
- D: CloudWatch logs/DMS exports are not consistent RDS backups.


---

---

### Question #279

A company has an application that is backed by an Amazon DynamoDB table. The company’s compliance requirements specify that database backups must be taken every month, must be available for 6 months, and must be retained for 7 years. Which solution will meet these requirements?

- A. Create an AWS Backup plan to back up the DynamoDB table on the first day of each month. Specify a lifecycle policy that transitions the backup to cold storage after 6 months. Set the retention period for each backup to 7 years.

- B. Create a DynamoDB on-demand backup of the DynamoDB table on the first day of each month. Transition the backup to Amazon S3 Glacier Flexible Retrieval after 6 months. Create an S3 Lifecycle policy to delete backups that are older than 7 years.

- C. Use the AWS SDK to develop a script that creates an on-demand backup of the DynamoDB table. Set up an Amazon EventBridge rule that runs the script on the first day of each month. Create a second script that will run on the second day of each month to transition DynamoDB backups that are older than 6 months to cold storage and to delete backups that are older than 7 years.

- D. Use the AWS CLI to create an on-demand backup of the DynamoDB table. Set up an Amazon EventBridge rule that runs the command on the first day of each month with a cron expression. Specify in the command to transition the backups to cold storage after 6 months and to delete the backups after 7 years.

**Correct:** A
**Why:** AWS Backup monthly backup plan with lifecycle to cold storage after 6 months and 7‑year retention meets requirements.

**Incorrect:**
- B: Manual or partial solutions add ops and risk consistency.
- C: Manual or partial solutions add ops and risk consistency.
- D: Manual or partial solutions add ops and risk consistency.


---

---

### Question #312

A company has an application that runs on several Amazon EC2 instances. Each EC2 instance has multiple Amazon Elastic Block Store (Amazon EBS) data volumes attached to it. The application’s EC2 instance configuration and data need to be backed up nightly. The application also needs to be recoverable in a different AWS Region. Which solution will meet these requirements in the MOST operationally ecient way?

- A. Write an AWS Lambda function that schedules nightly snapshots of the application’s EBS volumes and copies the snapshots to a different Region.

- B. Create a backup plan by using AWS Backup to perform nightly backups. Copy the backups to another Region. Add the application’s EC2 instances as resources.

- C. Create a backup plan by using AWS Backup to perform nightly backups. Copy the backups to another Region. Add the application’s EBS volumes as resources.

- D. Write an AWS Lambda function that schedules nightly snapshots of the application's EBS volumes and copies the snapshots to a different Availability Zone.

**Correct:** B
**Why:** AWS Backup can back up EC2 instances (including EBS) nightly and copy to another Region with minimal ops.

**Incorrect:**
- A: DIY Lambdas or backing up only volumes miss instance config or add ops.
- C: DIY Lambdas or backing up only volumes miss instance config or add ops.
- D: DIY Lambdas or backing up only volumes miss instance config or add ops.


---

---

### Question #335

A company is experiencing sudden increases in demand. The company needs to provision large Amazon EC2 instances from an Amazon Machine Image (AMI). The instances will run in an Auto Scaling group. The company needs a solution that provides minimum initialization latency to meet the demand. Which solution meets these requirements?

- A. Use the aws ec2 register-image command to create an AMI from a snapshot. Use AWS Step Functions to replace the AMI in the Auto Scaling group.

- B. Enable Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot. Provision an AMI by using the snapshot. Replace the AMI in the Auto Scaling group with the new AMI.

- C. Enable AMI creation and dene lifecycle rules in Amazon Data Lifecycle Manager (Amazon DLM). Create an AWS Lambda function that modies the AMI in the Auto Scaling group.

- D. Use Amazon EventBridge to invoke AWS Backup lifecycle policies that provision AMIs. Configure Auto Scaling group capacity limits as an event source in EventBridge.

**Correct:** B
**Why:** Enable EBS Fast Snapshot Restore on the AMI’s snapshot to minimize initialization latency during scale‑out.

**Incorrect:**
- A: Register‑image/DLM/Backup events don’t directly reduce restore latency.
- C: Register‑image/DLM/Backup events don’t directly reduce restore latency.
- D: Register‑image/DLM/Backup events don’t directly reduce restore latency.


---

---

### Question #453

A company wants to implement a backup strategy for Amazon EC2 data and multiple Amazon S3 buckets. Because of regulatory requirements, the company must retain backup files for a specic time period. The company must not alter the files for the duration of the retention period. Which solution will meet these requirements?

- A. Use AWS Backup to create a backup vault that has a vault lock in governance mode. Create the required backup plan.

- B. Use Amazon Data Lifecycle Manager to create the required automated snapshot policy.

- C. Use Amazon S3 File Gateway to create the backup. Configure the appropriate S3 Lifecycle management.

- D. Use AWS Backup to create a backup vault that has a vault lock in compliance mode. Create the required backup plan.

**Correct:** D
**Why:** AWS Backup Vault Lock in compliance mode enforces WORM retention for EC2 and S3 backups to meet regulatory requirements.

**Incorrect:**
- A: Governance mode can be bypassed by privileged users.
- B: DLM and File Gateway do not provide WORM‑style immutability across all backups.
- C: DLM and File Gateway do not provide WORM‑style immutability across all backups.


---

---

### Question #456

A company runs applications on Amazon EC2 instances in one AWS Region. The company wants to back up the EC2 instances to a second Region. The company also wants to provision EC2 resources in the second Region and manage the EC2 instances centrally from one AWS account. Which solution will meet these requirements MOST cost-effectively?

- A. Create a disaster recovery (DR) plan that has a similar number of EC2 instances in the second Region. Configure data replication.

- B. Create point-in-time Amazon Elastic Block Store (Amazon EBS) snapshots of the EC2 instances. Copy the snapshots to the second Region periodically.

- C. Create a backup plan by using AWS Backup. Configure cross-Region backup to the second Region for the EC2 instances.

- D. Deploy a similar number of EC2 instances in the second Region. Use AWS DataSync to transfer the data from the source Region to the second Region.

**Correct:** C
**Why:** AWS Backup with cross‑Region backup centrally handles EC2/EBS backups to a second Region cost‑effectively.

**Incorrect:**
- A: Maintaining warm DR fleets or DataSync replication increases cost/ops.
- B: Manual EBS snapshot copying lacks centralized policy/automation.
- D: Maintaining warm DR fleets or DataSync replication increases cost/ops.


---

---

### Question #475

A company is designing a containerized application that will use Amazon Elastic Container Service (Amazon ECS). The application needs to access a shared file system that is highly durable and can recover data to another AWS Region with a recovery point objective (RPO) of 8 hours. The file system needs to provide a mount target m each Availability Zone within a Region. A solutions architect wants to use AWS Backup to manage the replication to another Region. Which solution will meet these requirements?

- A. Amazon FSx for Windows File Server with a Multi-AZ deployment

- B. Amazon FSx for NetApp ONTAP with a Multi-AZ deployment

- C. Amazon Elastic File System (Amazon EFS) with the Standard storage class

- D. Amazon FSx for OpenZFS

**Correct:** C
**Why:** EFS provides regional, multi‑AZ mount targets and integrates with AWS Backup for cross‑Region backups (RPO control).

**Incorrect:**
- A: FSx options have different characteristics and restore/RPO models.
- B: FSx options have different characteristics and restore/RPO models.
- D: FSx options have different characteristics and restore/RPO models.


---

## AWS Certificate Manager (ACM)

### Question #286

A company has a static website that is hosted on Amazon CloudFront in front of Amazon S3. The static website uses a database backend. The company notices that the website does not reect updates that have been made in the website’s Git repository. The company checks the continuous integration and continuous delivery (CI/CD) pipeline between the Git repository and Amazon S3. The company veries that the webhooks are congured properly and that the CI/CD pipeline is sending messages that indicate successful deployments. A solutions architect needs to implement a solution that displays the updates on the website. Which solution will meet these requirements?

- A. Add an Application Load Balancer.

- B. Add Amazon ElastiCache for Redis or Memcached to the database layer of the web application.

- C. Invalidate the CloudFront cache.

- D. Use AWS Certicate Manager (ACM) to validate the website’s SSL certicate.

**Correct:** C
**Why:** Invalidate the CloudFront cache so the CDN serves the latest content from S3.

**Incorrect:**
- A: ALB/ElastiCache/ACM do not address CDN cache staleness.
- B: ALB/ElastiCache/ACM do not address CDN cache staleness.
- D: ALB/ElastiCache/ACM do not address CDN cache staleness.


---

---

### Question #330

A company is planning to store data on Amazon RDS DB instances. The company must encrypt the data at rest. What should a solutions architect do to meet this requirement?

- A. Create a key in AWS Key Management Service (AWS KMS). Enable encryption for the DB instances.

- B. Create an encryption key. Store the key in AWS Secrets Manager. Use the key to encrypt the DB instances.

- C. Generate a certicate in AWS Certicate Manager (ACM). Enable SSL/TLS on the DB instances by using the certicate.

- D. Generate a certicate in AWS Identity and Access Management (IAM). Enable SSL/TLS on the DB instances by using the certicate.

**Correct:** A
**Why:** Use a KMS CMK and enable at‑rest encryption for RDS at creation/restore from encrypted snapshot.

**Incorrect:**
- B: Secrets Manager/ACM/IAM certs don’t encrypt DB storage at rest.
- C: Secrets Manager/ACM/IAM certs don’t encrypt DB storage at rest.
- D: Secrets Manager/ACM/IAM certs don’t encrypt DB storage at rest.


---

---

### Question #359

A hospital needs to store patient records in an Amazon S3 bucket. The hospital’s compliance team must ensure that all protected health information (PHI) is encrypted in transit and at rest. The compliance team must administer the encryption key for data at rest. Which solution will meet these requirements?

- A. Create a public SSL/TLS certicate in AWS Certicate Manager (ACM). Associate the certicate with Amazon S3. Configure default encryption for each S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS). Assign the compliance team to manage the KMS keys.

- B. Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Configure default encryption for each S3 bucket to use server-side encryption with S3 managed encryption keys (SSE-S3). Assign the compliance team to manage the SSE-S3 keys.

- C. Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Configure default encryption for each S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS). Assign the compliance team to manage the KMS keys.

- D. Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Use Amazon Macie to protect the sensitive data that is stored in Amazon S3. Assign the compliance team to manage Macie.

**Correct:** C
**Why:** Enforce TLS in transit with aws:SecureTransport on the S3 bucket policy; use SSE‑KMS so the compliance team administers the CMK for data at rest.

**Incorrect:**
- A: ACM public certs aren’t attached directly to S3; also KMS administration is needed, not SSE‑S3.
- B: SSE‑S3 keys are managed by AWS, not the compliance team.
- D: Macie discovers PII; it does not fulfill encryption requirements.


---

## AWS Config & CloudTrail

### Question #255

A company has an ecommerce checkout workow that writes an order to a database and calls a service to process the payment. Users are experiencing timeouts during the checkout process. When users resubmit the checkout form, multiple unique orders are created for the same desired transaction. How should a solutions architect refactor this workow to prevent the creation of multiple orders?

- A. Configure the web application to send an order message to Amazon Kinesis Data Firehose. Set the payment service to retrieve the message from Kinesis Data Firehose and process the order.

- B. Create a rule in AWS CloudTrail to invoke an AWS Lambda function based on the logged application path request. Use Lambda to query the database, call the payment service, and pass in the order information.

- C. Store the order in the database. Send a message that includes the order number to Amazon Simple Notification Service (Amazon SNS). Set the payment service to poll Amazon SNS, retrieve the message, and process the order.

- D. Store the order in the database. Send a message that includes the order number to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the payment service to retrieve the message and process the order. Delete the message from the queue.

**Correct:** D
**Why:** Use an SQS FIFO queue with content-based deduplication to ensure exactly-once processing and preserve ordering.

**Incorrect:**
- A: Kinesis/SNS do not provide FIFO message de‑duplication semantics for this flow.
- B: Kinesis/SNS do not provide FIFO message de‑duplication semantics for this flow.
- C: Kinesis/SNS do not provide FIFO message de‑duplication semantics for this flow.


---

---

### Question #309

A solutions architect needs to optimize storage costs. The solutions architect must identify any Amazon S3 buckets that are no longer being accessed or are rarely accessed. Which solution will accomplish this goal with the LEAST operational overhead?

- A. Analyze bucket access patterns by using the S3 Storage Lens dashboard for advanced activity metrics.

- B. Analyze bucket access patterns by using the S3 dashboard in the AWS Management Console.

- C. Turn on the Amazon CloudWatch BucketSizeBytes metric for buckets. Analyze bucket access patterns by using the metrics data with Amazon Athena.

- D. Turn on AWS CloudTrail for S3 object monitoring. Analyze bucket access patterns by using CloudTrail logs that are integrated with Amazon CloudWatch Logs.

**Correct:** A
**Why:** S3 Storage Lens advanced metrics identify buckets with low/rare access with least ops.

**Incorrect:**
- B: Console metrics, CloudWatch, or CloudTrail require more work and are less precise.
- C: Console metrics, CloudWatch, or CloudTrail require more work and are less precise.
- D: Console metrics, CloudWatch, or CloudTrail require more work and are less precise.


---

---

### Question #315

A company experienced a breach that affected several applications in its on-premises data center. The attacker took advantage of vulnerabilities in the custom applications that were running on the servers. The company is now migrating its applications to run on Amazon EC2 instances. The company wants to implement a solution that actively scans for vulnerabilities on the EC2 instances and sends a report that details the ndings. Which solution will meet these requirements?

- A. Deploy AWS Shield to scan the EC2 instances for vulnerabilities. Create an AWS Lambda function to log any ndings to AWS CloudTrail.

- B. Deploy Amazon Macie and AWS Lambda functions to scan the EC2 instances for vulnerabilities. Log any ndings to AWS CloudTrail.

- C. Turn on Amazon GuardDuty. Deploy the GuardDuty agents to the EC2 instances. Configure an AWS Lambda function to automate the generation and distribution of reports that detail the ndings.

- D. Turn on Amazon Inspector. Deploy the Amazon Inspector agent to the EC2 instances. Configure an AWS Lambda function to automate the generation and distribution of reports that detail the ndings.

**Correct:** D
**Why:** Amazon Inspector scans EC2 for vulnerabilities; use Lambda/automation to generate and distribute reports.

**Incorrect:**
- A: Shield/Macie/GuardDuty are not host vulnerability scanners.
- B: Shield/Macie/GuardDuty are not host vulnerability scanners.
- C: Shield/Macie/GuardDuty are not host vulnerability scanners.


---

---

### Question #318

A company recently migrated its entire IT environment to the AWS Cloud. The company discovers that users are provisioning oversized Amazon EC2 instances and modifying security group rules without using the appropriate change control process. A solutions architect must devise a strategy to track and audit these inventory and configuration changes. Which actions should the solutions architect take to meet these requirements? (Choose two.)

- A. Enable AWS CloudTrail and use it for auditing.

- B. Use data lifecycle policies for the Amazon EC2 instances.

- C. Enable AWS Trusted Advisor and reference the security dashboard.

- D. Enable AWS Cong and create rules for auditing and compliance purposes.

E. Restore previous resource congurations with an AWS CloudFormation template.

**Correct:** A, D
**Why:** CloudTrail audits API changes; AWS Config tracks resource configurations and can enforce rules.

**Incorrect:**
- B: DLM is for EBS; Trusted Advisor is advisory; CloudFormation restore doesn’t audit.
- C: DLM is for EBS; Trusted Advisor is advisory; CloudFormation restore doesn’t audit.
- E: DLM is for EBS; Trusted Advisor is advisory; CloudFormation restore doesn’t audit.


---

---

### Question #395

An IAM user made several configuration changes to AWS resources in their company's account during a production deployment last week. A solutions architect learned that a couple of security group rules are not congured as desired. The solutions architect wants to conrm which IAM user was responsible for making changes. Which service should the solutions architect use to nd the desired information?

- A. Amazon GuardDuty

- B. Amazon Inspector

- C. AWS CloudTrail

- D. AWS Cong

**Correct:** C
**Why:** AWS CloudTrail records API calls and identifies the IAM principal who made each change.

**Incorrect:**
- A: GuardDuty/Inspector/Config do not directly answer “who made the API change” as CloudTrail does.
- B: GuardDuty/Inspector/Config do not directly answer “who made the API change” as CloudTrail does.
- D: GuardDuty/Inspector/Config do not directly answer “who made the API change” as CloudTrail does.


---

---

### Question #426

A company needs to store data from its healthcare application. The application’s data frequently changes. A new regulation requires audit access at all levels of the stored data. The company hosts the application on an on-premises infrastructure that is running out of storage capacity. A solutions architect must securely migrate the existing data to AWS while satisfying the new regulation. Which solution will meet these requirements?

- A. Use AWS DataSync to move the existing data to Amazon S3. Use AWS CloudTrail to log data events.

- B. Use AWS Snowcone to move the existing data to Amazon S3. Use AWS CloudTrail to log management events.

- C. Use Amazon S3 Transfer Acceleration to move the existing data to Amazon S3. Use AWS CloudTrail to log data events.

- D. Use AWS Storage Gateway to move the existing data to Amazon S3. Use AWS CloudTrail to log management events.

**Correct:** A
**Why:** DataSync migrates data securely to S3; CloudTrail data events provide audit access to object‑level operations.

**Incorrect:**
- B: Snowcone is for small/edge; also management events alone are insufficient.
- C: Transfer Acceleration is for uploads from clients, not bulk secure migration with auditing.
- D: Storage Gateway is hybrid access, not best for migration plus data‑event auditing.


---

---

### Question #433

A company is running its production and nonproduction environment workloads in multiple AWS accounts. The accounts are in an organization in AWS Organizations. The company needs to design a solution that will prevent the modication of cost usage tags. Which solution will meet these requirements?

- A. Create a custom AWS Cong rule to prevent tag modication except by authorized principals.

- B. Create a custom trail in AWS CloudTrail to prevent tag modication.

- C. Create a service control policy (SCP) to prevent tag modication except by authorized principals.

- D. Create custom Amazon CloudWatch logs to prevent tag modication.

**Correct:** C
**Why:** Use an SCP in Organizations to centrally prevent tag modification except for authorized principals.

**Incorrect:**
- A: Config/CloudTrail/CloudWatch cannot enforce prevention like SCPs.
- B: Config/CloudTrail/CloudWatch cannot enforce prevention like SCPs.
- D: Config/CloudTrail/CloudWatch cannot enforce prevention like SCPs.


---

## AWS DMS / SCT

### Question #259

A company is implementing new data retention policies for all databases that run on Amazon RDS DB instances. The company must retain daily backups for a minimum period of 2 years. The backups must be consistent and restorable. Which solution should a solutions architect recommend to meet these requirements?

- A. Create a backup vault in AWS Backup to retain RDS backups. Create a new backup plan with a daily schedule and an expiration period of 2 years after creation. Assign the RDS DB instances to the backup plan.

- B. Configure a backup window for the RDS DB instances for daily snapshots. Assign a snapshot retention policy of 2 years to each RDS DB instance. Use Amazon Data Lifecycle Manager (Amazon DLM) to schedule snapshot deletions.

- C. Configure database transaction logs to be automatically backed up to Amazon CloudWatch Logs with an expiration period of 2 years.

- D. Configure an AWS Database Migration Service (AWS DMS) replication task. Deploy a replication instance, and configure a change data capture (CDC) task to stream database changes to Amazon S3 as the target. Configure S3 Lifecycle policies to delete the snapshots after 2 years.

**Correct:** A
**Why:** AWS Backup can enforce daily backups and retain for 2 years with minimal ops.

**Incorrect:**
- B: DLM is for EBS; RDS snapshots retention isn’t managed that way.
- C: CloudWatch logs/DMS exports are not consistent RDS backups.
- D: CloudWatch logs/DMS exports are not consistent RDS backups.


---

---

### Question #281

A company runs a eet of web servers using an Amazon RDS for PostgreSQL DB instance. After a routine compliance check, the company sets a standard that requires a recovery point objective (RPO) of less than 1 second for all its production databases. Which solution meets these requirements?

- A. Enable a Multi-AZ deployment for the DB instance.

- B. Enable auto scaling for the DB instance in one Availability Zone.

- C. Configure the DB instance in one Availability Zone, and create multiple read replicas in a separate Availability Zone.

- D. Configure the DB instance in one Availability Zone, and configure AWS Database Migration Service (AWS DMS) change data capture (CDC) tasks.

**Correct:** A
**Why:** RDS Multi‑AZ uses synchronous replication to a standby, providing near‑zero RPO (<1s) and automatic failover.

**Incorrect:**
- B: Do not meet the <1‑second RPO requirement.
- C: Do not meet the <1‑second RPO requirement.
- D: Do not meet the <1‑second RPO requirement.


---

---

### Question #292

A company is preparing a new data platform that will ingest real-time streaming data from multiple sources. The company needs to transform the data before writing the data to Amazon S3. The company needs the ability to use SQL to query the transformed data. Which solutions will meet these requirements? (Choose two.)

- A. Use Amazon Kinesis Data Streams to stream the data. Use Amazon Kinesis Data Analytics to transform the data. Use Amazon Kinesis Data Firehose to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.

- B. Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use AWS Glue to transform the data and to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.

- C. Use AWS Database Migration Service (AWS DMS) to ingest the data. Use Amazon EMR to transform the data and to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.

- D. Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use Amazon Kinesis Data Analytics to transform the data and to write the data to Amazon S3. Use the Amazon RDS query editor to query the transformed data from Amazon S3.

E. Use Amazon Kinesis Data Streams to stream the data. Use AWS Glue to transform the data. Use Amazon Kinesis Data Firehose to write the data to Amazon S3. Use the Amazon RDS query editor to query the transformed data from Amazon S3.

**Correct:** A, B
**Why:** Kinesis Data Streams + KDA + Firehose + S3 + Athena, or MSK + Glue + S3 + Athena both satisfy transform to S3 and SQL on data.

**Incorrect:**
- C: DMS/Incorrect query endpoints do not meet requirements fully.
- D: DMS/Incorrect query endpoints do not meet requirements fully.
- E: DMS/Incorrect query endpoints do not meet requirements fully.


---

---

### Question #304

A company recently created a disaster recovery site in a different AWS Region. The company needs to transfer large amounts of data back and forth between NFS file systems in the two Regions on a periodic basis. Which solution will meet these requirements with the LEAST operational overhead?

- A. Use AWS DataSync.

- B. Use AWS Snowball devices.

- C. Set up an SFTP server on Amazon EC2.

- D. Use AWS Database Migration Service (AWS DMS).

**Correct:** A
**Why:** DataSync moves data between NFS file systems across Regions with minimal ops and scheduling.

**Incorrect:**
- B: Snowball/SFTP/DMS don’t fit ongoing NFS replication.
- C: Snowball/SFTP/DMS don’t fit ongoing NFS replication.
- D: Snowball/SFTP/DMS don’t fit ongoing NFS replication.


---

---

### Question #334

A company wants to give a customer the ability to use on-premises Microsoft Active Directory to download files that are stored in Amazon S3. The customer’s application uses an SFTP client to download the files. Which solution will meet these requirements with the LEAST operational overhead and no changes to the customer’s application?

- A. Set up AWS Transfer Family with SFTP for Amazon S3. Configure integrated Active Directory authentication.

- B. Set up AWS Database Migration Service (AWS DMS) to synchronize the on-premises client with Amazon S3. Configure integrated Active Directory authentication.

- C. Set up AWS DataSync to synchronize between the on-premises location and the S3 location by using AWS IAM Identity Center (AWS Single Sign-On).

- D. Set up a Windows Amazon EC2 instance with SFTP to connect the on-premises client with Amazon S3. Integrate AWS Identity and Access Management (IAM).

**Correct:** A
**Why:** AWS Transfer Family SFTP for S3 with AD auth meets two‑way SFTP needs without app changes.

**Incorrect:**
- B: DMS/DataSync/IAM‑backed EC2 SFTP add ops or don’t integrate AD SFTP properly.
- C: DMS/DataSync/IAM‑backed EC2 SFTP add ops or don’t integrate AD SFTP properly.
- D: DMS/DataSync/IAM‑backed EC2 SFTP add ops or don’t integrate AD SFTP properly.


---

---

### Question #338

A solutions architect must create a disaster recovery (DR) plan for a high-volume software as a service (SaaS) platform. All data for the platform is stored in an Amazon Aurora MySQL DB cluster. The DR plan must replicate data to a secondary AWS Region. Which solution will meet these requirements MOST cost-effectively?

- A. Use MySQL binary log replication to an Aurora cluster in the secondary Region. Provision one DB instance for the Aurora cluster in the secondary Region.

- B. Set up an Aurora global database for the DB cluster. When setup is complete, remove the DB instance from the secondary Region.

- C. Use AWS Database Migration Service (AWS DMS) to continuously replicate data to an Aurora cluster in the secondary Region. Remove the DB instance from the secondary Region.

- D. Set up an Aurora global database for the DB cluster. Specify a minimum of one DB instance in the secondary Region.

**Correct:** D
**Why:** Aurora Global Database replicates data to a secondary Region with minimal lag; requires at least one instance there.

**Incorrect:**
- A: MySQL binlog/DMS add ops and cost without native Aurora benefits.
- B: Removing the instance breaks the global cluster.
- C: MySQL binlog/DMS add ops and cost without native Aurora benefits.


---

---

### Question #435

A company needs to migrate a MySQL database from its on-premises data center to AWS within 2 weeks. The database is 20 TB in size. The company wants to complete the migration with minimal downtime. Which solution will migrate the database MOST cost-effectively?

- A. Order an AWS Snowball Edge Storage Optimized device. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with replication of ongoing changes. Send the Snowball Edge device to AWS to nish the migration and continue the ongoing replication.

- B. Order an AWS Snowmobile vehicle. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with ongoing changes. Send the Snowmobile vehicle back to AWS to nish the migration and continue the ongoing replication.

- C. Order an AWS Snowball Edge Compute Optimized with GPU device. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with ongoing changes. Send the Snowball device to AWS to nish the migration and continue the ongoing replication

- D. Order a 1 GB dedicated AWS Direct Connect connection to establish a connection with the data center. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with replication of ongoing changes.

**Correct:** A
**Why:** Use Snowball Edge to seed 20 TB quickly and DMS for CDC to minimize downtime cost‑effectively.

**Incorrect:**
- B: Snowmobile is for exabyte‑scale.
- C: Compute Optimized with GPU is unnecessary.
- D: New DX is costly/time‑consuming and not needed for 20 TB in 2 weeks.


---

---

### Question #440

A company used an Amazon RDS for MySQL DB instance during application testing. Before terminating the DB instance at the end of the test cycle, a solutions architect created two backups. The solutions architect created the first backup by using the mysqldump utility to create a database dump. The solutions architect created the second backup by enabling the nal DB snapshot option on RDS termination. The company is now planning for a new test cycle and wants to create a new DB instance from the most recent backup. The company has chosen a MySQL-compatible edition ofAmazon Aurora to host the DB instance. Which solutions will create the new DB instance? (Choose two.)

- A. Import the RDS snapshot directly into Aurora.

- B. Upload the RDS snapshot to Amazon S3. Then import the RDS snapshot into Aurora.

- C. Upload the database dump to Amazon S3. Then import the database dump into Aurora.

- D. Use AWS Database Migration Service (AWS DMS) to import the RDS snapshot into Aurora.

E. Upload the database dump to Amazon S3. Then use AWS Database Migration Service (AWS DMS) to import the database dump into Aurora.

**Correct:** C, E
**Why:** Import from mysqldump in S3 to Aurora or use DMS to load from the dump; you cannot import an RDS snapshot directly into Aurora.

**Incorrect:**
- A: Snapshots are not directly importable to Aurora; DMS ingests data, not snapshots.
- B: Snapshots are not directly importable to Aurora; DMS ingests data, not snapshots.
- D: Snapshots are not directly importable to Aurora; DMS ingests data, not snapshots.


---

## AWS Direct Connect & VPN

### Question #294

An application that is hosted on Amazon EC2 instances needs to access an Amazon S3 bucket. Trac must not traverse the internet. How should a solutions architect configure access to meet these requirements?

- A. Create a private hosted zone by using Amazon Route 53.

- B. Set up a gateway VPC endpoint for Amazon S3 in the VPC.

- C. Configure the EC2 instances to use a NAT gateway to access the S3 bucket.

- D. Establish an AWS Site-to-Site VPN connection between the VPC and the S3 bucket.

**Correct:** B
**Why:** S3 gateway VPC endpoint keeps traffic private between EC2 and S3.

**Incorrect:**
- A: Private hosted zones/NAT/VPN are unnecessary for S3 private access.
- C: Private hosted zones/NAT/VPN are unnecessary for S3 private access.
- D: Private hosted zones/NAT/VPN are unnecessary for S3 private access.


---

---

### Question #313

A company is building a mobile app on AWS. The company wants to expand its reach to millions of users. The company needs to build a platform so that authorized users can watch the company’s content on their mobile devices. What should a solutions architect recommend to meet these requirements?

- A. Publish content to a public Amazon S3 bucket. Use AWS Key Management Service (AWS KMS) keys to stream content.

- B. Set up IPsec VPN between the mobile app and the AWS environment to stream content.

- C. Use Amazon CloudFront. Provide signed URLs to stream content.

- D. Set up AWS Client VPN between the mobile app and the AWS environment to stream content.

**Correct:** C
**Why:** CloudFront with signed URLs securely streams to authorized users at scale on mobile devices.

**Incorrect:**
- A: S3 public/KMS, VPN/Client VPN are not suitable for mass content distribution.
- B: S3 public/KMS, VPN/Client VPN are not suitable for mass content distribution.
- D: S3 public/KMS, VPN/Client VPN are not suitable for mass content distribution.


---

---

### Question #323

A company’s facility has badge readers at every entrance throughout the building. When badges are scanned, the readers send a message over HTTPS to indicate who attempted to access that particular entrance. A solutions architect must design a system to process these messages from the sensors. The solution must be highly available, and the results must be made available for the company’s security team to analyze. Which system architecture should the solutions architect recommend?

- A. Launch an Amazon EC2 instance to serve as the HTTPS endpoint and to process the messages. Configure the EC2 instance to save the results to an Amazon S3 bucket.

- B. Create an HTTPS endpoint in Amazon API Gateway. Configure the API Gateway endpoint to invoke an AWS Lambda function to process the messages and save the results to an Amazon DynamoDB table.

- C. Use Amazon Route 53 to direct incoming sensor messages to an AWS Lambda function. Configure the Lambda function to process the messages and save the results to an Amazon DynamoDB table.

- D. Create a gateway VPC endpoint for Amazon S3. Configure a Site-to-Site VPN connection from the facility network to the VPC so that sensor data can be written directly to an S3 bucket by way of the VPC endpoint.

**Correct:** B
**Why:** API Gateway HTTPS endpoint invoking Lambda, storing to DynamoDB, is highly available and serverless.

**Incorrect:**
- A: EC2 endpoint or direct S3 via VPN increases ops/complexity.
- C: Route 53 cannot directly invoke code.
- D: EC2 endpoint or direct S3 via VPN increases ops/complexity.


---

---

### Question #331

A company must migrate 20 TB of data from a data center to the AWS Cloud within 30 days. The company’s network bandwidth is limited to 15 Mbps and cannot exceed 70% utilization. What should a solutions architect do to meet these requirements?

- A. Use AWS Snowball.

- B. Use AWS DataSync.

- C. Use a secure VPN connection.

- D. Use Amazon S3 Transfer Acceleration.

**Correct:** A
**Why:** Snowball devices can move 20 TB within the time/bandwidth constraints cost‑effectively.

**Incorrect:**
- B: DataSync/VPN/Transfer Acceleration won’t meet time/bandwidth constraints.
- C: DataSync/VPN/Transfer Acceleration won’t meet time/bandwidth constraints.
- D: DataSync/VPN/Transfer Acceleration won’t meet time/bandwidth constraints.


---

---

### Question #332

A company needs to provide its employees with secure access to condential and sensitive files. The company wants to ensure that the files can be accessed only by authorized users. The files must be downloaded securely to the employees’ devices. The files are stored in an on-premises Windows file server. However, due to an increase in remote usage, the file server is running out of capacity. . Which solution will meet these requirements?

- A. Migrate the file server to an Amazon EC2 instance in a public subnet. Configure the security group to limit inbound trac to the employees’ IP addresses.

- B. Migrate the files to an Amazon FSx for Windows File Server file system. Integrate the Amazon FSx file system with the on-premises Active Directory. Configure AWS Client VPN.

- C. Migrate the files to Amazon S3, and create a private VPC endpoint. Create a signed URL to allow download.

- D. Migrate the files to Amazon S3, and create a public VPC endpoint. Allow employees to sign on with AWS IAM Identity Center (AWS Single Sign-On).

**Correct:** B
**Why:** FSx for Windows with on‑prem AD integration preserves permissions; Client VPN provides secure remote access and downloads.

**Incorrect:**
- A: Public EC2, S3 signed URLs, or public endpoints don’t integrate with AD or meet security requirements.
- C: Public EC2, S3 signed URLs, or public endpoints don’t integrate with AD or meet security requirements.
- D: Public EC2, S3 signed URLs, or public endpoints don’t integrate with AD or meet security requirements.


---

---

### Question #374

A company is running several business applications in three separate VPCs within the us-east-1 Region. The applications must be able to communicate between VPCs. The applications also must be able to consistently send hundreds of gigabytes of data each day to a latency- sensitive application that runs in a single on-premises data center. A solutions architect needs to design a network connectivity solution that maximizes cost-effectiveness. Which solution meets these requirements?

- A. Configure three AWS Site-to-Site VPN connections from the data center to AWS. Establish connectivity by conguring one VPN connection for each VPC.

- B. Launch a third-party virtual network appliance in each VPC. Establish an IPsec VPN tunnel between the data center and each virtual appliance.

- C. Set up three AWS Direct Connect connections from the data center to a Direct Connect gateway in us-east-1. Establish connectivity by conguring each VPC to use one of the Direct Connect connections.

- D. Set up one AWS Direct Connect connection from the data center to AWS. Create a transit gateway, and attach each VPC to the transit gateway. Establish connectivity between the Direct Connect connection and the transit gateway.

**Correct:** D
**Why:** A single Direct Connect with a transit gateway lets multiple VPCs share the link and provides cost‑effective, consistent, low‑latency connectivity to on‑premises.

**Incorrect:**
- A: Multiple VPNs won’t meet latency/throughput needs.
- B: Third‑party appliances add cost/ops and complexity.
- C: Separate DX per VPC is unnecessary and costly.


---

---

### Question #398

A company needs to transfer 600 TB of data from its on-premises network-attached storage (NAS) system to the AWS Cloud. The data transfer must be complete within 2 weeks. The data is sensitive and must be encrypted in transit. The company’s internet connection can support an upload speed of 100 Mbps. Which solution meets these requirements MOST cost-effectively?

- A. Use Amazon S3 multi-part upload functionality to transfer the files over HTTPS.

- B. Create a VPN connection between the on-premises NAS system and the nearest AWS Region. Transfer the data over the VPN connection.

- C. Use the AWS Snow Family console to order several AWS Snowball Edge Storage Optimized devices. Use the devices to transfer the data to Amazon S3.

- D. Set up a 10 Gbps AWS Direct Connect connection between the company location and the nearest AWS Region. Transfer the data over a VPN connection into the Region to store the data in Amazon S3.

**Correct:** C
**Why:** Snowball Edge devices securely transfer large datasets (600 TB) within the required 2 weeks with encryption in transit and at rest.

**Incorrect:**
- A: 100 Mbps is far too slow for 600 TB in 2 weeks.
- B: 100 Mbps is far too slow for 600 TB in 2 weeks.
- D: Provisioning DX for a one‑off transfer is costly/time‑consuming.


---

---

### Question #435

A company needs to migrate a MySQL database from its on-premises data center to AWS within 2 weeks. The database is 20 TB in size. The company wants to complete the migration with minimal downtime. Which solution will migrate the database MOST cost-effectively?

- A. Order an AWS Snowball Edge Storage Optimized device. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with replication of ongoing changes. Send the Snowball Edge device to AWS to nish the migration and continue the ongoing replication.

- B. Order an AWS Snowmobile vehicle. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with ongoing changes. Send the Snowmobile vehicle back to AWS to nish the migration and continue the ongoing replication.

- C. Order an AWS Snowball Edge Compute Optimized with GPU device. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with ongoing changes. Send the Snowball device to AWS to nish the migration and continue the ongoing replication

- D. Order a 1 GB dedicated AWS Direct Connect connection to establish a connection with the data center. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with replication of ongoing changes.

**Correct:** A
**Why:** Use Snowball Edge to seed 20 TB quickly and DMS for CDC to minimize downtime cost‑effectively.

**Incorrect:**
- B: Snowmobile is for exabyte‑scale.
- C: Compute Optimized with GPU is unnecessary.
- D: New DX is costly/time‑consuming and not needed for 20 TB in 2 weeks.


---

---

### Question #439

A solutions architect congured a VPC that has a small range of IP addresses. The number of Amazon EC2 instances that are in the VPC is increasing, and there is an insucient number of IP addresses for future workloads. Which solution resolves this issue with the LEAST operational overhead?

- A. Add an additional IPv4 CIDR block to increase the number of IP addresses and create additional subnets in the VPC. Create new resources in the new subnets by using the new CIDR.

- B. Create a second VPC with additional subnets. Use a peering connection to connect the second VPC with the first VPC Update the routes and create new resources in the subnets of the second VPC.

- C. Use AWS Transit Gateway to add a transit gateway and connect a second VPC with the first VPUpdate the routes of the transit gateway and VPCs. Create new resources in the subnets of the second VPC.

- D. Create a second VPC. Create a Site-to-Site VPN connection between the first VPC and the second VPC by using a VPN-hosted solution on Amazon EC2 and a virtual private gateway. Update the route between VPCs to the trac through the VPN. Create new resources in the subnets of the second VPC.

**Correct:** A
**Why:** Add an additional IPv4 CIDR to the VPC and create new subnets—simple and low‑ops.

**Incorrect:**
- B: Creating/connecting a second VPC adds complexity.
- C: Creating/connecting a second VPC adds complexity.
- D: Creating/connecting a second VPC adds complexity.


---

---

### Question #445

A company is storing 700 terabytes of data on a large network-attached storage (NAS) system in its corporate data center. The company has a hybrid environment with a 10 Gbps AWS Direct Connect connection. After an audit from a regulator, the company has 90 days to move the data to the cloud. The company needs to move the data eciently and without disruption. The company still needs to be able to access and update the data during the transfer window. Which solution will meet these requirements?

- A. Create an AWS DataSync agent in the corporate data center. Create a data transfer task Start the transfer to an Amazon S3 bucket.

- B. Back up the data to AWS Snowball Edge Storage Optimized devices. Ship the devices to an AWS data center. Mount a target Amazon S3 bucket on the on-premises file system.

- C. Use rsync to copy the data directly from local storage to a designated Amazon S3 bucket over the Direct Connect connection.

- D. Back up the data on tapes. Ship the tapes to an AWS data center. Mount a target Amazon S3 bucket on the on-premises file system.

**Correct:** A
**Why:** DataSync over DX moves large NAS datasets efficiently while allowing ongoing access and updates during the window.

**Incorrect:**
- B: Snow/tapes disrupt access during transfer and add logistics.
- C: rsync over DX for 700 TB is error‑prone and operationally heavy.
- D: Snow/tapes disrupt access during transfer and add logistics.


---

---

### Question #448

A company has two VPCs named Management and Production. The Management VPC uses VPNs through a customer gateway to connect to a single device in the data center. The Production VPC uses a virtual private gateway with two attached AWS Direct Connect connections. The Management and Production VPCs both use a single VPC peering connection to allow communication between the applications. What should a solutions architect do to mitigate any single point of failure in this architecture?

- A. Add a set of VPNs between the Management and Production VPCs.

- B. Add a second virtual private gateway and attach it to the Management VPC.

- C. Add a second set of VPNs to the Management VPC from a second customer gateway device.

- D. Add a second VPC peering connection between the Management VPC and the Production VPC.

**Correct:** C
**Why:** Add redundant customer gateway hardware and VPNs to remove the single on‑premises device SPOF for the Management VPC.

**Incorrect:**
- A: Peering redundancy doesn’t fix the on‑prem single device.
- B: A second virtual private gateway in Management VPC doesn’t add on‑prem redundancy.
- D: Peering redundancy doesn’t fix the on‑prem single device.


---

---

### Question #451

A company is migrating its applications and databases to the AWS Cloud. The company will use Amazon Elastic Container Service (Amazon ECS), AWS Direct Connect, and Amazon RDS. Which activities will be managed by the company's operational team? (Choose three.)

- A. Management of the Amazon RDS infrastructure layer, operating system, and platforms

- B. Creation of an Amazon RDS DB instance and conguring the scheduled maintenance window

- C. Configuration of additional software components on Amazon ECS for monitoring, patch management, log management, and host intrusion detection

- D. Installation of patches for all minor and major database versions for Amazon RDS

E. Ensure the physical security of the Amazon RDS infrastructure in the data center

F. Encryption of the data that moves in transit through Direct Connect

**Correct:** B, C, F
**Why:** Customers create RDS instances and set maintenance windows (B), manage ECS host/agent and add monitoring/IDS (C when using EC2 launch type), and must encrypt data in transit over Direct Connect (F).

**Incorrect:**
- A: AWS manages RDS infrastructure/OS/platform and physical security.
- D: RDS applies minor patches automatically during the window; customers schedule, but do not install “all patches” directly.
- E: AWS manages RDS infrastructure/OS/platform and physical security.


---

---

### Question #474

A company has multiple VPCs across AWS Regions to support and run workloads that are isolated from workloads in other Regions. Because of a recent application launch requirement, the company’s VPCs must communicate with all other VPCs across all Regions. Which solution will meet these requirements with the LEAST amount of administrative effort?

- A. Use VPC peering to manage VPC communication in a single Region. Use VPC peering across Regions to manage VPC communications.

- B. Use AWS Direct Connect gateways across all Regions to connect VPCs across regions and manage VPC communications.

- C. Use AWS Transit Gateway to manage VPC communication in a single Region and Transit Gateway peering across Regions to manage VPC communications.

- D. Use AWS PrivateLink across all Regions to connect VPCs across Regions and manage VPC communications

**Correct:** C
**Why:** Use Transit Gateway for intra‑Region VPC connectivity and TGW peering for cross‑Region mesh—least admin effort.

**Incorrect:**
- A: Peering, DX gateways, or PrivateLink are less scalable for any‑to‑any VPC connectivity.
- B: Peering, DX gateways, or PrivateLink are less scalable for any‑to‑any VPC connectivity.
- D: Peering, DX gateways, or PrivateLink are less scalable for any‑to‑any VPC connectivity.


---

---

### Question #482

A company wants to migrate 100 GB of historical data from an on-premises location to an Amazon S3 bucket. The company has a 100 megabits per second (Mbps) internet connection on premises. The company needs to encrypt the data in transit to the S3 bucket. The company will store new data directly in Amazon S3. Which solution will meet these requirements with the LEAST operational overhead?

- A. Use the s3 sync command in the AWS CLI to move the data directly to an S3 bucket

- B. Use AWS DataSync to migrate the data from the on-premises location to an S3 bucket

- C. Use AWS Snowball to move the data to an S3 bucket

- D. Set up an IPsec VPN from the on-premises location to AWS. Use the s3 cp command in the AWS CLI to move the data directly to an S3 bucket

**Correct:** B
**Why:** DataSync securely transfers and encrypts data in transit with minimal setup and monitoring.

**Incorrect:**
- A: CLI/VPN approaches are more manual.
- C: Snowball is overkill for 100 GB and adds logistics.
- D: CLI/VPN approaches are more manual.


---

---

### Question #487

A company seeks a storage solution for its application. The solution must be highly available and scalable. The solution also must function as a file system be mountable by multiple Linux instances in AWS and on premises through native protocols, and have no minimum size requirements. The company has set up a Site-to-Site VPN for access from its on-premises network to its VPC. Which storage solution meets these requirements?

- A. Amazon FSx Multi-AZ deployments

- B. Amazon Elastic Block Store (Amazon EBS) Multi-Attach volumes

- C. Amazon Elastic File System (Amazon EFS) with multiple mount targets

- D. Amazon Elastic File System (Amazon EFS) with a single mount target and multiple access points

**Correct:** C
**Why:** EFS is a scalable, highly available NFS file system with multiple mount targets; accessible from on‑prem via VPN.

**Incorrect:**
- A: FSx options/EBS don’t match NFS multi‑attach across many clients.
- B: FSx options/EBS don’t match NFS multi‑attach across many clients.
- D: FSx options/EBS don’t match NFS multi‑attach across many clients.


---

---

### Question #499

A company needs to minimize the cost of its 1 Gbps AWS Direct Connect connection. The company's average connection utilization is less than 10%. A solutions architect must recommend a solution that will reduce the cost without compromising security. Which solution will meet these requirements?

- A. Set up a new 1 Gbps Direct Connect connection. Share the connection with another AWS account.

- B. Set up a new 200 Mbps Direct Connect connection in the AWS Management Console.

- C. Contact an AWS Direct Connect Partner to order a 1 Gbps connection. Share the connection with another AWS account.

- D. Contact an AWS Direct Connect Partner to order a 200 Mbps hosted connection for an existing AWS account.

**Correct:** D
**Why:** Use an AWS Direct Connect Partner to procure a lower‑bandwidth (e.g., 200 Mbps) hosted connection to reduce cost securely.

**Incorrect:**
- A: Sharing a 1 Gbps connection doesn’t reduce your cost reliably.
- B: You cannot directly order 200 Mbps in the console without a partner.
- C: Sharing a 1 Gbps connection doesn’t reduce your cost reliably.


---

## AWS Global Accelerator

### Question #261

A company recently announced the deployment of its retail website to a global audience. The website runs on multiple Amazon EC2 instances behind an Elastic Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The company wants to provide its customers with different versions of content based on the devices that the customers use to access the website. Which combination of actions should a solutions architect take to meet these requirements? (Choose two.)

- A. Configure Amazon CloudFront to cache multiple versions of the content.

- B. Configure a host header in a Network Load Balancer to forward trac to different instances.

- C. Configure a Lambda@Edge function to send specic objects to users based on the User-Agent header.

- D. Configure AWS Global Accelerator. Forward requests to a Network Load Balancer (NLB). Configure the NLB to set up host-based routing to different EC2 instances.

E. Configure AWS Global Accelerator. Forward requests to a Network Load Balancer (NLB). Configure the NLB to set up path-based routing to different EC2 instances.

**Correct:** A, C
**Why:** Use CloudFront for global caching and Lambda@Edge to vary content based on User‑Agent header.

**Incorrect:**
- B: NLB host/path routing or Global Accelerator aren’t needed for device‑specific content at the edge.
- D: NLB host/path routing or Global Accelerator aren’t needed for device‑specific content at the edge.
- E: NLB host/path routing or Global Accelerator aren’t needed for device‑specific content at the edge.


---

---

### Question #266

A company has a popular gaming platform running on AWS. The application is sensitive to latency because latency can impact the user experience and introduce unfair advantages to some players. The application is deployed in every AWS Region. It runs on Amazon EC2 instances that are part of Auto Scaling groups congured behind Application Load Balancers (ALBs). A solutions architect needs to implement a mechanism to monitor the health of the application and redirect trac to healthy endpoints. Which solution meets these requirements?

- A. Configure an accelerator in AWS Global Accelerator. Add a listener for the port that the application listens on, and attach it to a Regional endpoint in each Region. Add the ALB as the endpoint.

- B. Create an Amazon CloudFront distribution and specify the ALB as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the trac.

- C. Create an Amazon CloudFront distribution and specify Amazon S3 as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the trac.

- D. Configure an Amazon DynamoDB database to serve as the data store for the application. Create a DynamoDB Accelerator (DAX) cluster to act as the in-memory cache for DynamoDB hosting the application data.

**Correct:** A
**Why:** Global Accelerator provides health checks and intelligent routing to the nearest healthy Regional endpoint (ALB).

**Incorrect:**
- B: CloudFront/S3 or DynamoDB/DAX don’t meet UDP/TCP app health redirect needs across Regions.
- C: CloudFront/S3 or DynamoDB/DAX don’t meet UDP/TCP app health redirect needs across Regions.
- D: CloudFront/S3 or DynamoDB/DAX don’t meet UDP/TCP app health redirect needs across Regions.


---

---

### Question #352

A company is designing the network for an online multi-player game. The game uses the UDP networking protocol and will be deployed in eight AWS Regions. The network architecture needs to minimize latency and packet loss to give end users a high-quality gaming experience. Which solution will meet these requirements?

- A. Setup a transit gateway in each Region. Create inter-Region peering attachments between each transit gateway.

- B. Set up AWS Global Accelerator with UDP listeners and endpoint groups in each Region.

- C. Set up Amazon CloudFront with UDP turned on. Configure an origin in each Region.

- D. Set up a VPC peering mesh between each Region. Turn on UDP for each VPC.

**Correct:** B
**Why:** AWS Global Accelerator supports UDP and provides static anycast IPs with global edge network to reduce latency and packet loss across Regions.

**Incorrect:**
- A: Transit gateways and peering do not optimize internet edge-to-Region latency for end users.
- C: CloudFront does not proxy UDP.
- D: VPC peering is for inter-VPC traffic; end users still traverse the internet without edge acceleration.


---

---

### Question #367

A company is using Amazon Route 53 latency-based routing to route requests to its UDP-based application for users around the world. The application is hosted on redundant servers in the company's on-premises data centers in the United States, Asia, and Europe. The company’s compliance requirements state that the application must be hosted on premises. The company wants to improve the performance and availability of the application. What should a solutions architect do to meet these requirements?

- A. Configure three Network Load Balancers (NLBs) in the three AWS Regions to address the on-premises endpoints. Create an accelerator by using AWS Global Accelerator, and register the NLBs as its endpoints. Provide access to the application by using a CNAME that points to the accelerator DNS.

- B. Configure three Application Load Balancers (ALBs) in the three AWS Regions to address the on-premises endpoints. Create an accelerator by using AWS Global Accelerator, and register the ALBs as its endpoints. Provide access to the application by using a CNAME that points to the accelerator DNS.

- C. Configure three Network Load Balancers (NLBs) in the three AWS Regions to address the on-premises endpoints. In Route 53, create a latency-based record that points to the three NLBs, and use it as an origin for an Amazon CloudFront distribution. Provide access to the application by using a CNAME that points to the CloudFront DNS.

- D. Configure three Application Load Balancers (ALBs) in the three AWS Regions to address the on-premises endpoints. In Route 53, create a latency-based record that points to the three ALBs, and use it as an origin for an Amazon CloudFront distribution. Provide access to the application by using a CNAME that points to the CloudFront DNS.

**Correct:** A
**Why:** Place NLBs in three Regions that point to on‑prem endpoints; use Global Accelerator to improve performance/availability for UDP traffic while keeping hosting on‑prem.

**Incorrect:**
- B: ALB does not support UDP.
- C: CloudFront is HTTP/HTTPS and not for UDP; also adds unnecessary layers.
- D: CloudFront is HTTP/HTTPS and not for UDP; also adds unnecessary layers.


---

---

### Question #396

A company has implemented a self-managed DNS service on AWS. The solution consists of the following: • Amazon EC2 instances in different AWS Regions • Endpoints of a standard accelerator in AWS Global Accelerator The company wants to protect the solution against DDoS attacks. What should a solutions architect do to meet this requirement?

- A. Subscribe to AWS Shield Advanced. Add the accelerator as a resource to protect.

- B. Subscribe to AWS Shield Advanced. Add the EC2 instances as resources to protect.

- C. Create an AWS WAF web ACL that includes a rate-based rule. Associate the web ACL with the accelerator.

- D. Create an AWS WAF web ACL that includes a rate-based rule. Associate the web ACL with the EC2 instances.

**Correct:** A
**Why:** Shield Advanced protects Global Accelerator resources against DDoS; add the accelerator as a protected resource.

**Incorrect:**
- B: Protecting individual EC2 instances misses the accelerator edge.
- C: AWS WAF rate rules do not mitigate volumetric DDoS at the network layer.
- D: AWS WAF rate rules do not mitigate volumetric DDoS at the network layer.


---

---

### Question #408

A company runs an application that receives data from thousands of geographically dispersed remote devices that use UDP. The application processes the data immediately and sends a message back to the device if necessary. No data is stored. The company needs a solution that minimizes latency for the data transmission from the devices. The solution also must provide rapid failover to another AWS Region. Which solution will meet these requirements?

- A. Configure an Amazon Route 53 failover routing policy. Create a Network Load Balancer (NLB) in each of the two Regions. Configure the NLB to invoke an AWS Lambda function to process the data.

- B. Use AWS Global Accelerator. Create a Network Load Balancer (NLB) in each of the two Regions as an endpoint. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the NLProcess the data in Amazon ECS.

- C. Use AWS Global Accelerator. Create an Application Load Balancer (ALB) in each of the two Regions as an endpoint. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the ALB. Process the data in Amazon ECS.

- D. Configure an Amazon Route 53 failover routing policy. Create an Application Load Balancer (ALB) in each of the two Regions. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the ALB. Process the data in Amazon ECS.

**Correct:** B
**Why:** Global Accelerator reduces latency globally and provides fast failover; NLB supports UDP; ECS Fargate scales to process data.

**Incorrect:**
- A: Route 53 failover alone doesn’t optimize latency.
- C: ALB does not support UDP.
- D: Same as A plus ALB (no UDP).


---

---

### Question #461

A company is developing a mobile gaming app in a single AWS Region. The app runs on multiple Amazon EC2 instances in an Auto Scaling group. The company stores the app data in Amazon DynamoDB. The app communicates by using TCP trac and UDP trac between the users and the servers. The application will be used globally. The company wants to ensure the lowest possible latency for all users. Which solution will meet these requirements?

- A. Use AWS Global Accelerator to create an accelerator. Create an Application Load Balancer (ALB) behind an accelerator endpoint that uses Global Accelerator integration and listening on the TCP and UDP ports. Update the Auto Scaling group to register instances on the ALB.

- B. Use AWS Global Accelerator to create an accelerator. Create a Network Load Balancer (NLB) behind an accelerator endpoint that uses Global Accelerator integration and listening on the TCP and UDP ports. Update the Auto Scaling group to register instances on the NLB.

- C. Create an Amazon CloudFront content delivery network (CDN) endpoint. Create a Network Load Balancer (NLB) behind the endpoint and listening on the TCP and UDP ports. Update the Auto Scaling group to register instances on the NLB. Update CloudFront to use the NLB as the origin.

- D. Create an Amazon CloudFront content delivery network (CDN) endpoint. Create an Application Load Balancer (ALB) behind the endpoint and listening on the TCP and UDP ports. Update the Auto Scaling group to register instances on the ALB. Update CloudFront to use the ALB as the origin.

**Correct:** B
**Why:** Global Accelerator with an NLB supports both TCP and UDP and provides the lowest latency globally via the AWS edge.

**Incorrect:**
- A: ALB doesn’t support UDP; CloudFront is not for arbitrary TCP/UDP.
- C: ALB doesn’t support UDP; CloudFront is not for arbitrary TCP/UDP.
- D: ALB doesn’t support UDP; CloudFront is not for arbitrary TCP/UDP.


---

## AWS Glue

### Question #258

A company has an application that places hundreds of .csv files into an Amazon S3 bucket every hour. The files are 1 GB in size. Each time a file is uploaded, the company needs to convert the file to Apache Parquet format and place the output file into an S3 bucket. Which solution will meet these requirements with the LEAST operational overhead?

- A. Create an AWS Lambda function to download the .csv files, convert the files to Parquet format, and place the output files in an S3 bucket. Invoke the Lambda function for each S3 PUT event.

- B. Create an Apache Spark job to read the .csv files, convert the files to Parquet format, and place the output files in an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the Spark job.

- C. Create an AWS Glue table and an AWS Glue crawler for the S3 bucket where the application places the .csv files. Schedule an AWS Lambda function to periodically use Amazon Athena to query the AWS Glue table, convert the query results into Parquet format, and place the output files into an S3 bucket.

- D. Create an AWS Glue extract, transform, and load (ETL) job to convert the .csv files to Parquet format and place the output files into an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the ETL job.

**Correct:** D
**Why:** AWS Glue ETL is managed and scalable; invoke job on S3 PUT via Lambda for timely conversion to Parquet.

**Incorrect:**
- A: Lambda alone is inefficient for 1‑GB files at scale.
- B: EMR or Athena‑based conversions add more ops/indirection.
- C: EMR or Athena‑based conversions add more ops/indirection.


---

---

### Question #280

A company is using Amazon CloudFront with its website. The company has enabled logging on the CloudFront distribution, and logs are saved in one of the company’s Amazon S3 buckets. The company needs to perform advanced analyses on the logs and build visualizations. What should a solutions architect do to meet these requirements?

- A. Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket. Visualize the results with AWS Glue.

- B. Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket. Visualize the results with Amazon QuickSight.

- C. Use standard SQL queries in Amazon DynamoDB to analyze the CloudFront logs in the S3 bucket. Visualize the results with AWS Glue.

- D. Use standard SQL queries in Amazon DynamoDB to analyze the CloudFront logs in the S3 bucket. Visualize the results with Amazon QuickSight.

**Correct:** B
**Why:** Use Athena to query CloudFront logs in S3 and QuickSight for visualizations.

**Incorrect:**
- A: Glue/DynamoDB are not needed for log analysis/visualization here.
- C: Glue/DynamoDB are not needed for log analysis/visualization here.
- D: Glue/DynamoDB are not needed for log analysis/visualization here.


---

---

### Question #292

A company is preparing a new data platform that will ingest real-time streaming data from multiple sources. The company needs to transform the data before writing the data to Amazon S3. The company needs the ability to use SQL to query the transformed data. Which solutions will meet these requirements? (Choose two.)

- A. Use Amazon Kinesis Data Streams to stream the data. Use Amazon Kinesis Data Analytics to transform the data. Use Amazon Kinesis Data Firehose to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.

- B. Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use AWS Glue to transform the data and to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.

- C. Use AWS Database Migration Service (AWS DMS) to ingest the data. Use Amazon EMR to transform the data and to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.

- D. Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use Amazon Kinesis Data Analytics to transform the data and to write the data to Amazon S3. Use the Amazon RDS query editor to query the transformed data from Amazon S3.

E. Use Amazon Kinesis Data Streams to stream the data. Use AWS Glue to transform the data. Use Amazon Kinesis Data Firehose to write the data to Amazon S3. Use the Amazon RDS query editor to query the transformed data from Amazon S3.

**Correct:** A, B
**Why:** Kinesis Data Streams + KDA + Firehose + S3 + Athena, or MSK + Glue + S3 + Athena both satisfy transform to S3 and SQL on data.

**Incorrect:**
- C: DMS/Incorrect query endpoints do not meet requirements fully.
- D: DMS/Incorrect query endpoints do not meet requirements fully.
- E: DMS/Incorrect query endpoints do not meet requirements fully.


---

---

### Question #317

A company uses a legacy application to produce data in CSV format. The legacy application stores the output data in Amazon S3. The company is deploying a new commercial off-the-shelf (COTS) application that can perform complex SQL queries to analyze data that is stored in Amazon Redshift and Amazon S3 only. However, the COTS application cannot process the .csv files that the legacy application produces. The company cannot update the legacy application to produce data in another format. The company needs to implement a solution so that the COTS application can use the data that the legacy application produces. Which solution will meet these requirements with the LEAST operational overhead?

- A. Create an AWS Glue extract, transform, and load (ETL) job that runs on a schedule. Configure the ETL job to process the .csv files and store the processed data in Amazon Redshift.

- B. Develop a Python script that runs on Amazon EC2 instances to convert the .csv files to .sql files. Invoke the Python script on a cron schedule to store the output files in Amazon S3.

- C. Create an AWS Lambda function and an Amazon DynamoDB table. Use an S3 event to invoke the Lambda function. Configure the Lambda function to perform an extract, transform, and load (ETL) job to process the .csv files and store the processed data in the DynamoDB table.

- D. Use Amazon EventBridge to launch an Amazon EMR cluster on a weekly schedule. Configure the EMR cluster to perform an extract, transform, and load (ETL) job to process the .csv files and store the processed data in an Amazon Redshift table.

**Correct:** A
**Why:** Glue ETL job on a schedule to process CSVs and load into Redshift with least ops.

**Incorrect:**
- B: EC2 scripts, DynamoDB, or EMR add ops for the use case.
- C: EC2 scripts, DynamoDB, or EMR add ops for the use case.
- D: EC2 scripts, DynamoDB, or EMR add ops for the use case.


---

---

### Question #341

A company has an Amazon S3 data lake that is governed by AWS Lake Formation. The company wants to create a visualization in Amazon QuickSight by joining the data in the data lake with operational data that is stored in an Amazon Aurora MySQL database. The company wants to enforce column-level authorization so that the company’s marketing team can access only a subset of columns in the database. Which solution will meet these requirements with the LEAST operational overhead?

- A. Use Amazon EMR to ingest the data directly from the database to the QuickSight SPICE engine. Include only the required columns.

- B. Use AWS Glue Studio to ingest the data from the database to the S3 data lake. Attach an IAM policy to the QuickSight users to enforce column-level access control. Use Amazon S3 as the data source in QuickSight.

- C. Use AWS Glue Elastic Views to create a materialized view for the database in Amazon S3. Create an S3 bucket policy to enforce column- level access control for the QuickSight users. Use Amazon S3 as the data source in QuickSight.

- D. Use a Lake Formation blueprint to ingest the data from the database to the S3 data lake. Use Lake Formation to enforce column-level access control for the QuickSight users. Use Amazon Athena as the data source in QuickSight.

**Correct:** D
**Why:** Lake Formation ingestion and column‑level permissions with Athena as the query engine integrate cleanly with QuickSight.

**Incorrect:**
- A: EMR/Glue Studio or Elastic Views/S3 policies don’t provide column‑level governance end‑to‑end as simply.
- B: EMR/Glue Studio or Elastic Views/S3 policies don’t provide column‑level governance end‑to‑end as simply.
- C: EMR/Glue Studio or Elastic Views/S3 policies don’t provide column‑level governance end‑to‑end as simply.


---

---

### Question #351

A company is moving its data management application to AWS. The company wants to transition to an event-driven architecture. The architecture needs to be more distributed and to use serverless concepts while performing the different aspects of the workow. The company also wants to minimize operational overhead. Which solution will meet these requirements?

- A. Build out the workow in AWS Glue. Use AWS Glue to invoke AWS Lambda functions to process the workow steps.

- B. Build out the workow in AWS Step Functions. Deploy the application on Amazon EC2 instances. Use Step Functions to invoke the workow steps on the EC2 instances.

- C. Build out the workow in Amazon EventBridge. Use EventBridge to invoke AWS Lambda functions on a schedule to process the workow steps.

- D. Build out the workow in AWS Step Functions. Use Step Functions to create a state machine. Use the state machine to invoke AWS Lambda functions to process the workow steps.

**Correct:** D
**Why:** Step Functions orchestrates serverless, event-driven workflows with a state machine that invokes Lambda for steps, minimizing ops and providing retries and branching.

**Incorrect:**
- A: AWS Glue is for ETL, not general workflow orchestration; invoking Lambda from Glue adds complexity.
- B: Orchestrating EC2 increases operational overhead and is not serverless.
- C: EventBridge schedules are time-based, not a workflow engine; you still need orchestration and step control.


---

---

### Question #375

An ecommerce company is building a distributed application that involves several serverless functions and AWS services to complete order- processing tasks. These tasks require manual approvals as part of the workow. A solutions architect needs to design an architecture for the order-processing application. The solution must be able to combine multiple AWS Lambda functions into responsive serverless applications. The solution also must orchestrate data and services that run on Amazon EC2 instances, containers, or on-premises servers. Which solution will meet these requirements with the LEAST operational overhead?

- A. Use AWS Step Functions to build the application.

- B. Integrate all the application components in an AWS Glue job.

- C. Use Amazon Simple Queue Service (Amazon SQS) to build the application.

- D. Use AWS Lambda functions and Amazon EventBridge events to build the application.

**Correct:** A
**Why:** Step Functions orchestrates serverless and non‑serverless tasks, supports human approval steps, and simplifies complex workflows.

**Incorrect:**
- B: AWS Glue is for data ETL, not general orchestration with approvals.
- C: SQS/EventBridge alone do not provide orchestration/state management.
- D: SQS/EventBridge alone do not provide orchestration/state management.


---

---

### Question #432

An ecommerce company wants to use machine learning (ML) algorithms to build and train models. The company will use the models to visualize complex scenarios and to detect trends in customer data. The architecture team wants to integrate its ML models with a reporting platform to analyze the augmented data and use the data directly in its business intelligence dashboards. Which solution will meet these requirements with the LEAST operational overhead?

- A. Use AWS Glue to create an ML transform to build and train models. Use Amazon OpenSearch Service to visualize the data.

- B. Use Amazon SageMaker to build and train models. Use Amazon QuickSight to visualize the data.

- C. Use a pre-built ML Amazon Machine Image (AMI) from the AWS Marketplace to build and train models. Use Amazon OpenSearch Service to visualize the data.

- D. Use Amazon QuickSight to build and train models by using calculated elds. Use Amazon QuickSight to visualize the data.

**Correct:** B
**Why:** SageMaker builds/trains models with low ops; QuickSight provides native visualization and dashboarding over augmented data.

**Incorrect:**
- A: Glue transforms/Marketplace AMIs/QuickSight alone are not the right ML training path with least ops.
- C: Glue transforms/Marketplace AMIs/QuickSight alone are not the right ML training path with least ops.
- D: Glue transforms/Marketplace AMIs/QuickSight alone are not the right ML training path with least ops.


---

---

### Question #463

An IoT company is releasing a mattress that has sensors to collect data about a user’s sleep. The sensors will send data to an Amazon S3 bucket. The sensors collect approximately 2 MB of data every night for each mattress. The company must process and summarize the data for each mattress. The results need to be available as soon as possible. Data processing will require 1 GB of memory and will nish within 30 seconds. Which solution will meet these requirements MOST cost-effectively?

- A. Use AWS Glue with a Scala job

- B. Use Amazon EMR with an Apache Spark script

- C. Use AWS Lambda with a Python script

- D. Use AWS Glue with a PySpark job

**Correct:** C
**Why:** Lambda with a Python function is cost‑effective for 30‑second, 1 GB jobs triggered on S3 object arrival.

**Incorrect:**
- A: Glue/EMR are heavier/more costly for this small batch.
- B: Glue/EMR are heavier/more costly for this small batch.
- D: Glue/EMR are heavier/more costly for this small batch.


---

## AWS KMS

### Question #253

A solutions architect has created two IAM policies: Policy1 and Policy2. Both policies are attached to an IAM group. A cloud engineer is added as an IAM user to the IAM group. The policy documents are:

Policy 1
{
	"Version": "2012-10-17",
	"Statement": [
		{
			"Effect": "Allow",
			"Action": [
				"iam:Get*",
				"iam:List*",
				"kms:List*",
				"ec2:*",
				"ds:*",
				"logs:Get*",
				"logs:Describe*"
			],
			"Resource": "*"
		}
	]
}

Policy 2
{
	"Version": "2012-10-17",
	"Statement": [
		{
			"Effect": "Deny",
			"Action": "ds:Delete*",
			"Resource": "*"
		}
	]
}

Which action will the cloud engineer be able to perform?

- A. Deleting IAM users

- B. Deleting directories

- C. Deleting Amazon EC2 instances

- D. Deleting logs from Amazon CloudWatch Logs

**Correct:** C
**Why:** Policy 1 explicitly allows ec2:* so terminating/deleting EC2 instances is allowed. Policy 2 adds an explicit Deny only for ds:Delete*, which overrides ds:* but does not affect EC2. Deleting IAM users and CloudWatch Logs are not covered by the allowed actions (only iam:Get*/List* and logs:Get*/Describe* are allowed).

**Incorrect:**
- A: Not allowed. Only read-only IAM permissions (iam:Get*/List*); DeleteUser requires iam:DeleteUser.
- B: Explicitly denied by Policy 2 (ds:Delete*), which overrides any Allow.
- D: Not allowed. Only logs:Get*/Describe* are allowed; deletions require logs:Delete* permissions.


---

---

### Question #256

A solutions architect is implementing a document review application using an Amazon S3 bucket for storage. The solution must prevent accidental deletion of the documents and ensure that all versions of the documents are available. Users must be able to download, modify, and upload documents. Which combination of actions should be taken to meet these requirements? (Choose two.)

- A. Enable a read-only bucket ACL.

- B. Enable versioning on the bucket.

- C. Attach an IAM policy to the bucket.

- D. Enable MFA Delete on the bucket.

E. Encrypt the bucket using AWS KMS.

**Correct:** B, D
**Why:** Versioning retains all versions; MFA Delete protects against accidental/unauthorized permanent deletions.

**Incorrect:**
- A: ACLs/policies/KMS do not ensure version retention or deletion protection by themselves.
- C: ACLs/policies/KMS do not ensure version retention or deletion protection by themselves.
- E: ACLs/policies/KMS do not ensure version retention or deletion protection by themselves.


---

---

### Question #270

A company is using a centralized AWS account to store log data in various Amazon S3 buckets. A solutions architect needs to ensure that the data is encrypted at rest before the data is uploaded to the S3 buckets. The data also must be encrypted in transit. Which solution meets these requirements?

- A. Use client-side encryption to encrypt the data that is being uploaded to the S3 buckets.

- B. Use server-side encryption to encrypt the data that is being uploaded to the S3 buckets.

- C. Create bucket policies that require the use of server-side encryption with S3 managed encryption keys (SSE-S3) for S3 uploads.

- D. Enable the security option to encrypt the S3 buckets through the use of a default AWS Key Management Service (AWS KMS) key.

**Correct:** C
**Why:** Bucket policies can require SSE (e.g., deny if x‑amz‑server‑side‑encryption is missing) to ensure encryption at rest; clients typically use HTTPS by default.

**Incorrect:**
- A: Client‑side encryption is heavier and not enforced server‑side.
- B: Do not enforce encryption upon upload by policy.
- D: Do not enforce encryption upon upload by policy.


---

---

### Question #313

A company is building a mobile app on AWS. The company wants to expand its reach to millions of users. The company needs to build a platform so that authorized users can watch the company’s content on their mobile devices. What should a solutions architect recommend to meet these requirements?

- A. Publish content to a public Amazon S3 bucket. Use AWS Key Management Service (AWS KMS) keys to stream content.

- B. Set up IPsec VPN between the mobile app and the AWS environment to stream content.

- C. Use Amazon CloudFront. Provide signed URLs to stream content.

- D. Set up AWS Client VPN between the mobile app and the AWS environment to stream content.

**Correct:** C
**Why:** CloudFront with signed URLs securely streams to authorized users at scale on mobile devices.

**Incorrect:**
- A: S3 public/KMS, VPN/Client VPN are not suitable for mass content distribution.
- B: S3 public/KMS, VPN/Client VPN are not suitable for mass content distribution.
- D: S3 public/KMS, VPN/Client VPN are not suitable for mass content distribution.


---

---

### Question #330

A company is planning to store data on Amazon RDS DB instances. The company must encrypt the data at rest. What should a solutions architect do to meet this requirement?

- A. Create a key in AWS Key Management Service (AWS KMS). Enable encryption for the DB instances.

- B. Create an encryption key. Store the key in AWS Secrets Manager. Use the key to encrypt the DB instances.

- C. Generate a certicate in AWS Certicate Manager (ACM). Enable SSL/TLS on the DB instances by using the certicate.

- D. Generate a certicate in AWS Identity and Access Management (IAM). Enable SSL/TLS on the DB instances by using the certicate.

**Correct:** A
**Why:** Use a KMS CMK and enable at‑rest encryption for RDS at creation/restore from encrypted snapshot.

**Incorrect:**
- B: Secrets Manager/ACM/IAM certs don’t encrypt DB storage at rest.
- C: Secrets Manager/ACM/IAM certs don’t encrypt DB storage at rest.
- D: Secrets Manager/ACM/IAM certs don’t encrypt DB storage at rest.


---

---

### Question #336

A company hosts a multi-tier web application that uses an Amazon Aurora MySQL DB cluster for storage. The application tier is hosted on Amazon EC2 instances. The company’s IT security guidelines mandate that the database credentials be encrypted and rotated every 14 days. What should a solutions architect do to meet this requirement with the LEAST operational effort?

- A. Create a new AWS Key Management Service (AWS KMS) encryption key. Use AWS Secrets Manager to create a new secret that uses the KMS key with the appropriate credentials. Associate the secret with the Aurora DB cluster. Configure a custom rotation period of 14 days.

- B. Create two parameters in AWS Systems Manager Parameter Store: one for the user name as a string parameter and one that uses the SecureString type for the password. Select AWS Key Management Service (AWS KMS) encryption for the password parameter, and load these parameters in the application tier. Implement an AWS Lambda function that rotates the password every 14 days.

- C. Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system in all EC2 instances of the application tier. Restrict the access to the file on the file system so that the application can read the file and that only super users can modify the file. Implement an AWS Lambda function that rotates the key in Aurora every 14 days and writes new credentials into the file.

- D. Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon S3 bucket that the application uses to load the credentials. Download the file to the application regularly to ensure that the correct credentials are used. Implement an AWS Lambda function that rotates the Aurora credentials every 14 days and uploads these credentials to the file in the S3 bucket.

**Correct:** A
**Why:** Secrets Manager with KMS‑encrypted secret and built‑in rotation integrates with Aurora; set 14‑day rotation.

**Incorrect:**
- B: Parameter Store/EFS/S3 solutions require more glue code and ops.
- C: Parameter Store/EFS/S3 solutions require more glue code and ops.
- D: Parameter Store/EFS/S3 solutions require more glue code and ops.


---

---

### Question #339

A company has a custom application with embedded credentials that retrieves information from an Amazon RDS MySQL DB instance. Management says the application must be made more secure with the least amount of programming effort. What should a solutions architect do to meet these requirements?

- A. Use AWS Key Management Service (AWS KMS) to create keys. Configure the application to load the database credentials from AWS KMS. Enable automatic key rotation.

- B. Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS Secrets Manager. Configure the application to load the database credentials from Secrets Manager. Create an AWS Lambda function that rotates the credentials in Secret Manager.

- C. Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS Secrets Manager. Configure the application to load the database credentials from Secrets Manager. Set up a credentials rotation schedule for the application user in the RDS for MySQL database using Secrets Manager.

- D. Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS Systems Manager Parameter Store. Configure the application to load the database credentials from Parameter Store. Set up a credentials rotation schedule for the application user in the RDS for MySQL database using Parameter Store.

**Correct:** C
**Why:** Store DB creds in Secrets Manager and enable automated rotation for RDS MySQL with minimal code changes.

**Incorrect:**
- A: KMS or Parameter Store lack integrated RDS rotation automation.
- B: Lambda rotation is not needed; use built‑in rotation.
- D: KMS or Parameter Store lack integrated RDS rotation automation.


---

---

### Question #349

A company stores condential data in an Amazon Aurora PostgreSQL database in the ap-southeast-3 Region. The database is encrypted with an AWS Key Management Service (AWS KMS) customer managed key. The company was recently acquired and must securely share a backup of the database with the acquiring company’s AWS account in ap-southeast-3. What should a solutions architect do to meet these requirements?

- A. Create a database snapshot. Copy the snapshot to a new unencrypted snapshot. Share the new snapshot with the acquiring company’s AWS account.

- B. Create a database snapshot. Add the acquiring company’s AWS account to the KMS key policy. Share the snapshot with the acquiring company’s AWS account.

- C. Create a database snapshot that uses a different AWS managed KMS key. Add the acquiring company’s AWS account to the KMS key alias. Share the snapshot with the acquiring company's AWS account.

- D. Create a database snapshot. Download the database snapshot. Upload the database snapshot to an Amazon S3 bucket. Update the S3 bucket policy to allow access from the acquiring company’s AWS account.

**Correct:** B
**Why:** Share encrypted snapshot by adding the other account to the CMK key policy and sharing the snapshot.

**Incorrect:**
- A: Unencrypted copies, different KMS aliases, or S3 export are unnecessary/less secure.
- C: Unencrypted copies, different KMS aliases, or S3 export are unnecessary/less secure.
- D: Unencrypted copies, different KMS aliases, or S3 export are unnecessary/less secure.


---

---

### Question #359

A hospital needs to store patient records in an Amazon S3 bucket. The hospital’s compliance team must ensure that all protected health information (PHI) is encrypted in transit and at rest. The compliance team must administer the encryption key for data at rest. Which solution will meet these requirements?

- A. Create a public SSL/TLS certicate in AWS Certicate Manager (ACM). Associate the certicate with Amazon S3. Configure default encryption for each S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS). Assign the compliance team to manage the KMS keys.

- B. Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Configure default encryption for each S3 bucket to use server-side encryption with S3 managed encryption keys (SSE-S3). Assign the compliance team to manage the SSE-S3 keys.

- C. Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Configure default encryption for each S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS). Assign the compliance team to manage the KMS keys.

- D. Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Use Amazon Macie to protect the sensitive data that is stored in Amazon S3. Assign the compliance team to manage Macie.

**Correct:** C
**Why:** Enforce TLS in transit with aws:SecureTransport on the S3 bucket policy; use SSE‑KMS so the compliance team administers the CMK for data at rest.

**Incorrect:**
- A: ACM public certs aren’t attached directly to S3; also KMS administration is needed, not SSE‑S3.
- B: SSE‑S3 keys are managed by AWS, not the compliance team.
- D: Macie discovers PII; it does not fulfill encryption requirements.


---

---

### Question #364

A hospital is designing a new application that gathers symptoms from patients. The hospital has decided to use Amazon Simple Queue Service (Amazon SQS) and Amazon Simple Notification Service (Amazon SNS) in the architecture. A solutions architect is reviewing the infrastructure design. Data must be encrypted at rest and in transit. Only authorized personnel of the hospital should be able to access the data. Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)

- A. Turn on server-side encryption on the SQS components. Update the default key policy to restrict key usage to a set of authorized principals.

- B. Turn on server-side encryption on the SNS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply a key policy to restrict key usage to a set of authorized principals.

- C. Turn on encryption on the SNS components. Update the default key policy to restrict key usage to a set of authorized principals. Set a condition in the topic policy to allow only encrypted connections over TLS.

- D. Turn on server-side encryption on the SQS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply a key policy to restrict key usage to a set of authorized principals. Set a condition in the queue policy to allow only encrypted connections over TLS.

E. Turn on server-side encryption on the SQS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply an IAM policy to restrict key usage to a set of authorized principals. Set a condition in the queue policy to allow only encrypted connections over TLS.

**Correct:** B, D
**Why:** Use KMS CMKs for server‑side encryption on SNS and SQS; restrict key usage to authorized principals and enforce TLS in topic/queue policies.

**Incorrect:**
- A: Missing CMK use where required or rely on default key policy/IAM policy inappropriately.
- C: Missing CMK use where required or rely on default key policy/IAM policy inappropriately.
- E: Missing CMK use where required or rely on default key policy/IAM policy inappropriately.


---

---

### Question #371

A company needs to create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster to host a digital media streaming application. The EKS cluster will use a managed node group that is backed by Amazon Elastic Block Store (Amazon EBS) volumes for storage. The company must encrypt all data at rest by using a customer managed key that is stored in AWS Key Management Service (AWS KMS). Which combination of actions will meet this requirement with the LEAST operational overhead? (Choose two.)

- A. Use a Kubernetes plugin that uses the customer managed key to perform data encryption.

- B. After creation of the EKS cluster, locate the EBS volumes. Enable encryption by using the customer managed key.

- C. Enable EBS encryption by default in the AWS Region where the EKS cluster will be created. Select the customer managed key as the default key.

- D. Create the EKS cluster. Create an IAM role that has a policy that grants permission to the customer managed key. Associate the role with the EKS cluster.

E. Store the customer managed key as a Kubernetes secret in the EKS cluster. Use the customer managed key to encrypt the EBS volumes.

**Correct:** C, D
**Why:** Turn on EBS encryption by default in the Region and select the customer‑managed KMS key; ensure the cluster has permissions to use the CMK.

**Incorrect:**
- A: A plugin is unnecessary for EBS encryption at rest.
- B: You cannot enable encryption on existing EBS volumes post‑creation.
- E: Do not store CMKs as Kubernetes secrets; use KMS.


---

---

### Question #382

A company has a three-tier application on AWS that ingests sensor data from its users’ devices. The trac ows through a Network Load Balancer (NLB), then to Amazon EC2 instances for the web tier, and nally to EC2 instances for the application tier. The application tier makes calls to a database. What should a solutions architect do to improve the security of the data in transit?

- A. Configure a TLS listener. Deploy the server certicate on the NLB.

- B. Configure AWS Shield Advanced. Enable AWS WAF on the NLB.

- C. Change the load balancer to an Application Load Balancer (ALB). Enable AWS WAF on the ALB.

- D. Encrypt the Amazon Elastic Block Store (Amazon EBS) volume on the EC2 instances by using AWS Key Management Service (AWS KMS).

**Correct:** A
**Why:** Add a TLS listener and deploy certs on the NLB to encrypt data in transit from clients to the load balancer and to targets as needed.

**Incorrect:**
- B: Shield/WAF protect against attacks but don’t encrypt traffic.
- C: ALB is fine for HTTP/HTTPS but switching is unnecessary; TLS on NLB meets the need.
- D: EBS encryption is at rest, not in transit.


---

---

### Question #410

A company is deploying a new application on Amazon EC2 instances. The application writes data to Amazon Elastic Block Store (Amazon EBS) volumes. The company needs to ensure that all data that is written to the EBS volumes is encrypted at rest. Which solution will meet this requirement?

- A. Create an IAM role that species EBS encryption. Attach the role to the EC2 instances.

- B. Create the EBS volumes as encrypted volumes. Attach the EBS volumes to the EC2 instances.

- C. Create an EC2 instance tag that has a key of Encrypt and a value of True. Tag all instances that require encryption at the EBS level.

- D. Create an AWS Key Management Service (AWS KMS) key policy that enforces EBS encryption in the account. Ensure that the key policy is active.

**Correct:** B
**Why:** Create and attach encrypted EBS volumes (or enable EBS encryption by default) to ensure at-rest encryption.

**Incorrect:**
- A: Tags/roles/key policies do not enforce encryption by themselves.
- C: Tags/roles/key policies do not enforce encryption by themselves.
- D: Tags/roles/key policies do not enforce encryption by themselves.


---

---

### Question #438

A company wants to share accounting data with an external auditor. The data is stored in an Amazon RDS DB instance that resides in a private subnet. The auditor has its own AWS account and requires its own copy of the database. What is the MOST secure way for the company to share the database with the auditor?

- A. Create a read replica of the database. Configure IAM standard database authentication to grant the auditor access.

- B. Export the database contents to text files. Store the files in an Amazon S3 bucket. Create a new IAM user for the auditor. Grant the user access to the S3 bucket.

- C. Copy a snapshot of the database to an Amazon S3 bucket. Create an IAM user. Share the user's keys with the auditor to grant access to the object in the S3 bucket.

- D. Create an encrypted snapshot of the database. Share the snapshot with the auditor. Allow access to the AWS Key Management Service (AWS KMS) encryption key.

**Correct:** D
**Why:** Share an encrypted snapshot with the auditor and permit access to the KMS key; the auditor restores into its own account.

**Incorrect:**
- A: Read replica does not cross accounts easily and IAM DB auth is separate.
- B: Text export or snapshot in S3 is less secure/more work.
- C: Text export or snapshot in S3 is less secure/more work.


---

---

### Question #460

A company wants to securely exchange data between its software as a service (SaaS) application Salesforce account and Amazon S3. The company must encrypt the data at rest by using AWS Key Management Service (AWS KMS) customer managed keys (CMKs). The company must also encrypt the data in transit. The company has enabled API access for the Salesforce account.

- A. Create AWS Lambda functions to transfer the data securely from Salesforce to Amazon S3.

- B. Create an AWS Step Functions workow. Dene the task to transfer the data securely from Salesforce to Amazon S3.

- C. Create Amazon AppFlow ows to transfer the data securely from Salesforce to Amazon S3.

- D. Create a custom connector for Salesforce to transfer the data securely from Salesforce to Amazon S3.

**Correct:** C
**Why:** Amazon AppFlow has a native Salesforce connector with TLS in transit and SSE‑KMS at rest to S3 with CMKs.

**Incorrect:**
- A: More custom work; AppFlow is purpose‑built for this.
- B: More custom work; AppFlow is purpose‑built for this.
- D: More custom work; AppFlow is purpose‑built for this.


---

---

### Question #480

A business application is hosted on Amazon EC2 and uses Amazon S3 for encrypted object storage. The chief information security ocer has directed that no application trac between the two services should traverse the public internet. Which capability should the solutions architect use to meet the compliance requirements?

- A. AWS Key Management Service (AWS KMS)

- B. VPC endpoint

- C. Private subnet

- D. Virtual private gateway

**Correct:** B
**Why:** A VPC gateway endpoint for S3 keeps traffic on the AWS network, not the public internet.

**Incorrect:**
- A: KMS is for encryption, not network path.
- C: Subnet type/VPN don’t guarantee private S3 access.
- D: Subnet type/VPN don’t guarantee private S3 access.


---

---

### Question #491

A solutions architect is designing an asynchronous application to process credit card data validation requests for a bank. The application must be secure and be able to process each request at least once. Which solution will meet these requirements MOST cost-effectively?

- A. Use AWS Lambda event source mapping. Set Amazon Simple Queue Service (Amazon SQS) standard queues as the event source. Use AWS Key Management Service (SSE-KMS) for encryption. Add the kms:Decrypt permission for the Lambda execution role.

- B. Use AWS Lambda event source mapping. Use Amazon Simple Queue Service (Amazon SQS) FIFO queues as the event source. Use SQS managed encryption keys (SSE-SQS) for encryption. Add the encryption key invocation permission for the Lambda function.

- C. Use the AWS Lambda event source mapping. Set Amazon Simple Queue Service (Amazon SQS) FIFO queues as the event source. Use AWS KMS keys (SSE-KMS). Add the kms:Decrypt permission for the Lambda execution role.

- D. Use the AWS Lambda event source mapping. Set Amazon Simple Queue Service (Amazon SQS) standard queues as the event source. Use AWS KMS keys (SSE-KMS) for encryption. Add the encryption key invocation permission for the Lambda function.

**Correct:** A
**Why:** SQS standard queues provide at‑least‑once delivery; encrypt with SSE‑KMS and grant Lambda kms:Decrypt.

**Incorrect:**
- B: FIFO is not required and may cost more; managed keys choice varies.
- C: FIFO is not required and may cost more; managed keys choice varies.
- D: Similar to A but misstates key permissions pattern.


---

## AWS Lake Formation

### Question #341

A company has an Amazon S3 data lake that is governed by AWS Lake Formation. The company wants to create a visualization in Amazon QuickSight by joining the data in the data lake with operational data that is stored in an Amazon Aurora MySQL database. The company wants to enforce column-level authorization so that the company’s marketing team can access only a subset of columns in the database. Which solution will meet these requirements with the LEAST operational overhead?

- A. Use Amazon EMR to ingest the data directly from the database to the QuickSight SPICE engine. Include only the required columns.

- B. Use AWS Glue Studio to ingest the data from the database to the S3 data lake. Attach an IAM policy to the QuickSight users to enforce column-level access control. Use Amazon S3 as the data source in QuickSight.

- C. Use AWS Glue Elastic Views to create a materialized view for the database in Amazon S3. Create an S3 bucket policy to enforce column- level access control for the QuickSight users. Use Amazon S3 as the data source in QuickSight.

- D. Use a Lake Formation blueprint to ingest the data from the database to the S3 data lake. Use Lake Formation to enforce column-level access control for the QuickSight users. Use Amazon Athena as the data source in QuickSight.

**Correct:** D
**Why:** Lake Formation ingestion and column‑level permissions with Athena as the query engine integrate cleanly with QuickSight.

**Incorrect:**
- A: EMR/Glue Studio or Elastic Views/S3 policies don’t provide column‑level governance end‑to‑end as simply.
- B: EMR/Glue Studio or Elastic Views/S3 policies don’t provide column‑level governance end‑to‑end as simply.
- C: EMR/Glue Studio or Elastic Views/S3 policies don’t provide column‑level governance end‑to‑end as simply.


---

---

### Question #442

A company stores several petabytes of data across multiple AWS accounts. The company uses AWS Lake Formation to manage its data lake. The company's data science team wants to securely share selective data from its accounts with the company's engineering team for analytical purposes. Which solution will meet these requirements with the LEAST operational overhead?

- A. Copy the required data to a common account. Create an IAM access role in that account. Grant access by specifying a permission policy that includes users from the engineering team accounts as trusted entities.

- B. Use the Lake Formation permissions Grant command in each account where the data is stored to allow the required engineering team users to access the data.

- C. Use AWS Data Exchange to privately publish the required data to the required engineering team accounts.

- D. Use Lake Formation tag-based access control to authorize and grant cross-account permissions for the required data to the engineering team accounts.

**Correct:** D
**Why:** Lake Formation tag‑based access control simplifies cross‑account selective sharing with low operational overhead.

**Incorrect:**
- A: More manual copying/grants or different services add overhead.
- B: More manual copying/grants or different services add overhead.
- C: More manual copying/grants or different services add overhead.


---

---

### Question #495

A company is conducting an internal audit. The company wants to ensure that the data in an Amazon S3 bucket that is associated with the company’s AWS Lake Formation data lake does not contain sensitive customer or employee data. The company wants to discover personally identiable information (PII) or nancial information, including passport numbers and credit card numbers. Which solution will meet these requirements?

- A. Configure AWS Audit Manager on the account. Select the Payment Card Industry Data Security Standards (PCI DSS) for auditing.

- B. Configure Amazon S3 Inventory on the S3 bucket Configure Amazon Athena to query the inventory.

- C. Configure Amazon Macie to run a data discovery job that uses managed identiers for the required data types.

- D. Use Amazon S3 Select to run a report across the S3 bucket.

**Correct:** C
**Why:** Amazon Macie can scan S3 for PII/financial data with managed identifiers and produce findings.

**Incorrect:**
- A: Audit Manager/S3 Inventory/S3 Select do not detect PII patterns as Macie does.
- B: Audit Manager/S3 Inventory/S3 Select do not detect PII patterns as Macie does.
- D: Audit Manager/S3 Inventory/S3 Select do not detect PII patterns as Macie does.


---

## AWS Lambda

### Question #255

A company has an ecommerce checkout workow that writes an order to a database and calls a service to process the payment. Users are experiencing timeouts during the checkout process. When users resubmit the checkout form, multiple unique orders are created for the same desired transaction. How should a solutions architect refactor this workow to prevent the creation of multiple orders?

- A. Configure the web application to send an order message to Amazon Kinesis Data Firehose. Set the payment service to retrieve the message from Kinesis Data Firehose and process the order.

- B. Create a rule in AWS CloudTrail to invoke an AWS Lambda function based on the logged application path request. Use Lambda to query the database, call the payment service, and pass in the order information.

- C. Store the order in the database. Send a message that includes the order number to Amazon Simple Notification Service (Amazon SNS). Set the payment service to poll Amazon SNS, retrieve the message, and process the order.

- D. Store the order in the database. Send a message that includes the order number to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the payment service to retrieve the message and process the order. Delete the message from the queue.

**Correct:** D
**Why:** Use an SQS FIFO queue with content-based deduplication to ensure exactly-once processing and preserve ordering.

**Incorrect:**
- A: Kinesis/SNS do not provide FIFO message de‑duplication semantics for this flow.
- B: Kinesis/SNS do not provide FIFO message de‑duplication semantics for this flow.
- C: Kinesis/SNS do not provide FIFO message de‑duplication semantics for this flow.


---

---

### Question #257

A company is building a solution that will report Amazon EC2 Auto Scaling events across all the applications in an AWS account. The company needs to use a serverless solution to store the EC2 Auto Scaling status data in Amazon S3. The company then will use the data in Amazon S3 to provide near-real-time updates in a dashboard. The solution must not affect the speed of EC2 instance launches. How should the company move the data to Amazon S3 to meet these requirements?

- A. Use an Amazon CloudWatch metric stream to send the EC2 Auto Scaling status data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.

- B. Launch an Amazon EMR cluster to collect the EC2 Auto Scaling status data and send the data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.

- C. Create an Amazon EventBridge rule to invoke an AWS Lambda function on a schedule. Configure the Lambda function to send the EC2 Auto Scaling status data directly to Amazon S3.

- D. Use a bootstrap script during the launch of an EC2 instance to install Amazon Kinesis Agent. Configure Kinesis Agent to collect the EC2 Auto Scaling status data and send the data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.

**Correct:** A
**Why:** CloudWatch metric streams → Kinesis Data Firehose → S3 is serverless and near real time without impacting launches.

**Incorrect:**
- B: EMR/Kinesis Agent add unnecessary infrastructure.
- C: Scheduled polling adds latency and complexity.
- D: EMR/Kinesis Agent add unnecessary infrastructure.


---

---

### Question #258

A company has an application that places hundreds of .csv files into an Amazon S3 bucket every hour. The files are 1 GB in size. Each time a file is uploaded, the company needs to convert the file to Apache Parquet format and place the output file into an S3 bucket. Which solution will meet these requirements with the LEAST operational overhead?

- A. Create an AWS Lambda function to download the .csv files, convert the files to Parquet format, and place the output files in an S3 bucket. Invoke the Lambda function for each S3 PUT event.

- B. Create an Apache Spark job to read the .csv files, convert the files to Parquet format, and place the output files in an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the Spark job.

- C. Create an AWS Glue table and an AWS Glue crawler for the S3 bucket where the application places the .csv files. Schedule an AWS Lambda function to periodically use Amazon Athena to query the AWS Glue table, convert the query results into Parquet format, and place the output files into an S3 bucket.

- D. Create an AWS Glue extract, transform, and load (ETL) job to convert the .csv files to Parquet format and place the output files into an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the ETL job.

**Correct:** D
**Why:** AWS Glue ETL is managed and scalable; invoke job on S3 PUT via Lambda for timely conversion to Parquet.

**Incorrect:**
- A: Lambda alone is inefficient for 1‑GB files at scale.
- B: EMR or Athena‑based conversions add more ops/indirection.
- C: EMR or Athena‑based conversions add more ops/indirection.


---

---

### Question #266

A company has a popular gaming platform running on AWS. The application is sensitive to latency because latency can impact the user experience and introduce unfair advantages to some players. The application is deployed in every AWS Region. It runs on Amazon EC2 instances that are part of Auto Scaling groups congured behind Application Load Balancers (ALBs). A solutions architect needs to implement a mechanism to monitor the health of the application and redirect trac to healthy endpoints. Which solution meets these requirements?

- A. Configure an accelerator in AWS Global Accelerator. Add a listener for the port that the application listens on, and attach it to a Regional endpoint in each Region. Add the ALB as the endpoint.

- B. Create an Amazon CloudFront distribution and specify the ALB as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the trac.

- C. Create an Amazon CloudFront distribution and specify Amazon S3 as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the trac.

- D. Configure an Amazon DynamoDB database to serve as the data store for the application. Create a DynamoDB Accelerator (DAX) cluster to act as the in-memory cache for DynamoDB hosting the application data.

**Correct:** A
**Why:** Global Accelerator provides health checks and intelligent routing to the nearest healthy Regional endpoint (ALB).

**Incorrect:**
- B: CloudFront/S3 or DynamoDB/DAX don’t meet UDP/TCP app health redirect needs across Regions.
- C: CloudFront/S3 or DynamoDB/DAX don’t meet UDP/TCP app health redirect needs across Regions.
- D: CloudFront/S3 or DynamoDB/DAX don’t meet UDP/TCP app health redirect needs across Regions.


---

---

### Question #267

A company has one million users that use its mobile app. The company must analyze the data usage in near-real time. The company also must encrypt the data in near-real time and must store the data in a centralized location in Apache Parquet format for further processing. Which solution will meet these requirements with the LEAST operational overhead?

- A. Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon Kinesis Data Analytics application to analyze the data. Invoke an AWS Lambda function to send the data to the Kinesis Data Analytics application.

- B. Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon EMR cluster to analyze the data. Invoke an AWS Lambda function to send the data to the EMR cluster.

- C. Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create an Amazon EMR cluster to analyze the data.

- D. Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create an Amazon Kinesis Data Analytics application to analyze the data.

**Correct:** A
**Why:** Kinesis Data Streams → Kinesis Data Analytics for near real‑time analytics, Firehose to S3 with Parquet conversion; Athena for SQL.

**Incorrect:**
- B: Do not provide both real‑time analytics and Parquet delivery with least ops as cleanly.
- C: Do not provide both real‑time analytics and Parquet delivery with least ops as cleanly.
- D: Do not provide both real‑time analytics and Parquet delivery with least ops as cleanly.


---

---

### Question #268

A gaming company has a web application that displays scores. The application runs on Amazon EC2 instances behind an Application Load Balancer. The application stores data in an Amazon RDS for MySQL database. Users are starting to experience long delays and interruptions that are caused by database read performance. The company wants to improve the user experience while minimizing changes to the application’s architecture. What should a solutions architect do to meet these requirements?

- A. Use Amazon ElastiCache in front of the database.

- B. Use RDS Proxy between the application and the database.

- C. Migrate the application from EC2 instances to AWS Lambda.

- D. Migrate the database from Amazon RDS for MySQL to Amazon DynamoDB.

**Correct:** A
**Why:** Add ElastiCache in front of RDS to reduce read pressure without major architecture change.

**Incorrect:**
- B: RDS Proxy assists connections, not read latency.
- C: Lambda/DynamoDB are re‑architectures.
- D: Lambda/DynamoDB are re‑architectures.


---

---

### Question #274

A company runs an application on Amazon EC2 instances. The company needs to implement a disaster recovery (DR) solution for the application. The DR solution needs to have a recovery time objective (RTO) of less than 4 hours. The DR solution also needs to use the fewest possible AWS resources during normal operations. Which solution will meet these requirements in the MOST operationally ecient way?

- A. Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a secondary AWS Region. Automate infrastructure deployment in the secondary Region by using AWS Lambda and custom scripts.

- B. Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a secondary AWS Region. Automate infrastructure deployment in the secondary Region by using AWS CloudFormation.

- C. Launch EC2 instances in a secondary AWS Region. Keep the EC2 instances in the secondary Region active at all times.

- D. Launch EC2 instances in a secondary Availability Zone. Keep the EC2 instances in the secondary Availability Zone active at all times.

**Correct:** B
**Why:** AMIs copied cross‑Region and CloudFormation to provision infra offers <4h RTO with minimal steady‑state resources.

**Incorrect:**
- A: Custom scripts add ops.
- C: Keeping instances running increases steady‑state cost.
- D: Keeping instances running increases steady‑state cost.


---

---

### Question #278

A company wants to create an application to store employee data in a hierarchical structured relationship. The company needs a minimum-latency response to high-trac queries for the employee data and must protect any sensitive data. The company also needs to receive monthly email messages if any nancial information is present in the employee data. Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)

- A. Use Amazon Redshift to store the employee data in hierarchies. Unload the data to Amazon S3 every month.

- B. Use Amazon DynamoDB to store the employee data in hierarchies. Export the data to Amazon S3 every month.

- C. Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly events to AWS Lambda.

- D. Use Amazon Athena to analyze the employee data in Amazon S3. Integrate Athena with Amazon QuickSight to publish analysis dashboards and share the dashboards with users.

E. Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly notications through an Amazon Simple Notification Service (Amazon SNS) subscription.

**Correct:** B, E
**Why:** DynamoDB suits hierarchical, low‑latency queries; Macie + EventBridge → SNS can notify monthly when financial data is detected.

**Incorrect:**
- A: Redshift/Athena aren’t optimized for low‑latency hierarchical queries.
- C: Lambda trigger pattern is unnecessary for monthly notifications.
- D: Redshift/Athena aren’t optimized for low‑latency hierarchical queries.


---

---

### Question #283

A research company runs experiments that are powered by a simulation application and a visualization application. The simulation application runs on Linux and outputs intermediate data to an NFS share every 5 minutes. The visualization application is a Windows desktop application that displays the simulation output and requires an SMB file system. The company maintains two synchronized file systems. This strategy is causing data duplication and inecient resource usage. The company needs to migrate the applications to AWS without making code changes to either application. Which solution will meet these requirements?

- A. Migrate both applications to AWS Lambda. Create an Amazon S3 bucket to exchange data between the applications.

- B. Migrate both applications to Amazon Elastic Container Service (Amazon ECS). Configure Amazon FSx File Gateway for storage.

- C. Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization application to Windows EC2 instances. Configure Amazon Simple Queue Service (Amazon SQS) to exchange data between the applications.

- D. Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization application to Windows EC2 instances. Configure Amazon FSx for NetApp ONTAP for storage.

**Correct:** D
**Why:** FSx for NetApp ONTAP supports both NFS and SMB on the same data, removing duplicate storage and maintaining app compatibility.

**Incorrect:**
- A: Lambda/ECS + File Gateway or SQS don’t solve multi‑protocol file sharing.
- B: Lambda/ECS + File Gateway or SQS don’t solve multi‑protocol file sharing.
- C: Lambda/ECS + File Gateway or SQS don’t solve multi‑protocol file sharing.


---

---

### Question #285

A company hosts its static website by using Amazon S3. The company wants to add a contact form to its webpage. The contact form will have dynamic server-side components for users to input their name, email address, phone number, and user message. The company anticipates that there will be fewer than 100 site visits each month. Which solution will meet these requirements MOST cost-effectively?

- A. Host a dynamic contact form page in Amazon Elastic Container Service (Amazon ECS). Set up Amazon Simple Email Service (Amazon SES) to connect to any third-party email provider.

- B. Create an Amazon API Gateway endpoint with an AWS Lambda backend that makes a call to Amazon Simple Email Service (Amazon SES).

- C. Convert the static webpage to dynamic by deploying Amazon Lightsail. Use client-side scripting to build the contact form. Integrate the form with Amazon WorkMail.

- D. Create a t2.micro Amazon EC2 instance. Deploy a LAMP (Linux, Apache, MySQL, PHP/Perl/Python) stack to host the webpage. Use client- side scripting to build the contact form. Integrate the form with Amazon WorkMail.

**Correct:** B
**Why:** API Gateway + Lambda → SES adds minimal serverless backend to a static S3 site.

**Incorrect:**
- A: ECS/Lightsail/EC2 add cost and ops for very low traffic.
- C: ECS/Lightsail/EC2 add cost and ops for very low traffic.
- D: ECS/Lightsail/EC2 add cost and ops for very low traffic.


---

---

### Question #289

A company has an AWS Lambda function that needs read access to an Amazon S3 bucket that is located in the same AWS account. Which solution will meet these requirements in the MOST secure manner?

- A. Apply an S3 bucket policy that grants read access to the S3 bucket.

- B. Apply an IAM role to the Lambda function. Apply an IAM policy to the role to grant read access to the S3 bucket.

- C. Embed an access key and a secret key in the Lambda function’s code to grant the required IAM permissions for read access to the S3 bucket.

- D. Apply an IAM role to the Lambda function. Apply an IAM policy to the role to grant read access to all S3 buckets in the account.

**Correct:** B
**Why:** Attach an IAM role to Lambda with least‑privilege S3 read access to the specific bucket.

**Incorrect:**
- A: Bucket‑wide or all‑buckets access is overbroad.
- C: Never embed keys in code.
- D: Bucket‑wide or all‑buckets access is overbroad.


---

---

### Question #297

A company deploys an application on ve Amazon EC2 instances. An Application Load Balancer (ALB) distributes trac to the instances by using a target group. The average CPU usage on each of the instances is below 10% most of the time, with occasional surges to 65%. A solutions architect needs to implement a solution to automate the scalability of the application. The solution must optimize the cost of the architecture and must ensure that the application has enough CPU resources when surges occur. Which solution will meet these requirements?

- A. Create an Amazon CloudWatch alarm that enters the ALARM state when the CPUUtilization metric is less than 20%. Create an AWS Lambda function that the CloudWatch alarm invokes to terminate one of the EC2 instances in the ALB target group.

- B. Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing target group as the target group. Set a target tracking scaling policy that is based on the ASGAverageCPUUtilization metric. Set the minimum instances to 2, the desired capacity to 3, the maximum instances to 6, and the target value to 50%. Add the EC2 instances to the Auto Scaling group.

- C. Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing target group as the target group. Set the minimum instances to 2, the desired capacity to 3, and the maximum instances to 6. Add the EC2 instances to the Auto Scaling group.

- D. Create two Amazon CloudWatch alarms. Configure the first CloudWatch alarm to enter the ALARM state when the average CPUUtilization metric is below 20%. Configure the second CloudWatch alarm to enter the ALARM state when the average CPUUtilization matric is above 50%. Configure the alarms to publish to an Amazon Simple Notification Service (Amazon SNS) topic to send an email message. After receiving the message, log in to decrease or increase the number of EC2 instances that are running.

**Correct:** B
**Why:** Create an Auto Scaling group with target tracking on CPU and attach to existing ALB with appropriate min/desired/max.

**Incorrect:**
- A: Manual alarms/emails are not automation.
- C: Missing scaling policy.
- D: Manual alarms/emails are not automation.


---

---

### Question #303

A company is launching a new application deployed on an Amazon Elastic Container Service (Amazon ECS) cluster and is using the Fargate launch type for ECS tasks. The company is monitoring CPU and memory usage because it is expecting high trac to the application upon its launch. However, the company wants to reduce costs when utilization decreases. What should a solutions architect recommend?

- A. Use Amazon EC2 Auto Scaling to scale at certain periods based on previous trac patterns.

- B. Use an AWS Lambda function to scale Amazon ECS based on metric breaches that trigger an Amazon CloudWatch alarm.

- C. Use Amazon EC2 Auto Scaling with simple scaling policies to scale when ECS metric breaches trigger an Amazon CloudWatch alarm.

- D. Use AWS Application Auto Scaling with target tracking policies to scale when ECS metric breaches trigger an Amazon CloudWatch alarm.

**Correct:** D
**Why:** Use Application Auto Scaling target tracking for ECS on Fargate based on CPU/memory metrics.

**Incorrect:**
- A: EC2 Auto Scaling not applicable to Fargate tasks.
- B: Lambda intermediates are unnecessary.
- C: EC2 Auto Scaling not applicable to Fargate tasks.


---

---

### Question #311

A company is using AWS to design a web application that will process insurance quotes. Users will request quotes from the application. Quotes must be separated by quote type, must be responded to within 24 hours, and must not get lost. The solution must maximize operational eciency and must minimize maintenance. Which solution meets these requirements?

- A. Create multiple Amazon Kinesis data streams based on the quote type. Configure the web application to send messages to the proper data stream. Configure each backend group of application servers to use the Kinesis Client Library (KCL) to pool messages from its own data stream.

- B. Create an AWS Lambda function and an Amazon Simple Notification Service (Amazon SNS) topic for each quote type. Subscribe the Lambda function to its associated SNS topic. Configure the application to publish requests for quotes to the appropriate SNS topic.

- C. Create a single Amazon Simple Notification Service (Amazon SNS) topic. Subscribe Amazon Simple Queue Service (Amazon SQS) queues to the SNS topic. Configure SNS message filtering to publish messages to the proper SQS queue based on the quote type. Configure each backend application server to use its own SQS queue.

- D. Create multiple Amazon Kinesis Data Firehose delivery streams based on the quote type to deliver data streams to an Amazon OpenSearch Service cluster. Configure the application to send messages to the proper delivery stream. Configure each backend group of application servers to search for the messages from OpenSearch Service and process them accordingly.

**Correct:** C
**Why:** SNS to SQS with message filtering separates quote types; SQS ensures durability (no loss) and 24‑hour processing window.

**Incorrect:**
- A: Kinesis or Firehose/OpenSearch are not needed for durable request queues.
- B: SNS + Lambda per type adds code and lacks durable backlog.
- D: Kinesis or Firehose/OpenSearch are not needed for durable request queues.


---

---

### Question #312

A company has an application that runs on several Amazon EC2 instances. Each EC2 instance has multiple Amazon Elastic Block Store (Amazon EBS) data volumes attached to it. The application’s EC2 instance configuration and data need to be backed up nightly. The application also needs to be recoverable in a different AWS Region. Which solution will meet these requirements in the MOST operationally ecient way?

- A. Write an AWS Lambda function that schedules nightly snapshots of the application’s EBS volumes and copies the snapshots to a different Region.

- B. Create a backup plan by using AWS Backup to perform nightly backups. Copy the backups to another Region. Add the application’s EC2 instances as resources.

- C. Create a backup plan by using AWS Backup to perform nightly backups. Copy the backups to another Region. Add the application’s EBS volumes as resources.

- D. Write an AWS Lambda function that schedules nightly snapshots of the application's EBS volumes and copies the snapshots to a different Availability Zone.

**Correct:** B
**Why:** AWS Backup can back up EC2 instances (including EBS) nightly and copy to another Region with minimal ops.

**Incorrect:**
- A: DIY Lambdas or backing up only volumes miss instance config or add ops.
- C: DIY Lambdas or backing up only volumes miss instance config or add ops.
- D: DIY Lambdas or backing up only volumes miss instance config or add ops.


---

---

### Question #315

A company experienced a breach that affected several applications in its on-premises data center. The attacker took advantage of vulnerabilities in the custom applications that were running on the servers. The company is now migrating its applications to run on Amazon EC2 instances. The company wants to implement a solution that actively scans for vulnerabilities on the EC2 instances and sends a report that details the ndings. Which solution will meet these requirements?

- A. Deploy AWS Shield to scan the EC2 instances for vulnerabilities. Create an AWS Lambda function to log any ndings to AWS CloudTrail.

- B. Deploy Amazon Macie and AWS Lambda functions to scan the EC2 instances for vulnerabilities. Log any ndings to AWS CloudTrail.

- C. Turn on Amazon GuardDuty. Deploy the GuardDuty agents to the EC2 instances. Configure an AWS Lambda function to automate the generation and distribution of reports that detail the ndings.

- D. Turn on Amazon Inspector. Deploy the Amazon Inspector agent to the EC2 instances. Configure an AWS Lambda function to automate the generation and distribution of reports that detail the ndings.

**Correct:** D
**Why:** Amazon Inspector scans EC2 for vulnerabilities; use Lambda/automation to generate and distribute reports.

**Incorrect:**
- A: Shield/Macie/GuardDuty are not host vulnerability scanners.
- B: Shield/Macie/GuardDuty are not host vulnerability scanners.
- C: Shield/Macie/GuardDuty are not host vulnerability scanners.


---

---

### Question #316

A company uses an Amazon EC2 instance to run a script to poll for and process messages in an Amazon Simple Queue Service (Amazon SQS) queue. The company wants to reduce operational costs while maintaining its ability to process a growing number of messages that are added to the queue. What should a solutions architect recommend to meet these requirements?

- A. Increase the size of the EC2 instance to process messages faster.

- B. Use Amazon EventBridge to turn off the EC2 instance when the instance is underutilized.

- C. Migrate the script on the EC2 instance to an AWS Lambda function with the appropriate runtime.

- D. Use AWS Systems Manager Run Command to run the script on demand.

**Correct:** C
**Why:** Replace the EC2 poller with a Lambda function subscribed to SQS; scales automatically, lower cost.

**Incorrect:**
- A: Larger instance, turning off, or Run Command do not reduce ops or scale on demand.
- B: Larger instance, turning off, or Run Command do not reduce ops or scale on demand.
- D: Larger instance, turning off, or Run Command do not reduce ops or scale on demand.


---

---

### Question #317

A company uses a legacy application to produce data in CSV format. The legacy application stores the output data in Amazon S3. The company is deploying a new commercial off-the-shelf (COTS) application that can perform complex SQL queries to analyze data that is stored in Amazon Redshift and Amazon S3 only. However, the COTS application cannot process the .csv files that the legacy application produces. The company cannot update the legacy application to produce data in another format. The company needs to implement a solution so that the COTS application can use the data that the legacy application produces. Which solution will meet these requirements with the LEAST operational overhead?

- A. Create an AWS Glue extract, transform, and load (ETL) job that runs on a schedule. Configure the ETL job to process the .csv files and store the processed data in Amazon Redshift.

- B. Develop a Python script that runs on Amazon EC2 instances to convert the .csv files to .sql files. Invoke the Python script on a cron schedule to store the output files in Amazon S3.

- C. Create an AWS Lambda function and an Amazon DynamoDB table. Use an S3 event to invoke the Lambda function. Configure the Lambda function to perform an extract, transform, and load (ETL) job to process the .csv files and store the processed data in the DynamoDB table.

- D. Use Amazon EventBridge to launch an Amazon EMR cluster on a weekly schedule. Configure the EMR cluster to perform an extract, transform, and load (ETL) job to process the .csv files and store the processed data in an Amazon Redshift table.

**Correct:** A
**Why:** Glue ETL job on a schedule to process CSVs and load into Redshift with least ops.

**Incorrect:**
- B: EC2 scripts, DynamoDB, or EMR add ops for the use case.
- C: EC2 scripts, DynamoDB, or EMR add ops for the use case.
- D: EC2 scripts, DynamoDB, or EMR add ops for the use case.


---

---

### Question #319

A company has hundreds of Amazon EC2 Linux-based instances in the AWS Cloud. Systems administrators have used shared SSH keys to manage the instances. After a recent audit, the company’s security team is mandating the removal of all shared keys. A solutions architect must design a solution that provides secure access to the EC2 instances. Which solution will meet this requirement with the LEAST amount of administrative overhead?

- A. Use AWS Systems Manager Session Manager to connect to the EC2 instances.

- B. Use AWS Security Token Service (AWS STS) to generate one-time SSH keys on demand.

- C. Allow shared SSH access to a set of bastion instances. Configure all other instances to allow only SSH access from the bastion instances.

- D. Use an Amazon Cognito custom authorizer to authenticate users. Invoke an AWS Lambda function to generate a temporary SSH key.

**Correct:** A
**Why:** Session Manager provides secure, auditable access without SSH keys or open ports.

**Incorrect:**
- B: STS one‑time keys, bastions, or custom auth add overhead.
- C: STS one‑time keys, bastions, or custom auth add overhead.
- D: STS one‑time keys, bastions, or custom auth add overhead.


---

---

### Question #322

A solutions architect is designing a multi-tier application for a company. The application's users upload images from a mobile device. The application generates a thumbnail of each image and returns a message to the user to conrm that the image was uploaded successfully. The thumbnail generation can take up to 60 seconds, but the company wants to provide a faster response time to its users to notify them that the original image was received. The solutions architect must design the application to asynchronously dispatch requests to the different application tiers. What should the solutions architect do to meet these requirements?

- A. Write a custom AWS Lambda function to generate the thumbnail and alert the user. Use the image upload process as an event source to invoke the Lambda function.

- B. Create an AWS Step Functions workow. Configure Step Functions to handle the orchestration between the application tiers and alert the user when thumbnail generation is complete.

- C. Create an Amazon Simple Queue Service (Amazon SQS) message queue. As images are uploaded, place a message on the SQS queue for thumbnail generation. Alert the user through an application message that the image was received.

- D. Create Amazon Simple Notification Service (Amazon SNS) notification topics and subscriptions. Use one subscription with the application to generate the thumbnail after the image upload is complete. Use a second subscription to message the user's mobile app by way of a push notification after thumbnail generation is complete.

**Correct:** C
**Why:** Queue requests in SQS for async thumbnail generation and immediately acknowledge upload to users.

**Incorrect:**
- A: Lambda direct or Step Functions add latency or over‑orchestrate; SNS lacks durable work queue.
- B: Lambda direct or Step Functions add latency or over‑orchestrate; SNS lacks durable work queue.
- D: Lambda direct or Step Functions add latency or over‑orchestrate; SNS lacks durable work queue.


---

---

### Question #323

A company’s facility has badge readers at every entrance throughout the building. When badges are scanned, the readers send a message over HTTPS to indicate who attempted to access that particular entrance. A solutions architect must design a system to process these messages from the sensors. The solution must be highly available, and the results must be made available for the company’s security team to analyze. Which system architecture should the solutions architect recommend?

- A. Launch an Amazon EC2 instance to serve as the HTTPS endpoint and to process the messages. Configure the EC2 instance to save the results to an Amazon S3 bucket.

- B. Create an HTTPS endpoint in Amazon API Gateway. Configure the API Gateway endpoint to invoke an AWS Lambda function to process the messages and save the results to an Amazon DynamoDB table.

- C. Use Amazon Route 53 to direct incoming sensor messages to an AWS Lambda function. Configure the Lambda function to process the messages and save the results to an Amazon DynamoDB table.

- D. Create a gateway VPC endpoint for Amazon S3. Configure a Site-to-Site VPN connection from the facility network to the VPC so that sensor data can be written directly to an S3 bucket by way of the VPC endpoint.

**Correct:** B
**Why:** API Gateway HTTPS endpoint invoking Lambda, storing to DynamoDB, is highly available and serverless.

**Incorrect:**
- A: EC2 endpoint or direct S3 via VPN increases ops/complexity.
- C: Route 53 cannot directly invoke code.
- D: EC2 endpoint or direct S3 via VPN increases ops/complexity.


---

---

### Question #335

A company is experiencing sudden increases in demand. The company needs to provision large Amazon EC2 instances from an Amazon Machine Image (AMI). The instances will run in an Auto Scaling group. The company needs a solution that provides minimum initialization latency to meet the demand. Which solution meets these requirements?

- A. Use the aws ec2 register-image command to create an AMI from a snapshot. Use AWS Step Functions to replace the AMI in the Auto Scaling group.

- B. Enable Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot. Provision an AMI by using the snapshot. Replace the AMI in the Auto Scaling group with the new AMI.

- C. Enable AMI creation and dene lifecycle rules in Amazon Data Lifecycle Manager (Amazon DLM). Create an AWS Lambda function that modies the AMI in the Auto Scaling group.

- D. Use Amazon EventBridge to invoke AWS Backup lifecycle policies that provision AMIs. Configure Auto Scaling group capacity limits as an event source in EventBridge.

**Correct:** B
**Why:** Enable EBS Fast Snapshot Restore on the AMI’s snapshot to minimize initialization latency during scale‑out.

**Incorrect:**
- A: Register‑image/DLM/Backup events don’t directly reduce restore latency.
- C: Register‑image/DLM/Backup events don’t directly reduce restore latency.
- D: Register‑image/DLM/Backup events don’t directly reduce restore latency.


---

---

### Question #336

A company hosts a multi-tier web application that uses an Amazon Aurora MySQL DB cluster for storage. The application tier is hosted on Amazon EC2 instances. The company’s IT security guidelines mandate that the database credentials be encrypted and rotated every 14 days. What should a solutions architect do to meet this requirement with the LEAST operational effort?

- A. Create a new AWS Key Management Service (AWS KMS) encryption key. Use AWS Secrets Manager to create a new secret that uses the KMS key with the appropriate credentials. Associate the secret with the Aurora DB cluster. Configure a custom rotation period of 14 days.

- B. Create two parameters in AWS Systems Manager Parameter Store: one for the user name as a string parameter and one that uses the SecureString type for the password. Select AWS Key Management Service (AWS KMS) encryption for the password parameter, and load these parameters in the application tier. Implement an AWS Lambda function that rotates the password every 14 days.

- C. Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system in all EC2 instances of the application tier. Restrict the access to the file on the file system so that the application can read the file and that only super users can modify the file. Implement an AWS Lambda function that rotates the key in Aurora every 14 days and writes new credentials into the file.

- D. Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon S3 bucket that the application uses to load the credentials. Download the file to the application regularly to ensure that the correct credentials are used. Implement an AWS Lambda function that rotates the Aurora credentials every 14 days and uploads these credentials to the file in the S3 bucket.

**Correct:** A
**Why:** Secrets Manager with KMS‑encrypted secret and built‑in rotation integrates with Aurora; set 14‑day rotation.

**Incorrect:**
- B: Parameter Store/EFS/S3 solutions require more glue code and ops.
- C: Parameter Store/EFS/S3 solutions require more glue code and ops.
- D: Parameter Store/EFS/S3 solutions require more glue code and ops.


---

---

### Question #337

A company has deployed a web application on AWS. The company hosts the backend database on Amazon RDS for MySQL with a primary DB instance and ve read replicas to support scaling needs. The read replicas must lag no more than 1 second behind the primary DB instance. The database routinely runs scheduled stored procedures. As trac on the website increases, the replicas experience additional lag during periods of peak load. A solutions architect must reduce the replication lag as much as possible. The solutions architect must minimize changes to the application code and must minimize ongoing operational overhead. Which solution will meet these requirements?

- A. Migrate the database to Amazon Aurora MySQL. Replace the read replicas with Aurora Replicas, and configure Aurora Auto Scaling. Replace the stored procedures with Aurora MySQL native functions.

- B. Deploy an Amazon ElastiCache for Redis cluster in front of the database. Modify the application to check the cache before the application queries the database. Replace the stored procedures with AWS Lambda functions.

- C. Migrate the database to a MySQL database that runs on Amazon EC2 instances. Choose large, compute optimized EC2 instances for all replica nodes. Maintain the stored procedures on the EC2 instances.

- D. Migrate the database to Amazon DynamoDB. Provision a large number of read capacity units (RCUs) to support the required throughput, and configure on-demand capacity scaling. Replace the stored procedures with DynamoDB streams.

**Correct:** A
**Why:** Aurora MySQL with Aurora Replicas and Auto Scaling reduces replica lag and operational overhead.

**Incorrect:**
- B: Cache/EC2 self‑managed or DynamoDB require more changes.
- C: Cache/EC2 self‑managed or DynamoDB require more changes.
- D: Cache/EC2 self‑managed or DynamoDB require more changes.


---

---

### Question #339

A company has a custom application with embedded credentials that retrieves information from an Amazon RDS MySQL DB instance. Management says the application must be made more secure with the least amount of programming effort. What should a solutions architect do to meet these requirements?

- A. Use AWS Key Management Service (AWS KMS) to create keys. Configure the application to load the database credentials from AWS KMS. Enable automatic key rotation.

- B. Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS Secrets Manager. Configure the application to load the database credentials from Secrets Manager. Create an AWS Lambda function that rotates the credentials in Secret Manager.

- C. Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS Secrets Manager. Configure the application to load the database credentials from Secrets Manager. Set up a credentials rotation schedule for the application user in the RDS for MySQL database using Secrets Manager.

- D. Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS Systems Manager Parameter Store. Configure the application to load the database credentials from Parameter Store. Set up a credentials rotation schedule for the application user in the RDS for MySQL database using Parameter Store.

**Correct:** C
**Why:** Store DB creds in Secrets Manager and enable automated rotation for RDS MySQL with minimal code changes.

**Incorrect:**
- A: KMS or Parameter Store lack integrated RDS rotation automation.
- B: Lambda rotation is not needed; use built‑in rotation.
- D: KMS or Parameter Store lack integrated RDS rotation automation.


---

---

### Question #342

A transaction processing company has weekly scripted batch jobs that run on Amazon EC2 instances. The EC2 instances are in an Auto Scaling group. The number of transactions can vary, but the baseline CPU utilization that is noted on each run is at least 60%. The company needs to provision the capacity 30 minutes before the jobs run. Currently, engineers complete this task by manually modifying the Auto Scaling group parameters. The company does not have the resources to analyze the required capacity trends for the Auto Scaling group counts. The company needs an automated way to modify the Auto Scaling group’s desired capacity. Which solution will meet these requirements with the LEAST operational overhead?

- A. Create a dynamic scaling policy for the Auto Scaling group. Configure the policy to scale based on the CPU utilization metric. Set the target value for the metric to 60%.

- B. Create a scheduled scaling policy for the Auto Scaling group. Set the appropriate desired capacity, minimum capacity, and maximum capacity. Set the recurrence to weekly. Set the start time to 30 minutes before the batch jobs run.

- C. Create a predictive scaling policy for the Auto Scaling group. Configure the policy to scale based on forecast. Set the scaling metric to CPU utilization. Set the target value for the metric to 60%. In the policy, set the instances to pre-launch 30 minutes before the jobs run.

- D. Create an Amazon EventBridge event to invoke an AWS Lambda function when the CPU utilization metric value for the Auto Scaling group reaches 60%. Configure the Lambda function to increase the Auto Scaling group’s desired capacity and maximum capacity by 20%.

**Correct:** C
**Why:** Predictive scaling can forecast and pre‑launch capacity 30 minutes before the weekly batch run.

**Incorrect:**
- A: Dynamic/scheduled without forecasting or EventBridge on metric threshold won’t proactively pre‑launch as needed.
- B: Dynamic/scheduled without forecasting or EventBridge on metric threshold won’t proactively pre‑launch as needed.
- D: Dynamic/scheduled without forecasting or EventBridge on metric threshold won’t proactively pre‑launch as needed.


---

---

### Question #345

A company wants to restrict access to the content of one of its main web applications and to protect the content by using authorization techniques available on AWS. The company wants to implement a serverless architecture and an authentication solution for fewer than 100 users. The solution needs to integrate with the main web application and serve web content globally. The solution must also scale as the company's user base grows while providing the lowest login latency possible. Which solution will meet these requirements MOST cost-effectively?

- A. Use Amazon Cognito for authentication. Use Lambda@Edge for authorization. Use Amazon CloudFront to serve the web application globally.

- B. Use AWS Directory Service for Microsoft Active Directory for authentication. Use AWS Lambda for authorization. Use an Application Load Balancer to serve the web application globally.

- C. Use Amazon Cognito for authentication. Use AWS Lambda for authorization. Use Amazon S3 Transfer Acceleration to serve the web application globally.

- D. Use AWS Directory Service for Microsoft Active Directory for authentication. Use Lambda@Edge for authorization. Use AWS Elastic Beanstalk to serve the web application globally.

**Correct:** A
**Why:** Cognito for auth, Lambda@Edge for authorization, and CloudFront for global delivery provide a serverless, low‑latency solution.

**Incorrect:**
- B: Directory Service and Beanstalk/ALB add ops;
- C: S3 Transfer Acceleration isn’t for auth.
- D: Directory Service and Beanstalk/ALB add ops;


---

---

### Question #351

A company is moving its data management application to AWS. The company wants to transition to an event-driven architecture. The architecture needs to be more distributed and to use serverless concepts while performing the different aspects of the workow. The company also wants to minimize operational overhead. Which solution will meet these requirements?

- A. Build out the workow in AWS Glue. Use AWS Glue to invoke AWS Lambda functions to process the workow steps.

- B. Build out the workow in AWS Step Functions. Deploy the application on Amazon EC2 instances. Use Step Functions to invoke the workow steps on the EC2 instances.

- C. Build out the workow in Amazon EventBridge. Use EventBridge to invoke AWS Lambda functions on a schedule to process the workow steps.

- D. Build out the workow in AWS Step Functions. Use Step Functions to create a state machine. Use the state machine to invoke AWS Lambda functions to process the workow steps.

**Correct:** D
**Why:** Step Functions orchestrates serverless, event-driven workflows with a state machine that invokes Lambda for steps, minimizing ops and providing retries and branching.

**Incorrect:**
- A: AWS Glue is for ETL, not general workflow orchestration; invoking Lambda from Glue adds complexity.
- B: Orchestrating EC2 increases operational overhead and is not serverless.
- C: EventBridge schedules are time-based, not a workflow engine; you still need orchestration and step control.


---

---

### Question #354

A company hosts a serverless application on AWS. The application uses Amazon API Gateway, AWS Lambda, and an Amazon RDS for PostgreSQL database. The company notices an increase in application errors that result from database connection timeouts during times of peak trac or unpredictable trac. The company needs a solution that reduces the application failures with the least amount of change to the code. What should a solutions architect do to meet these requirements?

- A. Reduce the Lambda concurrency rate.

- B. Enable RDS Proxy on the RDS DB instance.

- C. Resize the RDS DB instance class to accept more connections.

- D. Migrate the database to Amazon DynamoDB with on-demand scaling.

**Correct:** B
**Why:** RDS Proxy pools and reuses connections for Lambda, preventing connection storms/timeouts during bursts with minimal code change.

**Incorrect:**
- A: Reducing Lambda concurrency throttles the app and does not fix connection pooling.
- C: Upsizing may delay but not solve connection exhaustion.
- D: Migrating to DynamoDB is a large rewrite.


---

---

### Question #355

A company is migrating an old application to AWS. The application runs a batch job every hour and is CPU intensive. The batch job takes 15 minutes on average with an on-premises server. The server has 64 virtual CPU (vCPU) and 512 GiB of memory. Which solution will run the batch job within 15 minutes with the LEAST operational overhead?

- A. Use AWS Lambda with functional scaling.

- B. Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate.

- C. Use Amazon Lightsail with AWS Auto Scaling.

- D. Use AWS Batch on Amazon EC2.

**Correct:** D
**Why:** AWS Batch on EC2 efficiently runs scheduled, CPU‑intensive batch jobs with needed large vCPU/memory shapes and minimal ops.

**Incorrect:**
- A: Lambda is not suitable for long, heavy jobs and has runtime/size limits.
- B: Fargate max vCPU/memory may not meet 64 vCPU/512 GiB needs and costs more.
- C: Lightsail is not appropriate for scalable batch.


---

---

### Question #366

A company’s web application consists of an Amazon API Gateway API in front of an AWS Lambda function and an Amazon DynamoDB database. The Lambda function handles the business logic, and the DynamoDB table hosts the data. The application uses Amazon Cognito user pools to identify the individual users of the application. A solutions architect needs to update the application so that only users who have a subscription can access premium content. Which solution will meet this requirement with the LEAST operational overhead?

- A. Enable API caching and throttling on the API Gateway API.

- B. Set up AWS WAF on the API Gateway API. Create a rule to filter users who have a subscription.

- C. Apply ne-grained IAM permissions to the premium content in the DynamoDB table.

- D. Implement API usage plans and API keys to limit the access of users who do not have a subscription.

**Correct:** D
**Why:** API Gateway usage plans and API keys provide an easy way to gate/limit access for non‑subscribers with low operational overhead.

**Incorrect:**
- A: Caching/throttling don’t enforce subscription access.
- B: WAF filters traffic patterns, not subscription entitlements.
- C: Fine‑grained IAM on DynamoDB is complex and not aligned with Cognito user pool tokens alone.


---

---

### Question #369

A company has migrated an application to Amazon EC2 Linux instances. One of these EC2 instances runs several 1-hour tasks on a schedule. These tasks were written by different teams and have no common programming language. The company is concerned about performance and scalability while these tasks run on a single instance. A solutions architect needs to implement a solution to resolve these concerns. Which solution will meet these requirements with the LEAST operational overhead?

- A. Use AWS Batch to run the tasks as jobs. Schedule the jobs by using Amazon EventBridge (Amazon CloudWatch Events).

- B. Convert the EC2 instance to a container. Use AWS App Runner to create the container on demand to run the tasks as jobs.

- C. Copy the tasks into AWS Lambda functions. Schedule the Lambda functions by using Amazon EventBridge (Amazon CloudWatch Events).

- D. Create an Amazon Machine Image (AMI) of the EC2 instance that runs the tasks. Create an Auto Scaling group with the AMI to run multiple copies of the instance.

**Correct:** A
**Why:** AWS Batch runs heterogeneous jobs (any language) as managed jobs and can be scheduled with EventBridge, improving scalability with minimal ops.

**Incorrect:**
- B: App Runner targets web services/containers, not scheduled arbitrary jobs as simply.
- C: Rewriting tasks into Lambda functions is heavy and may hit limits.
- D: Cloning instances increases cost/ops and doesn’t address scheduling/queuing.


---

---

### Question #375

An ecommerce company is building a distributed application that involves several serverless functions and AWS services to complete order- processing tasks. These tasks require manual approvals as part of the workow. A solutions architect needs to design an architecture for the order-processing application. The solution must be able to combine multiple AWS Lambda functions into responsive serverless applications. The solution also must orchestrate data and services that run on Amazon EC2 instances, containers, or on-premises servers. Which solution will meet these requirements with the LEAST operational overhead?

- A. Use AWS Step Functions to build the application.

- B. Integrate all the application components in an AWS Glue job.

- C. Use Amazon Simple Queue Service (Amazon SQS) to build the application.

- D. Use AWS Lambda functions and Amazon EventBridge events to build the application.

**Correct:** A
**Why:** Step Functions orchestrates serverless and non‑serverless tasks, supports human approval steps, and simplifies complex workflows.

**Incorrect:**
- B: AWS Glue is for data ETL, not general orchestration with approvals.
- C: SQS/EventBridge alone do not provide orchestration/state management.
- D: SQS/EventBridge alone do not provide orchestration/state management.


---

---

### Question #377

A company recently deployed a new auditing system to centralize information about operating system versions, patching, and installed software for Amazon EC2 instances. A solutions architect must ensure all instances provisioned through EC2 Auto Scaling groups successfully send reports to the auditing system as soon as they are launched and terminated. Which solution achieves these goals MOST eciently?

- A. Use a scheduled AWS Lambda function and run a script remotely on all EC2 instances to send data to the audit system.

- B. Use EC2 Auto Scaling lifecycle hooks to run a custom script to send data to the audit system when instances are launched and terminated.

- C. Use an EC2 Auto Scaling launch configuration to run a custom script through user data to send data to the audit system when instances are launched and terminated.

- D. Run a custom script on the instance operating system to send data to the audit system. Configure the script to be invoked by the EC2 Auto Scaling group when the instance starts and is terminated.

**Correct:** B
**Why:** EC2 Auto Scaling lifecycle hooks trigger custom scripts on launch/terminate so instances report to the audit system reliably.

**Incorrect:**
- A: Running remote scripts on all instances is brittle and inefficient.
- C: Launch configuration user data runs only on launch, not termination.
- D: Auto Scaling cannot invoke scripts inside the instance without lifecycle hooks.


---

---

### Question #379

A company hosts a frontend application that uses an Amazon API Gateway API backend that is integrated with AWS Lambda. When the API receives requests, the Lambda function loads many libraries. Then the Lambda function connects to an Amazon RDS database, processes the data, and returns the data to the frontend application. The company wants to ensure that response latency is as low as possible for all its users with the fewest number of changes to the company's operations. Which solution will meet these requirements?

- A. Establish a connection between the frontend application and the database to make queries faster by bypassing the API.

- B. Configure provisioned concurrency for the Lambda function that handles the requests.

- C. Cache the results of the queries in Amazon S3 for faster retrieval of similar datasets.

- D. Increase the size of the database to increase the number of connections Lambda can establish at one time.

**Correct:** B
**Why:** Provisioned concurrency keeps Lambda execution environments warm, reducing cold starts (library load time) and latency with minimal changes.

**Incorrect:**
- A: Direct DB access from frontend bypasses API and is insecure.
- C: Caching S3 results doesn’t address dynamic queries and connections.
- D: Upsizing DB does not remove Lambda cold starts.


---

---

### Question #380

A company is migrating its on-premises workload to the AWS Cloud. The company already uses several Amazon EC2 instances and Amazon RDS DB instances. The company wants a solution that automatically starts and stops the EC2 instances and DB instances outside of business hours. The solution must minimize cost and infrastructure maintenance. Which solution will meet these requirements?

- A. Scale the EC2 instances by using elastic resize. Scale the DB instances to zero outside of business hours.

- B. Explore AWS Marketplace for partner solutions that will automatically start and stop the EC2 instances and DB instances on a schedule.

- C. Launch another EC2 instance. Configure a crontab schedule to run shell scripts that will start and stop the existing EC2 instances and DB instances on a schedule.

- D. Create an AWS Lambda function that will start and stop the EC2 instances and DB instances. Configure Amazon EventBridge to invoke the Lambda function on a schedule.

**Correct:** D
**Why:** An EventBridge‑scheduled Lambda can start/stop EC2 and RDS on a schedule with minimal cost and ops.

**Incorrect:**
- A: EC2 “elastic resize” and RDS scale‑to‑zero do not apply.
- B: Marketplace adds cost/complexity.
- C: Managing a cron EC2 instance adds ops.


---

---

### Question #393

A payment processing company records all voice communication with its customers and stores the audio files in an Amazon S3 bucket. The company needs to capture the text from the audio files. The company must remove from the text any personally identiable information (PII) that belongs to customers. What should a solutions architect do to meet these requirements?

- A. Process the audio files by using Amazon Kinesis Video Streams. Use an AWS Lambda function to scan for known PII patterns.

- B. When an audio file is uploaded to the S3 bucket, invoke an AWS Lambda function to start an Amazon Textract task to analyze the call recordings.

- C. Configure an Amazon Transcribe transcription job with PII redaction turned on. When an audio file is uploaded to the S3 bucket, invoke an AWS Lambda function to start the transcription job. Store the output in a separate S3 bucket.

- D. Create an Amazon Connect contact flow that ingests the audio files with transcription turned on. Embed an AWS Lambda function to scan for known PII patterns. Use Amazon EventBridge to start the contact flow when an audio file is uploaded to the S3 bucket.

**Correct:** C
**Why:** Amazon Transcribe supports PII redaction; invoke jobs on upload and store sanitized text output separately.

**Incorrect:**
- A: Not appropriate services for transcription and PII redaction here.
- B: Textract is for documents, not audio.
- D: Not appropriate services for transcription and PII redaction here.


---

---

### Question #397

An ecommerce company needs to run a scheduled daily job to aggregate and filter sales records for analytics. The company stores the sales records in an Amazon S3 bucket. Each object can be up to 10 GB in size. Based on the number of sales events, the job can take up to an hour to complete. The CPU and memory usage of the job are constant and are known in advance. A solutions architect needs to minimize the amount of operational effort that is needed for the job to run. Which solution meets these requirements?

- A. Create an AWS Lambda function that has an Amazon EventBridge notification. Schedule the EventBridge event to run once a day.

- B. Create an AWS Lambda function. Create an Amazon API Gateway HTTP API, and integrate the API with the function. Create an Amazon EventBridge scheduled event that calls the API and invokes the function.

- C. Create an Amazon Elastic Container Service (Amazon ECS) cluster with an AWS Fargate launch type. Create an Amazon EventBridge scheduled event that launches an ECS task on the cluster to run the job.

- D. Create an Amazon Elastic Container Service (Amazon ECS) cluster with an Amazon EC2 launch type and an Auto Scaling group with at least one EC2 instance. Create an Amazon EventBridge scheduled event that launches an ECS task on the cluster to run the job.

**Correct:** C
**Why:** ECS on Fargate with an EventBridge schedule runs the containerized job with known CPU/memory, minimizing ops (no servers to manage).

**Incorrect:**
- A: Lambda is not suitable for up to 10 GB objects and 1‑hour jobs.
- B: Lambda is not suitable for up to 10 GB objects and 1‑hour jobs.
- D: EC2 launch type requires managing instances/ASG.


---

---

### Question #399

A nancial company hosts a web application on AWS. The application uses an Amazon API Gateway Regional API endpoint to give users the ability to retrieve current stock prices. The company’s security team has noticed an increase in the number of API requests. The security team is concerned that HTTP ood attacks might take the application oine. A solutions architect must design a solution to protect the application from this type of attack. Which solution meets these requirements with the LEAST operational overhead?

- A. Create an Amazon CloudFront distribution in front of the API Gateway Regional API endpoint with a maximum TTL of 24 hours.

- B. Create a Regional AWS WAF web ACL with a rate-based rule. Associate the web ACL with the API Gateway stage.

- C. Use Amazon CloudWatch metrics to monitor the Count metric and alert the security team when the predened rate is reached.

- D. Create an Amazon CloudFront distribution with Lambda@Edge in front of the API Gateway Regional API endpoint. Create an AWS Lambda function to block requests from IP addresses that exceed the predened rate.

**Correct:** B
**Why:** A Regional WAF web ACL with a rate‑based rule attached to the API Gateway stage mitigates HTTP flood attacks with low ops.

**Incorrect:**
- A: CloudFront alone doesn’t handle rate limiting for Regional APIs.
- C: Monitoring doesn’t mitigate attacks.
- D: Lambda@Edge scripting is more complex and operationally heavy.


---

---

### Question #403

A developer has an application that uses an AWS Lambda function to upload files to Amazon S3 and needs the required permissions to perform the task. The developer already has an IAM user with valid IAM credentials required for Amazon S3. What should a solutions architect do to grant the permissions?

- A. Add required IAM permissions in the resource policy of the Lambda function.

- B. Create a signed request using the existing IAM credentials in the Lambda function.

- C. Create a new IAM user and use the existing IAM credentials in the Lambda function.

- D. Create an IAM execution role with the required permissions and attach the IAM role to the Lambda function.

**Correct:** D
**Why:** Grant permissions via an IAM execution role attached to the Lambda function (least privilege, no static creds).

**Incorrect:**
- A: Lambda resource policy controls who can invoke, not its data access.
- B: Do not embed or create user credentials in code.
- C: Do not embed or create user credentials in code.


---

---

### Question #404

A company has deployed a serverless application that invokes an AWS Lambda function when new documents are uploaded to an Amazon S3 bucket. The application uses the Lambda function to process the documents. After a recent marketing campaign, the company noticed that the application did not process many of the documents. What should a solutions architect do to improve the architecture of this application?

- A. Set the Lambda function's runtime timeout value to 15 minutes.

- B. Configure an S3 bucket replication policy. Stage the documents in the S3 bucket for later processing.

- C. Deploy an additional Lambda function. Load balance the processing of the documents across the two Lambda functions.

- D. Create an Amazon Simple Queue Service (Amazon SQS) queue. Send the requests to the queue. Configure the queue as an event source for Lambda.

**Correct:** D
**Why:** Decouple S3 events with SQS; Lambda polls the queue, improving durability and handling bursts.

**Incorrect:**
- A: Increasing timeout does not fix event loss or throttling.
- B: Replication doesn’t address processing backpressure.
- C: Load balancing Lambdas is not applicable.


---

---

### Question #408

A company runs an application that receives data from thousands of geographically dispersed remote devices that use UDP. The application processes the data immediately and sends a message back to the device if necessary. No data is stored. The company needs a solution that minimizes latency for the data transmission from the devices. The solution also must provide rapid failover to another AWS Region. Which solution will meet these requirements?

- A. Configure an Amazon Route 53 failover routing policy. Create a Network Load Balancer (NLB) in each of the two Regions. Configure the NLB to invoke an AWS Lambda function to process the data.

- B. Use AWS Global Accelerator. Create a Network Load Balancer (NLB) in each of the two Regions as an endpoint. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the NLProcess the data in Amazon ECS.

- C. Use AWS Global Accelerator. Create an Application Load Balancer (ALB) in each of the two Regions as an endpoint. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the ALB. Process the data in Amazon ECS.

- D. Configure an Amazon Route 53 failover routing policy. Create an Application Load Balancer (ALB) in each of the two Regions. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the ALB. Process the data in Amazon ECS.

**Correct:** B
**Why:** Global Accelerator reduces latency globally and provides fast failover; NLB supports UDP; ECS Fargate scales to process data.

**Incorrect:**
- A: Route 53 failover alone doesn’t optimize latency.
- C: ALB does not support UDP.
- D: Same as A plus ALB (no UDP).


---

---

### Question #412

An image-hosting company stores its objects in Amazon S3 buckets. The company wants to avoid accidental exposure of the objects in the S3 buckets to the public. All S3 objects in the entire AWS account need to remain private. Which solution will meet these requirements?

- A. Use Amazon GuardDuty to monitor S3 bucket policies. Create an automatic remediation action rule that uses an AWS Lambda function to remediate any change that makes the objects public.

- B. Use AWS Trusted Advisor to nd publicly accessible S3 buckets. Configure email notications in Trusted Advisor when a change is detected. Manually change the S3 bucket policy if it allows public access.

- C. Use AWS Resource Access Manager to nd publicly accessible S3 buckets. Use Amazon Simple Notification Service (Amazon SNS) to invoke an AWS Lambda function when a change is detected. Deploy a Lambda function that programmatically remediates the change.

- D. Use the S3 Block Public Access feature on the account level. Use AWS Organizations to create a service control policy (SCP) that prevents IAM users from changing the setting. Apply the SCP to the account.

**Correct:** D
**Why:** Enable S3 Block Public Access at the account level and lock it with an SCP to prevent changes.

**Incorrect:**
- A: Detection and manual remediation have higher risk/overhead.
- B: Detection and manual remediation have higher risk/overhead.
- C: Detection and manual remediation have higher risk/overhead.


---

---

### Question #417

A company uses Amazon EC2 instances and AWS Lambda functions to run its application. The company has VPCs with public subnets and private subnets in its AWS account. The EC2 instances run in a private subnet in one of the VPCs. The Lambda functions need direct network access to the EC2 instances for the application to work. The application will run for at least 1 year. The company expects the number of Lambda functions that the application uses to increase during that time. The company wants to maximize its savings on all application resources and to keep network latency between the services low. Which solution will meet these requirements?

- A. Purchase an EC2 Instance Savings Plan Optimize the Lambda functions’ duration and memory usage and the number of invocations. Connect the Lambda functions to the private subnet that contains the EC2 instances.

- B. Purchase an EC2 Instance Savings Plan Optimize the Lambda functions' duration and memory usage, the number of invocations, and the amount of data that is transferred. Connect the Lambda functions to a public subnet in the same VPC where the EC2 instances run.

- C. Purchase a Compute Savings Plan. Optimize the Lambda functions’ duration and memory usage, the number of invocations, and the amount of data that is transferred. Connect the Lambda functions to the private subnet that contains the EC2 instances.

- D. Purchase a Compute Savings Plan. Optimize the Lambda functions’ duration and memory usage, the number of invocations, and the amount of data that is transferred. Keep the Lambda functions in the Lambda service VPC.

**Correct:** C
**Why:** Compute Savings Plan covers EC2 and Lambda; attach Lambda to the private subnet for low‑latency access to EC2.

**Incorrect:**
- A: EC2 Instance Savings Plan does not apply to Lambda.
- B: EC2 Instance Savings Plan does not apply to Lambda.
- D: Keeping Lambda in the service VPC prevents direct access to private EC2.


---

---

### Question #422

A company is developing a new machine learning (ML) model solution on AWS. The models are developed as independent microservices that fetch approximately 1 GB of model data from Amazon S3 at startup and load the data into memory. Users access the models through an asynchronous API. Users can send a request or a batch of requests and specify where the results should be sent. The company provides models to hundreds of users. The usage patterns for the models are irregular. Some models could be unused for days or weeks. Other models could receive batches of thousands of requests at a time. Which design should a solutions architect recommend to meet these requirements?

- A. Direct the requests from the API to a Network Load Balancer (NLB). Deploy the models as AWS Lambda functions that are invoked by the NLB.

- B. Direct the requests from the API to an Application Load Balancer (ALB). Deploy the models as Amazon Elastic Container Service (Amazon ECS) services that read from an Amazon Simple Queue Service (Amazon SQS) queue. Use AWS App Mesh to scale the instances of the ECS cluster based on the SQS queue size.

- C. Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue. Deploy the models as AWS Lambda functions that are invoked by SQS events. Use AWS Auto Scaling to increase the number of vCPUs for the Lambda functions based on the SQS queue size.

- D. Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue. Deploy the models as Amazon Elastic Container Service (Amazon ECS) services that read from the queue. Enable AWS Auto Scaling on Amazon ECS for both the cluster and copies of the service based on the queue size.

**Correct:** D
**Why:** Queue requests in SQS and run models on ECS services that scale on queue depth; suitable for 1 GB model loads and bursty use.

**Incorrect:**
- A: NLB→Lambda is not a pattern; Lambda cold starts with 1 GB model pulls are costly.
- B: ALB→ECS without decoupling risks backpressure; App Mesh is unnecessary.
- C: Lambda with 1 GB model loads and vCPU scaling is not appropriate.


---

---

### Question #427

A solutions architect is implementing a complex Java application with a MySQL database. The Java application must be deployed on Apache Tomcat and must be highly available. What should the solutions architect do to meet these requirements?

- A. Deploy the application in AWS Lambda. Configure an Amazon API Gateway API to connect with the Lambda functions.

- B. Deploy the application by using AWS Elastic Beanstalk. Configure a load-balanced environment and a rolling deployment policy.

- C. Migrate the database to Amazon ElastiCache. Configure the ElastiCache security group to allow access from the application.

- D. Launch an Amazon EC2 instance. Install a MySQL server on the EC2 instance. Configure the application on the server. Create an AMI. Use the AMI to create a launch template with an Auto Scaling group.

**Correct:** B
**Why:** Elastic Beanstalk for Tomcat with load‑balanced, rolling deployments meets HA needs with low ops; use RDS for MySQL.

**Incorrect:**
- A: Lambda is not for Tomcat Java web apps.
- C: ElastiCache is not a database.
- D: DIY EC2/ASG raises operational burden.


---

---

### Question #428

A serverless application uses Amazon API Gateway, AWS Lambda, and Amazon DynamoDB. The Lambda function needs permissions to read and write to the DynamoDB table. Which solution will give the Lambda function access to the DynamoDB table MOST securely?

- A. Create an IAM user with programmatic access to the Lambda function. Attach a policy to the user that allows read and write access to the DynamoDB table. Store the access_key_id and secret_access_key parameters as part of the Lambda environment variables. Ensure that other AWS users do not have read and write access to the Lambda function configuration.

- B. Create an IAM role that includes Lambda as a trusted service. Attach a policy to the role that allows read and write access to the DynamoDB table. Update the configuration of the Lambda function to use the new role as the execution role.

- C. Create an IAM user with programmatic access to the Lambda function. Attach a policy to the user that allows read and write access to the DynamoDB table. Store the access_key_id and secret_access_key parameters in AWS Systems Manager Parameter Store as secure string parameters. Update the Lambda function code to retrieve the secure string parameters before connecting to the DynamoDB table.

- D. Create an IAM role that includes DynamoDB as a trusted service. Attach a policy to the role that allows read and write access from the Lambda function. Update the code of the Lambda function to attach to the new role as an execution role.

**Correct:** B
**Why:** Use an IAM role trusted by Lambda with a policy granting DynamoDB access; attach as the function’s execution role.

**Incorrect:**
- A: Do not use static IAM user creds in Lambda.
- C: Do not use static IAM user creds in Lambda.
- D: DynamoDB is not a trusted principal for the role.


---

---

### Question #430

A manufacturing company has machine sensors that upload .csv files to an Amazon S3 bucket. These .csv files must be converted into images and must be made available as soon as possible for the automatic generation of graphical reports. The images become irrelevant after 1 month, but the .csv files must be kept to train machine learning (ML) models twice a year. The ML trainings and audits are planned weeks in advance. Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)

- A. Launch an Amazon EC2 Spot Instance that downloads the .csv files every hour, generates the image files, and uploads the images to the S3 bucket.

- B. Design an AWS Lambda function that converts the .csv files into images and stores the images in the S3 bucket. Invoke the Lambda function when a .csv file is uploaded.

- C. Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files from S3 Standard to S3 Glacier 1 day after they are uploaded. Expire the image files after 30 days.

- D. Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) 1 day after they are uploaded. Expire the image files after 30 days.

E. Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 1 day after they are uploaded. Keep the image files in Reduced Redundancy Storage (RRS).

**Correct:** B, C
**Why:** Invoke Lambda on S3 put to convert CSV to images quickly; transition CSVs to Glacier and expire images after 30 days to cut cost.

**Incorrect:**
- A: Managing Spot EC2 adds ops and latency.
- D: One Zone‑IA/RRS are not optimal and RRS is deprecated.
- E: One Zone‑IA/RRS are not optimal and RRS is deprecated.


---

---

### Question #434

A company hosts its application in the AWS Cloud. The application runs on Amazon EC2 instances behind an Elastic Load Balancer in an Auto Scaling group and with an Amazon DynamoDB table. The company wants to ensure the application can be made available in anotherAWS Region with minimal downtime. What should a solutions architect do to meet these requirements with the LEAST amount of downtime?

- A. Create an Auto Scaling group and a load balancer in the disaster recovery Region. Configure the DynamoDB table as a global table. Configure DNS failover to point to the new disaster recovery Region's load balancer.

- B. Create an AWS CloudFormation template to create EC2 instances, load balancers, and DynamoDB tables to be launched when needed Configure DNS failover to point to the new disaster recovery Region's load balancer.

- C. Create an AWS CloudFormation template to create EC2 instances and a load balancer to be launched when needed. Configure the DynamoDB table as a global table. Configure DNS failover to point to the new disaster recovery Region's load balancer.

- D. Create an Auto Scaling group and load balancer in the disaster recovery Region. Configure the DynamoDB table as a global table. Create an Amazon CloudWatch alarm to trigger an AWS Lambda function that updates Amazon Route 53 pointing to the disaster recovery load balancer.

**Correct:** A
**Why:** Pre‑provision ALB/ASG in DR Region and use DynamoDB global tables; DNS failover minimizes downtime.

**Incorrect:**
- B: Delayed provisioning increases downtime.
- C: Delayed provisioning increases downtime.
- D: Extra Lambda automation is unnecessary.


---

---

### Question #441

A company hosts a multi-tier web application on Amazon Linux Amazon EC2 instances behind an Application Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The company observes that the Auto Scaling group launches more On-Demand Instances when the application's end users access high volumes of static web content. The company wants to optimize cost. What should a solutions architect do to redesign the application MOST cost-effectively?

- A. Update the Auto Scaling group to use Reserved Instances instead of On-Demand Instances.

- B. Update the Auto Scaling group to scale by launching Spot Instances instead of On-Demand Instances.

- C. Create an Amazon CloudFront distribution to host the static web contents from an Amazon S3 bucket.

- D. Create an AWS Lambda function behind an Amazon API Gateway API to host the static website contents.

**Correct:** C
**Why:** Serve static content from S3 via CloudFront to reduce load on EC2/ALB and lower cost.

**Incorrect:**
- A: Instance pricing choices don’t remove static load driver.
- B: Instance pricing choices don’t remove static load driver.
- D: Lambda+API Gateway is unnecessary for static hosting.


---

---

### Question #444

A company has hired a solutions architect to design a reliable architecture for its application. The application consists of one Amazon RDS DB instance and two manually provisioned Amazon EC2 instances that run web servers. The EC2 instances are located in a single Availability Zone. An employee recently deleted the DB instance, and the application was unavailable for 24 hours as a result. The company is concerned with the overall reliability of its environment. What should the solutions architect do to maximize reliability of the application's infrastructure?

- A. Delete one EC2 instance and enable termination protection on the other EC2 instance. Update the DB instance to be Multi-AZ, and enable deletion protection.

- B. Update the DB instance to be Multi-AZ, and enable deletion protection. Place the EC2 instances behind an Application Load Balancer, and run them in an EC2 Auto Scaling group across multiple Availability Zones.

- C. Create an additional DB instance along with an Amazon API Gateway and an AWS Lambda function. Configure the application to invoke the Lambda function through API Gateway. Have the Lambda function write the data to the two DB instances.

- D. Place the EC2 instances in an EC2 Auto Scaling group that has multiple subnets located in multiple Availability Zones. Use Spot Instances instead of On-Demand Instances. Set up Amazon CloudWatch alarms to monitor the health of the instances Update the DB instance to be Multi-AZ, and enable deletion protection.

**Correct:** B
**Why:** Protect RDS with Multi‑AZ and deletion protection; place EC2 in ALB+Auto Scaling across AZs to maximize reliability.

**Incorrect:**
- A: Either reduce availability or add unnecessary components/cost.
- C: Either reduce availability or add unnecessary components/cost.
- D: Either reduce availability or add unnecessary components/cost.


---

---

### Question #447

A company has a stateless web application that runs on AWS Lambda functions that are invoked by Amazon API Gateway. The company wants to deploy the application across multiple AWS Regions to provide Regional failover capabilities. What should a solutions architect do to route trac to multiple Regions?

- A. Create Amazon Route 53 health checks for each Region. Use an active-active failover configuration.

- B. Create an Amazon CloudFront distribution with an origin for each Region. Use CloudFront health checks to route trac.

- C. Create a transit gateway. Attach the transit gateway to the API Gateway endpoint in each Region. Configure the transit gateway to route requests.

- D. Create an Application Load Balancer in the primary Region. Set the target group to point to the API Gateway endpoint hostnames in each Region.

**Correct:** A
**Why:** Use Route 53 health checks with active‑active routing across Regions for serverless endpoints.

**Incorrect:**
- B: CloudFront is for HTTP caching, not API multi‑Region routing.
- C: Transit gateway/ALB cannot directly route to API Gateway across Regions as required.
- D: Transit gateway/ALB cannot directly route to API Gateway across Regions as required.


---

---

### Question #452

A company runs a Java-based job on an Amazon EC2 instance. The job runs every hour and takes 10 seconds to run. The job runs on a scheduled interval and consumes 1 GB of memory. The CPU utilization of the instance is low except for short surges during which the job uses the maximum CPU available. The company wants to optimize the costs to run the job. Which solution will meet these requirements?

- A. Use AWS App2Container (A2C) to containerize the job. Run the job as an Amazon Elastic Container Service (Amazon ECS) task on AWS Fargate with 0.5 virtual CPU (vCPU) and 1 GB of memory.

- B. Copy the code into an AWS Lambda function that has 1 GB of memory. Create an Amazon EventBridge scheduled rule to run the code each hour.

- C. Use AWS App2Container (A2C) to containerize the job. Install the container in the existing Amazon Machine Image (AMI). Ensure that the schedule stops the container when the task nishes.

- D. Configure the existing schedule to stop the EC2 instance at the completion of the job and restart the EC2 instance when the next job starts.

**Correct:** B
**Why:** Lambda with 1 GB memory and an EventBridge schedule runs for ~10 seconds/hour at the lowest cost; no servers to manage.

**Incorrect:**
- A: Containerize/EC2 approaches incur more baseline cost and management for such a short job.
- C: Containerize/EC2 approaches incur more baseline cost and management for such a short job.
- D: Containerize/EC2 approaches incur more baseline cost and management for such a short job.


---

---

### Question #457

A company that uses AWS is building an application to transfer data to a product manufacturer. The company has its own identity provider (IdP). The company wants the IdP to authenticate application users while the users use the application to transfer data. The company must use Applicability Statement 2 (AS2) protocol. Which solution will meet these requirements?

- A. Use AWS DataSync to transfer the data. Create an AWS Lambda function for IdP authentication.

- B. Use Amazon AppFlow ows to transfer the data. Create an Amazon Elastic Container Service (Amazon ECS) task for IdP authentication.

- C. Use AWS Transfer Family to transfer the data. Create an AWS Lambda function for IdP authentication.

- D. Use AWS Storage Gateway to transfer the data. Create an Amazon Cognito identity pool for IdP authentication.

**Correct:** C
**Why:** AWS Transfer Family supports AS2. You can integrate with your IdP and authenticate users while using the AS2 protocol.

**Incorrect:**
- A: DataSync, AppFlow, and Storage Gateway do not provide AS2 transfers with IdP‑based auth.
- B: DataSync, AppFlow, and Storage Gateway do not provide AS2 transfers with IdP‑based auth.
- D: DataSync, AppFlow, and Storage Gateway do not provide AS2 transfers with IdP‑based auth.


---

---

### Question #458

A solutions architect is designing a RESTAPI in Amazon API Gateway for a cash payback service. The application requires 1 GB of memory and 2 GB of storage for its computation resources. The application will require that the data is in a relational format. Which additional combination ofAWS services will meet these requirements with the LEAST administrative effort? (Choose two.)

- A. Amazon EC2

- B. AWS Lambda

- C. Amazon RDS

- D. Amazon DynamoDB

E. Amazon Elastic Kubernetes Services (Amazon EKS)

**Correct:** B, C
**Why:** Lambda can be sized to 1 GB memory and sufficient ephemeral storage; use Amazon RDS for relational storage—minimal admin.

**Incorrect:**
- A: EC2/EKS add operational overhead.
- D: DynamoDB is non‑relational.
- E: EC2/EKS add operational overhead.


---

---

### Question #460

A company wants to securely exchange data between its software as a service (SaaS) application Salesforce account and Amazon S3. The company must encrypt the data at rest by using AWS Key Management Service (AWS KMS) customer managed keys (CMKs). The company must also encrypt the data in transit. The company has enabled API access for the Salesforce account.

- A. Create AWS Lambda functions to transfer the data securely from Salesforce to Amazon S3.

- B. Create an AWS Step Functions workow. Dene the task to transfer the data securely from Salesforce to Amazon S3.

- C. Create Amazon AppFlow ows to transfer the data securely from Salesforce to Amazon S3.

- D. Create a custom connector for Salesforce to transfer the data securely from Salesforce to Amazon S3.

**Correct:** C
**Why:** Amazon AppFlow has a native Salesforce connector with TLS in transit and SSE‑KMS at rest to S3 with CMKs.

**Incorrect:**
- A: More custom work; AppFlow is purpose‑built for this.
- B: More custom work; AppFlow is purpose‑built for this.
- D: More custom work; AppFlow is purpose‑built for this.


---

---

### Question #463

An IoT company is releasing a mattress that has sensors to collect data about a user’s sleep. The sensors will send data to an Amazon S3 bucket. The sensors collect approximately 2 MB of data every night for each mattress. The company must process and summarize the data for each mattress. The results need to be available as soon as possible. Data processing will require 1 GB of memory and will nish within 30 seconds. Which solution will meet these requirements MOST cost-effectively?

- A. Use AWS Glue with a Scala job

- B. Use Amazon EMR with an Apache Spark script

- C. Use AWS Lambda with a Python script

- D. Use AWS Glue with a PySpark job

**Correct:** C
**Why:** Lambda with a Python function is cost‑effective for 30‑second, 1 GB jobs triggered on S3 object arrival.

**Incorrect:**
- A: Glue/EMR are heavier/more costly for this small batch.
- B: Glue/EMR are heavier/more costly for this small batch.
- D: Glue/EMR are heavier/more costly for this small batch.


---

---

### Question #478

A law rm needs to share information with the public. The information includes hundreds of files that must be publicly readable. Modications or deletions of the files by anyone before a designated future date are prohibited. Which solution will meet these requirements in the MOST secure way?

- A. Upload all files to an Amazon S3 bucket that is congured for static website hosting. Grant read-only IAM permissions to any AWS principals that access the S3 bucket until the designated date.

- B. Create a new Amazon S3 bucket with S3 Versioning enabled. Use S3 Object Lock with a retention period in accordance with the designated date. Configure the S3 bucket for static website hosting. Set an S3 bucket policy to allow read-only access to the objects.

- C. Create a new Amazon S3 bucket with S3 Versioning enabled. Configure an event trigger to run an AWS Lambda function in case of object modication or deletion. Configure the Lambda function to replace the objects with the original versions from a private S3 bucket.

- D. Upload all files to an Amazon S3 bucket that is congured for static website hosting. Select the folder that contains the files. Use S3 Object Lock with a retention period in accordance with the designated date. Grant read-only IAM permissions to any AWS principals that access the S3 bucket.

**Correct:** B
**Why:** Enable Versioning and Object Lock (compliance) with a retention period; host as static website; allow public read via bucket policy.

**Incorrect:**
- A: Without Object Lock on the bucket, files could be altered/deleted.
- C: Lambda rollback is reactive and less secure.
- D: Without Object Lock on the bucket, files could be altered/deleted.


---

---

### Question #483

A company containerized a Windows job that runs on .NET 6 Framework under a Windows container. The company wants to run this job in the AWS Cloud. The job runs every 10 minutes. The job’s runtime varies between 1 minute and 3 minutes. Which solution will meet these requirements MOST cost-effectively?

- A. Create an AWS Lambda function based on the container image of the job. Configure Amazon EventBridge to invoke the function every 10 minutes.

- B. Use AWS Batch to create a job that uses AWS Fargate resources. Configure the job scheduling to run every 10 minutes.

- C. Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate to run the job. Create a scheduled task based on the container image of the job to run every 10 minutes.

- D. Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate to run the job. Create a standalone task based on the container image of the job. Use Windows task scheduler to run the job every 10 minutes.

**Correct:** C
**Why:** ECS on Fargate scheduled tasks run Windows containers cost‑effectively for short, periodic jobs; Lambda doesn’t support Windows containers.

**Incorrect:**
- A: Lambda cannot run Windows containers.
- B: Batch is heavier for a fixed, short schedule.
- D: Standalone task + Windows scheduler increases ops.


---

---

### Question #490

A gaming company uses Amazon DynamoDB to store user information such as geographic location, player data, and leaderboards. The company needs to configure continuous backups to an Amazon S3 bucket with a minimal amount of coding. The backups must not affect availability of the application and must not affect the read capacity units (RCUs) that are dened for the table. Which solution meets these requirements?

- A. Use an Amazon EMR cluster. Create an Apache Hive job to back up the data to Amazon S3.

- B. Export the data directly from DynamoDB to Amazon S3 with continuous backups. Turn on point-in-time recovery for the table.

- C. Configure Amazon DynamoDB Streams. Create an AWS Lambda function to consume the stream and export the data to an Amazon S3 bucket.

- D. Create an AWS Lambda function to export the data from the database tables to Amazon S3 on a regular basis. Turn on point-in-time recovery for the table.

**Correct:** B
**Why:** DynamoDB table export to S3 provides continuous, server‑side exports without consuming RCUs and with no downtime; enable PITR.

**Incorrect:**
- A: EMR/Lambda/Streams add code/ops and may affect capacity.
- C: EMR/Lambda/Streams add code/ops and may affect capacity.
- D: EMR/Lambda/Streams add code/ops and may affect capacity.


---

---

### Question #491

A solutions architect is designing an asynchronous application to process credit card data validation requests for a bank. The application must be secure and be able to process each request at least once. Which solution will meet these requirements MOST cost-effectively?

- A. Use AWS Lambda event source mapping. Set Amazon Simple Queue Service (Amazon SQS) standard queues as the event source. Use AWS Key Management Service (SSE-KMS) for encryption. Add the kms:Decrypt permission for the Lambda execution role.

- B. Use AWS Lambda event source mapping. Use Amazon Simple Queue Service (Amazon SQS) FIFO queues as the event source. Use SQS managed encryption keys (SSE-SQS) for encryption. Add the encryption key invocation permission for the Lambda function.

- C. Use the AWS Lambda event source mapping. Set Amazon Simple Queue Service (Amazon SQS) FIFO queues as the event source. Use AWS KMS keys (SSE-KMS). Add the kms:Decrypt permission for the Lambda execution role.

- D. Use the AWS Lambda event source mapping. Set Amazon Simple Queue Service (Amazon SQS) standard queues as the event source. Use AWS KMS keys (SSE-KMS) for encryption. Add the encryption key invocation permission for the Lambda function.

**Correct:** A
**Why:** SQS standard queues provide at‑least‑once delivery; encrypt with SSE‑KMS and grant Lambda kms:Decrypt.

**Incorrect:**
- B: FIFO is not required and may cost more; managed keys choice varies.
- C: FIFO is not required and may cost more; managed keys choice varies.
- D: Similar to A but misstates key permissions pattern.


---

---

### Question #492

A company has multiple AWS accounts for development work. Some staff consistently use oversized Amazon EC2 instances, which causes the company to exceed the yearly budget for the development accounts. The company wants to centrally restrict the creation of AWS resources in these accounts. Which solution will meet these requirements with the LEAST development effort?

- A. Develop AWS Systems Manager templates that use an approved EC2 creation process. Use the approved Systems Manager templates to provision EC2 instances.

- B. Use AWS Organizations to organize the accounts into organizational units (OUs). Dene and attach a service control policy (SCP) to control the usage of EC2 instance types.

- C. Configure an Amazon EventBridge rule that invokes an AWS Lambda function when an EC2 instance is created. Stop disallowed EC2 instance types.

- D. Set up AWS Service Catalog products for the staff to create the allowed EC2 instance types. Ensure that staff can deploy EC2 instances only by using the Service Catalog products.

**Correct:** B
**Why:** Organize accounts and attach an SCP that denies disallowed EC2 instance types (least effort, central control).

**Incorrect:**
- A: Heavier build or reactive after creation.
- C: Heavier build or reactive after creation.
- D: Heavier build or reactive after creation.


---

---

### Question #498

A company uses Amazon S3 to store high-resolution pictures in an S3 bucket. To minimize application changes, the company stores the pictures as the latest version of an S3 object. The company needs to retain only the two most recent versions of the pictures. The company wants to reduce costs. The company has identied the S3 bucket as a large expense. Which solution will reduce the S3 costs with the LEAST operational overhead?

- A. Use S3 Lifecycle to delete expired object versions and retain the two most recent versions.

- B. Use an AWS Lambda function to check for older versions and delete all but the two most recent versions.

- C. Use S3 Batch Operations to delete noncurrent object versions and retain only the two most recent versions.

- D. Deactivate versioning on the S3 bucket and retain the two most recent versions.

**Correct:** A
**Why:** S3 Lifecycle can retain a set number of noncurrent versions and expire older versions automatically—low overhead.

**Incorrect:**
- B: Lambda/Batch Operations add ongoing work.
- C: Lambda/Batch Operations add ongoing work.
- D: Disabling versioning loses protection and does not retain two versions.


---

## AWS Network Firewall

### Question #327

A solutions architect must secure a VPC network that hosts Amazon EC2 instances. The EC2 instances contain highly sensitive data and run in a private subnet. According to company policy, the EC2 instances that run in the VPC can access only approved third-party software repositories on the internet for software product updates that use the third party’s URL. Other internet trac must be blocked. Which solution meets these requirements?

- A. Update the route table for the private subnet to route the outbound trac to an AWS Network Firewall firewall. Configure domain list rule groups.

- B. Set up an AWS WAF web ACL. Create a custom set of rules that filter trac requests based on source and destination IP address range sets.

- C. Implement strict inbound security group rules. Configure an outbound rule that allows trac only to the authorized software repositories on the internet by specifying the URLs.

- D. Configure an Application Load Balancer (ALB) in front of the EC2 instances. Direct all outbound trac to the ALB. Use a URL-based rule listener in the ALB’s target group for outbound access to the internet.

**Correct:** A
**Why:** AWS Network Firewall with domain list rules provides egress filtering to approved repositories from private subnets.

**Incorrect:**
- B: WAF is for HTTP(S) to apps; SGs cannot filter by domain/URL; ALB is inbound.
- C: WAF is for HTTP(S) to apps; SGs cannot filter by domain/URL; ALB is inbound.
- D: WAF is for HTTP(S) to apps; SGs cannot filter by domain/URL; ALB is inbound.


---

## AWS Organizations / IAM Identity Center (SSO)

### Question #332

A company needs to provide its employees with secure access to condential and sensitive files. The company wants to ensure that the files can be accessed only by authorized users. The files must be downloaded securely to the employees’ devices. The files are stored in an on-premises Windows file server. However, due to an increase in remote usage, the file server is running out of capacity. . Which solution will meet these requirements?

- A. Migrate the file server to an Amazon EC2 instance in a public subnet. Configure the security group to limit inbound trac to the employees’ IP addresses.

- B. Migrate the files to an Amazon FSx for Windows File Server file system. Integrate the Amazon FSx file system with the on-premises Active Directory. Configure AWS Client VPN.

- C. Migrate the files to Amazon S3, and create a private VPC endpoint. Create a signed URL to allow download.

- D. Migrate the files to Amazon S3, and create a public VPC endpoint. Allow employees to sign on with AWS IAM Identity Center (AWS Single Sign-On).

**Correct:** B
**Why:** FSx for Windows with on‑prem AD integration preserves permissions; Client VPN provides secure remote access and downloads.

**Incorrect:**
- A: Public EC2, S3 signed URLs, or public endpoints don’t integrate with AD or meet security requirements.
- C: Public EC2, S3 signed URLs, or public endpoints don’t integrate with AD or meet security requirements.
- D: Public EC2, S3 signed URLs, or public endpoints don’t integrate with AD or meet security requirements.


---

---

### Question #334

A company wants to give a customer the ability to use on-premises Microsoft Active Directory to download files that are stored in Amazon S3. The customer’s application uses an SFTP client to download the files. Which solution will meet these requirements with the LEAST operational overhead and no changes to the customer’s application?

- A. Set up AWS Transfer Family with SFTP for Amazon S3. Configure integrated Active Directory authentication.

- B. Set up AWS Database Migration Service (AWS DMS) to synchronize the on-premises client with Amazon S3. Configure integrated Active Directory authentication.

- C. Set up AWS DataSync to synchronize between the on-premises location and the S3 location by using AWS IAM Identity Center (AWS Single Sign-On).

- D. Set up a Windows Amazon EC2 instance with SFTP to connect the on-premises client with Amazon S3. Integrate AWS Identity and Access Management (IAM).

**Correct:** A
**Why:** AWS Transfer Family SFTP for S3 with AD auth meets two‑way SFTP needs without app changes.

**Incorrect:**
- B: DMS/DataSync/IAM‑backed EC2 SFTP add ops or don’t integrate AD SFTP properly.
- C: DMS/DataSync/IAM‑backed EC2 SFTP add ops or don’t integrate AD SFTP properly.
- D: DMS/DataSync/IAM‑backed EC2 SFTP add ops or don’t integrate AD SFTP properly.


---

---

### Question #412

An image-hosting company stores its objects in Amazon S3 buckets. The company wants to avoid accidental exposure of the objects in the S3 buckets to the public. All S3 objects in the entire AWS account need to remain private. Which solution will meet these requirements?

- A. Use Amazon GuardDuty to monitor S3 bucket policies. Create an automatic remediation action rule that uses an AWS Lambda function to remediate any change that makes the objects public.

- B. Use AWS Trusted Advisor to nd publicly accessible S3 buckets. Configure email notications in Trusted Advisor when a change is detected. Manually change the S3 bucket policy if it allows public access.

- C. Use AWS Resource Access Manager to nd publicly accessible S3 buckets. Use Amazon Simple Notification Service (Amazon SNS) to invoke an AWS Lambda function when a change is detected. Deploy a Lambda function that programmatically remediates the change.

- D. Use the S3 Block Public Access feature on the account level. Use AWS Organizations to create a service control policy (SCP) that prevents IAM users from changing the setting. Apply the SCP to the account.

**Correct:** D
**Why:** Enable S3 Block Public Access at the account level and lock it with an SCP to prevent changes.

**Incorrect:**
- A: Detection and manual remediation have higher risk/overhead.
- B: Detection and manual remediation have higher risk/overhead.
- C: Detection and manual remediation have higher risk/overhead.


---

---

### Question #419

A company uses AWS Organizations with all features enabled and runs multiple Amazon EC2 workloads in the ap-southeast-2 Region. The company has a service control policy (SCP) that prevents any resources from being created in any other Region. A security policy requires the company to encrypt all data at rest. An audit discovers that employees have created Amazon Elastic Block Store (Amazon EBS) volumes for EC2 instances without encrypting the volumes. The company wants any new EC2 instances that any IAM user or root user launches in ap-southeast-2 to use encrypted EBS volumes. The company wants a solution that will have minimal effect on employees who create EBS volumes. Which combination of steps will meet these requirements? (Choose two.)

- A. In the Amazon EC2 console, select the EBS encryption account attribute and dene a default encryption key.

- B. Create an IAM permission boundary. Attach the permission boundary to the root organizational unit (OU). Dene the boundary to deny the ec2:CreateVolume action when the ec2:Encrypted condition equals false.

- C. Create an SCP. Attach the SCP to the root organizational unit (OU). Dene the SCP to deny the ec2:CreateVolume action whenthe ec2:Encrypted condition equals false.

- D. Update the IAM policies for each account to deny the ec2:CreateVolume action when the ec2:Encrypted condition equals false.

E. In the Organizations management account, specify the Default EBS volume encryption setting.

**Correct:** A, C
**Why:** Enable EBS default encryption in the Region/account and enforce with an SCP that denies unencrypted volume creation.

**Incorrect:**
- B: Permission boundaries or per‑account policy updates are heavier and easier to bypass than an SCP.
- D: Permission boundaries or per‑account policy updates are heavier and easier to bypass than an SCP.
- E: No organization‑level default EBS encryption setting to apply globally.


---

---

### Question #433

A company is running its production and nonproduction environment workloads in multiple AWS accounts. The accounts are in an organization in AWS Organizations. The company needs to design a solution that will prevent the modication of cost usage tags. Which solution will meet these requirements?

- A. Create a custom AWS Cong rule to prevent tag modication except by authorized principals.

- B. Create a custom trail in AWS CloudTrail to prevent tag modication.

- C. Create a service control policy (SCP) to prevent tag modication except by authorized principals.

- D. Create custom Amazon CloudWatch logs to prevent tag modication.

**Correct:** C
**Why:** Use an SCP in Organizations to centrally prevent tag modification except for authorized principals.

**Incorrect:**
- A: Config/CloudTrail/CloudWatch cannot enforce prevention like SCPs.
- B: Config/CloudTrail/CloudWatch cannot enforce prevention like SCPs.
- D: Config/CloudTrail/CloudWatch cannot enforce prevention like SCPs.


---

---

### Question #455

A company uses AWS Organizations. The company wants to operate some of its AWS accounts with different budgets. The company wants to receive alerts and automatically prevent provisioning of additional resources on AWS accounts when the allocated budget threshold is met during a specic period. Which combination of solutions will meet these requirements? (Choose three.)

- A. Use AWS Budgets to create a budget. Set the budget amount under the Cost and Usage Reports section of the required AWS accounts.

- B. Use AWS Budgets to create a budget. Set the budget amount under the Billing dashboards of the required AWS accounts.

- C. Create an IAM user for AWS Budgets to run budget actions with the required permissions.

- D. Create an IAM role for AWS Budgets to run budget actions with the required permissions.

E. Add an alert to notify the company when each account meets its budget threshold. Add a budget action that selects the IAM identity created with the appropriate cong rule to prevent provisioning of additional resources.

F. Add an alert to notify the company when each account meets its budget threshold. Add a budget action that selects the IAM identity created with the appropriate service control policy (SCP) to prevent provisioning of additional resources.

**Correct:** B, D, F
**Why:** Create budgets in AWS Budgets (B); create an IAM role for Budgets to execute actions (D); use a Budgets action to apply an SCP to prevent further provisioning when thresholds are met (F).

**Incorrect:**
- A: “Cost and Usage Reports” section is not where you set the budget amount.
- C: A user is not ideal; use a role for Budgets actions.
- E: AWS Config is unrelated to Budgets actions.


---

---

### Question #459

A company uses AWS Organizations to run workloads within multiple AWS accounts. A tagging policy adds department tags to AWS resources when the company creates tags. An accounting team needs to determine spending on Amazon EC2 consumption. The accounting team must determine which departments are responsible for the costs regardless ofAWS account. The accounting team has access to AWS Cost Explorer for all AWS accounts within the organization and needs to access all reports from Cost Explorer. Which solution meets these requirements in the MOST operationally ecient way?

- A. From the Organizations management account billing console, activate a user-dened cost allocation tag named department. Create one cost report in Cost Explorer grouping by tag name, and filter by EC2.

- B. From the Organizations management account billing console, activate an AWS-dened cost allocation tag named department. Create one cost report in Cost Explorer grouping by tag name, and filter by EC2.

- C. From the Organizations member account billing console, activate a user-dened cost allocation tag named department. Create one cost report in Cost Explorer grouping by the tag name, and filter by EC2.

- D. From the Organizations member account billing console, activate an AWS-dened cost allocation tag named department. Create one cost report in Cost Explorer grouping by tag name, and filter by EC2.

**Correct:** A
**Why:** Activate the user‑defined cost allocation tag in the management account and build one Cost Explorer report grouped by that tag and filtered to EC2.

**Incorrect:**
- B: “AWS‑defined” cost tag for department does not exist; use your user‑defined tag.
- C: Activations must be in the management account for org‑wide reporting.
- D: Activations must be in the management account for org‑wide reporting.


---

---

### Question #467

A company uses AWS Organizations. A member account has purchased a Compute Savings Plan. Because of changes in the workloads inside the member account, the account no longer receives the full benet of the Compute Savings Plan commitment. The company uses less than 50% of its purchased compute power.

- A. Turn on discount sharing from the Billing Preferences section of the account console in the member account that purchased the Compute Savings Plan.

- B. Turn on discount sharing from the Billing Preferences section of the account console in the company's Organizations management account.

- C. Migrate additional compute workloads from another AWS account to the account that has the Compute Savings Plan.

- D. Sell the excess Savings Plan commitment in the Reserved Instance Marketplace.

**Correct:** B
**Why:** Turn on discount sharing in the management account so unused Savings Plan benefit applies across accounts.

**Incorrect:**
- A: Enabling in a member account is insufficient for org‑wide sharing.
- C: Moving workloads or selling commitments is unnecessary.
- D: Moving workloads or selling commitments is unnecessary.


---

---

### Question #484

A company wants to move from many standalone AWS accounts to a consolidated, multi-account architecture. The company plans to create many new AWS accounts for different business units. The company needs to authenticate access to these AWS accounts by using a centralized corporate directory service. Which combination of actions should a solutions architect recommend to meet these requirements? (Choose two.)

- A. Create a new organization in AWS Organizations with all features turned on. Create the new AWS accounts in the organization.

- B. Set up an Amazon Cognito identity pool. Configure AWS IAM Identity Center (AWS Single Sign-On) to accept Amazon Cognito authentication.

- C. Configure a service control policy (SCP) to manage the AWS accounts. Add AWS IAM Identity Center (AWS Single Sign-On) to AWS Directory Service.

- D. Create a new organization in AWS Organizations. Configure the organization's authentication mechanism to use AWS Directory Service directly.

E. Set up AWS IAM Identity Center (AWS Single Sign-On) in the organization. Configure IAM Identity Center, and integrate it with the company's corporate directory service.

**Correct:** A, E
**Why:** Create an organization with all features and create accounts; configure IAM Identity Center and integrate with the corporate directory for centralized SSO.

**Incorrect:**
- B: Cognito/SCP auth or direct Directory Service auth are not the right setup.
- C: Cognito/SCP auth or direct Directory Service auth are not the right setup.
- D: Cognito/SCP auth or direct Directory Service auth are not the right setup.


---

---

### Question #488

A 4-year-old media company is using the AWS Organizations all features feature set to organize its AWS accounts. According to the company's nance team, the billing information on the member accounts must not be accessible to anyone, including the root user of the member accounts. Which solution will meet these requirements?

- A. Add all nance team users to an IAM group. Attach an AWS managed policy named Billing to the group.

- B. Attach an identity-based policy to deny access to the billing information to all users, including the root user.

- C. Create a service control policy (SCP) to deny access to the billing information. Attach the SCP to the root organizational unit (OU).

- D. Convert from the Organizations all features feature set to the Organizations consolidated billing feature set.

**Correct:** C
**Why:** Apply an SCP to deny billing console/API access in member accounts—even the root user is affected.

**Incorrect:**
- A: IAM policies cannot restrict root; switching to consolidated billing does not solve access control.
- B: IAM policies cannot restrict root; switching to consolidated billing does not solve access control.
- D: IAM policies cannot restrict root; switching to consolidated billing does not solve access control.


---

---

### Question #492

A company has multiple AWS accounts for development work. Some staff consistently use oversized Amazon EC2 instances, which causes the company to exceed the yearly budget for the development accounts. The company wants to centrally restrict the creation of AWS resources in these accounts. Which solution will meet these requirements with the LEAST development effort?

- A. Develop AWS Systems Manager templates that use an approved EC2 creation process. Use the approved Systems Manager templates to provision EC2 instances.

- B. Use AWS Organizations to organize the accounts into organizational units (OUs). Dene and attach a service control policy (SCP) to control the usage of EC2 instance types.

- C. Configure an Amazon EventBridge rule that invokes an AWS Lambda function when an EC2 instance is created. Stop disallowed EC2 instance types.

- D. Set up AWS Service Catalog products for the staff to create the allowed EC2 instance types. Ensure that staff can deploy EC2 instances only by using the Service Catalog products.

**Correct:** B
**Why:** Organize accounts and attach an SCP that denies disallowed EC2 instance types (least effort, central control).

**Incorrect:**
- A: Heavier build or reactive after creation.
- C: Heavier build or reactive after creation.
- D: Heavier build or reactive after creation.


---

## AWS Secrets Manager

### Question #291

A media company uses Amazon CloudFront for its publicly available streaming video content. The company wants to secure the video content that is hosted in Amazon S3 by controlling who has access. Some of the company’s users are using a custom HTTP client that does not support cookies. Some of the company’s users are unable to change the hardcoded URLs that they are using for access. Which services or methods will meet these requirements with the LEAST impact to the users? (Choose two.)

- A. Signed cookies

- B. Signed URLs

- C. AWS AppSync

- D. JSON Web Token (JWT)

E. AWS Secrets Manager

**Correct:** A, B
**Why:** Use signed cookies where clients support cookies and signed URLs where they don’t or URLs are hardcoded.

**Incorrect:**
- C: AppSync/JWT/Secrets Manager do not solve CloudFront authorization distribution.
- D: AppSync/JWT/Secrets Manager do not solve CloudFront authorization distribution.
- E: AppSync/JWT/Secrets Manager do not solve CloudFront authorization distribution.


---

---

### Question #330

A company is planning to store data on Amazon RDS DB instances. The company must encrypt the data at rest. What should a solutions architect do to meet this requirement?

- A. Create a key in AWS Key Management Service (AWS KMS). Enable encryption for the DB instances.

- B. Create an encryption key. Store the key in AWS Secrets Manager. Use the key to encrypt the DB instances.

- C. Generate a certicate in AWS Certicate Manager (ACM). Enable SSL/TLS on the DB instances by using the certicate.

- D. Generate a certicate in AWS Identity and Access Management (IAM). Enable SSL/TLS on the DB instances by using the certicate.

**Correct:** A
**Why:** Use a KMS CMK and enable at‑rest encryption for RDS at creation/restore from encrypted snapshot.

**Incorrect:**
- B: Secrets Manager/ACM/IAM certs don’t encrypt DB storage at rest.
- C: Secrets Manager/ACM/IAM certs don’t encrypt DB storage at rest.
- D: Secrets Manager/ACM/IAM certs don’t encrypt DB storage at rest.


---

---

### Question #336

A company hosts a multi-tier web application that uses an Amazon Aurora MySQL DB cluster for storage. The application tier is hosted on Amazon EC2 instances. The company’s IT security guidelines mandate that the database credentials be encrypted and rotated every 14 days. What should a solutions architect do to meet this requirement with the LEAST operational effort?

- A. Create a new AWS Key Management Service (AWS KMS) encryption key. Use AWS Secrets Manager to create a new secret that uses the KMS key with the appropriate credentials. Associate the secret with the Aurora DB cluster. Configure a custom rotation period of 14 days.

- B. Create two parameters in AWS Systems Manager Parameter Store: one for the user name as a string parameter and one that uses the SecureString type for the password. Select AWS Key Management Service (AWS KMS) encryption for the password parameter, and load these parameters in the application tier. Implement an AWS Lambda function that rotates the password every 14 days.

- C. Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system in all EC2 instances of the application tier. Restrict the access to the file on the file system so that the application can read the file and that only super users can modify the file. Implement an AWS Lambda function that rotates the key in Aurora every 14 days and writes new credentials into the file.

- D. Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon S3 bucket that the application uses to load the credentials. Download the file to the application regularly to ensure that the correct credentials are used. Implement an AWS Lambda function that rotates the Aurora credentials every 14 days and uploads these credentials to the file in the S3 bucket.

**Correct:** A
**Why:** Secrets Manager with KMS‑encrypted secret and built‑in rotation integrates with Aurora; set 14‑day rotation.

**Incorrect:**
- B: Parameter Store/EFS/S3 solutions require more glue code and ops.
- C: Parameter Store/EFS/S3 solutions require more glue code and ops.
- D: Parameter Store/EFS/S3 solutions require more glue code and ops.


---

---

### Question #339

A company has a custom application with embedded credentials that retrieves information from an Amazon RDS MySQL DB instance. Management says the application must be made more secure with the least amount of programming effort. What should a solutions architect do to meet these requirements?

- A. Use AWS Key Management Service (AWS KMS) to create keys. Configure the application to load the database credentials from AWS KMS. Enable automatic key rotation.

- B. Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS Secrets Manager. Configure the application to load the database credentials from Secrets Manager. Create an AWS Lambda function that rotates the credentials in Secret Manager.

- C. Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS Secrets Manager. Configure the application to load the database credentials from Secrets Manager. Set up a credentials rotation schedule for the application user in the RDS for MySQL database using Secrets Manager.

- D. Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS Systems Manager Parameter Store. Configure the application to load the database credentials from Parameter Store. Set up a credentials rotation schedule for the application user in the RDS for MySQL database using Parameter Store.

**Correct:** C
**Why:** Store DB creds in Secrets Manager and enable automated rotation for RDS MySQL with minimal code changes.

**Incorrect:**
- A: KMS or Parameter Store lack integrated RDS rotation automation.
- B: Lambda rotation is not needed; use built‑in rotation.
- D: KMS or Parameter Store lack integrated RDS rotation automation.


---

## AWS Snow Family

### Question #293

A company has an on-premises volume backup solution that has reached its end of life. The company wants to use AWS as part of a new backup solution and wants to maintain local access to all the data while it is backed up on AWS. The company wants to ensure that the data backed up on AWS is automatically and securely transferred. Which solution meets these requirements?

- A. Use AWS Snowball to migrate data out of the on-premises solution to Amazon S3. Configure on-premises systems to mount the Snowball S3 endpoint to provide local access to the data.

- B. Use AWS Snowball Edge to migrate data out of the on-premises solution to Amazon S3. Use the Snowball Edge file interface to provide on- premises systems with local access to the data.

- C. Use AWS Storage Gateway and configure a cached volume gateway. Run the Storage Gateway software appliance on premises and configure a percentage of data to cache locally. Mount the gateway storage volumes to provide local access to the data.

- D. Use AWS Storage Gateway and configure a stored volume gateway. Run the Storage Gateway software appliance on premises and map the gateway storage volumes to on-premises storage. Mount the gateway storage volumes to provide local access to the data.

**Correct:** C
**Why:** Volume Gateway cached volumes keep recent data local, store full data in S3, and throttle bandwidth; provides local access.

**Incorrect:**
- A: Snow devices are one‑time migrations, not ongoing backups with local access.
- B: Snow devices are one‑time migrations, not ongoing backups with local access.
- D: Stored volumes keep full copy local, not desired here to minimize on‑prem scale.


---

---

### Question #301

A university research laboratory needs to migrate 30 TB of data from an on-premises Windows file server to Amazon FSx for Windows File Server. The laboratory has a 1 Gbps network link that many other departments in the university share. The laboratory wants to implement a data migration service that will maximize the performance of the data transfer. However, the laboratory needs to be able to control the amount of bandwidth that the service uses to minimize the impact on other departments. The data migration must take place within the next 5 days. Which AWS solution will meet these requirements?

- A. AWS Snowcone

- B. Amazon FSx File Gateway

- C. AWS DataSync

- D. AWS Transfer Family

**Correct:** C
**Why:** AWS DataSync supports bandwidth throttling and high‑performance transfer to FSx Windows within the 5‑day window.

**Incorrect:**
- A: Snowcone/FSx File Gateway/Transfer Family are not optimal for bulk controlled‑bandwidth migration.
- B: Snowcone/FSx File Gateway/Transfer Family are not optimal for bulk controlled‑bandwidth migration.
- D: Snowcone/FSx File Gateway/Transfer Family are not optimal for bulk controlled‑bandwidth migration.


---

---

### Question #304

A company recently created a disaster recovery site in a different AWS Region. The company needs to transfer large amounts of data back and forth between NFS file systems in the two Regions on a periodic basis. Which solution will meet these requirements with the LEAST operational overhead?

- A. Use AWS DataSync.

- B. Use AWS Snowball devices.

- C. Set up an SFTP server on Amazon EC2.

- D. Use AWS Database Migration Service (AWS DMS).

**Correct:** A
**Why:** DataSync moves data between NFS file systems across Regions with minimal ops and scheduling.

**Incorrect:**
- B: Snowball/SFTP/DMS don’t fit ongoing NFS replication.
- C: Snowball/SFTP/DMS don’t fit ongoing NFS replication.
- D: Snowball/SFTP/DMS don’t fit ongoing NFS replication.


---

---

### Question #331

A company must migrate 20 TB of data from a data center to the AWS Cloud within 30 days. The company’s network bandwidth is limited to 15 Mbps and cannot exceed 70% utilization. What should a solutions architect do to meet these requirements?

- A. Use AWS Snowball.

- B. Use AWS DataSync.

- C. Use a secure VPN connection.

- D. Use Amazon S3 Transfer Acceleration.

**Correct:** A
**Why:** Snowball devices can move 20 TB within the time/bandwidth constraints cost‑effectively.

**Incorrect:**
- B: DataSync/VPN/Transfer Acceleration won’t meet time/bandwidth constraints.
- C: DataSync/VPN/Transfer Acceleration won’t meet time/bandwidth constraints.
- D: DataSync/VPN/Transfer Acceleration won’t meet time/bandwidth constraints.


---

---

### Question #398

A company needs to transfer 600 TB of data from its on-premises network-attached storage (NAS) system to the AWS Cloud. The data transfer must be complete within 2 weeks. The data is sensitive and must be encrypted in transit. The company’s internet connection can support an upload speed of 100 Mbps. Which solution meets these requirements MOST cost-effectively?

- A. Use Amazon S3 multi-part upload functionality to transfer the files over HTTPS.

- B. Create a VPN connection between the on-premises NAS system and the nearest AWS Region. Transfer the data over the VPN connection.

- C. Use the AWS Snow Family console to order several AWS Snowball Edge Storage Optimized devices. Use the devices to transfer the data to Amazon S3.

- D. Set up a 10 Gbps AWS Direct Connect connection between the company location and the nearest AWS Region. Transfer the data over a VPN connection into the Region to store the data in Amazon S3.

**Correct:** C
**Why:** Snowball Edge devices securely transfer large datasets (600 TB) within the required 2 weeks with encryption in transit and at rest.

**Incorrect:**
- A: 100 Mbps is far too slow for 600 TB in 2 weeks.
- B: 100 Mbps is far too slow for 600 TB in 2 weeks.
- D: Provisioning DX for a one‑off transfer is costly/time‑consuming.


---

---

### Question #426

A company needs to store data from its healthcare application. The application’s data frequently changes. A new regulation requires audit access at all levels of the stored data. The company hosts the application on an on-premises infrastructure that is running out of storage capacity. A solutions architect must securely migrate the existing data to AWS while satisfying the new regulation. Which solution will meet these requirements?

- A. Use AWS DataSync to move the existing data to Amazon S3. Use AWS CloudTrail to log data events.

- B. Use AWS Snowcone to move the existing data to Amazon S3. Use AWS CloudTrail to log management events.

- C. Use Amazon S3 Transfer Acceleration to move the existing data to Amazon S3. Use AWS CloudTrail to log data events.

- D. Use AWS Storage Gateway to move the existing data to Amazon S3. Use AWS CloudTrail to log management events.

**Correct:** A
**Why:** DataSync migrates data securely to S3; CloudTrail data events provide audit access to object‑level operations.

**Incorrect:**
- B: Snowcone is for small/edge; also management events alone are insufficient.
- C: Transfer Acceleration is for uploads from clients, not bulk secure migration with auditing.
- D: Storage Gateway is hybrid access, not best for migration plus data‑event auditing.


---

---

### Question #435

A company needs to migrate a MySQL database from its on-premises data center to AWS within 2 weeks. The database is 20 TB in size. The company wants to complete the migration with minimal downtime. Which solution will migrate the database MOST cost-effectively?

- A. Order an AWS Snowball Edge Storage Optimized device. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with replication of ongoing changes. Send the Snowball Edge device to AWS to nish the migration and continue the ongoing replication.

- B. Order an AWS Snowmobile vehicle. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with ongoing changes. Send the Snowmobile vehicle back to AWS to nish the migration and continue the ongoing replication.

- C. Order an AWS Snowball Edge Compute Optimized with GPU device. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with ongoing changes. Send the Snowball device to AWS to nish the migration and continue the ongoing replication

- D. Order a 1 GB dedicated AWS Direct Connect connection to establish a connection with the data center. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with replication of ongoing changes.

**Correct:** A
**Why:** Use Snowball Edge to seed 20 TB quickly and DMS for CDC to minimize downtime cost‑effectively.

**Incorrect:**
- B: Snowmobile is for exabyte‑scale.
- C: Compute Optimized with GPU is unnecessary.
- D: New DX is costly/time‑consuming and not needed for 20 TB in 2 weeks.


---

---

### Question #445

A company is storing 700 terabytes of data on a large network-attached storage (NAS) system in its corporate data center. The company has a hybrid environment with a 10 Gbps AWS Direct Connect connection. After an audit from a regulator, the company has 90 days to move the data to the cloud. The company needs to move the data eciently and without disruption. The company still needs to be able to access and update the data during the transfer window. Which solution will meet these requirements?

- A. Create an AWS DataSync agent in the corporate data center. Create a data transfer task Start the transfer to an Amazon S3 bucket.

- B. Back up the data to AWS Snowball Edge Storage Optimized devices. Ship the devices to an AWS data center. Mount a target Amazon S3 bucket on the on-premises file system.

- C. Use rsync to copy the data directly from local storage to a designated Amazon S3 bucket over the Direct Connect connection.

- D. Back up the data on tapes. Ship the tapes to an AWS data center. Mount a target Amazon S3 bucket on the on-premises file system.

**Correct:** A
**Why:** DataSync over DX moves large NAS datasets efficiently while allowing ongoing access and updates during the window.

**Incorrect:**
- B: Snow/tapes disrupt access during transfer and add logistics.
- C: rsync over DX for 700 TB is error‑prone and operationally heavy.
- D: Snow/tapes disrupt access during transfer and add logistics.


---

---

### Question #482

A company wants to migrate 100 GB of historical data from an on-premises location to an Amazon S3 bucket. The company has a 100 megabits per second (Mbps) internet connection on premises. The company needs to encrypt the data in transit to the S3 bucket. The company will store new data directly in Amazon S3. Which solution will meet these requirements with the LEAST operational overhead?

- A. Use the s3 sync command in the AWS CLI to move the data directly to an S3 bucket

- B. Use AWS DataSync to migrate the data from the on-premises location to an S3 bucket

- C. Use AWS Snowball to move the data to an S3 bucket

- D. Set up an IPsec VPN from the on-premises location to AWS. Use the s3 cp command in the AWS CLI to move the data directly to an S3 bucket

**Correct:** B
**Why:** DataSync securely transfers and encrypts data in transit with minimal setup and monitoring.

**Incorrect:**
- A: CLI/VPN approaches are more manual.
- C: Snowball is overkill for 100 GB and adds logistics.
- D: CLI/VPN approaches are more manual.


---

---

### Question #496

A company uses on-premises servers to host its applications. The company is running out of storage capacity. The applications use both block storage and NFS storage. The company needs a high-performing solution that supports local caching without re-architecting its existing applications. Which combination of actions should a solutions architect take to meet these requirements? (Choose two.)

- A. Mount Amazon S3 as a file system to the on-premises servers.

- B. Deploy an AWS Storage Gateway file gateway to replace NFS storage.

- C. Deploy AWS Snowball Edge to provision NFS mounts to on-premises servers.

- D. Deploy an AWS Storage Gateway volume gateway to replace the block storage.

E. Deploy Amazon Elastic File System (Amazon EFS) volumes and mount them to on-premises servers.

**Correct:** B, D
**Why:** File Gateway replaces NFS with local cache; Volume Gateway replaces block storage with cached volumes—no app re‑architecture.

**Incorrect:**
- A: S3 is not mounted natively.
- C: Snowball Edge is for migration, not ongoing storage.
- E: EFS is NFS in AWS, not a cached on‑prem replacement.


---

---

### Question #500

A company has multiple Windows file servers on premises. The company wants to migrate and consolidate its files into an Amazon FSx for Windows File Server file system. File permissions must be preserved to ensure that access rights do not change. Which solutions will meet these requirements? (Choose two.)

- A. Deploy AWS DataSync agents on premises. Schedule DataSync tasks to transfer the data to the FSx for Windows File Server file system.

- B. Copy the shares on each file server into Amazon S3 buckets by using the AWS CLI. Schedule AWS DataSync tasks to transfer the data to the FSx for Windows File Server file system.

- C. Remove the drives from each file server. Ship the drives to AWS for import into Amazon S3. Schedule AWS DataSync tasks to transfer the data to the FSx for Windows File Server file system.

- D. Order an AWS Snowcone device. Connect the device to the on-premises network. Launch AWS DataSync agents on the device. Schedule DataSync tasks to transfer the data to the FSx for Windows File Server file system.

E. Order an AWS Snowball Edge Storage Optimized device. Connect the device to the on-premises network. Copy data to the device by using the AWS CLI. Ship the device back to AWS for import into Amazon S3. Schedule AWS DataSync tasks to transfer the data to the FSx for Windows File Server file system.

**Correct:** A, E
**Why:** DataSync agents preserve metadata/ACLs when migrating to FSx for Windows; for large data sets, Snowball Edge to S3 plus DataSync to FSx is efficient.

**Incorrect:**
- B: Copying to S3 with CLI loses Windows ACLs/metadata.
- C: Drives/Snowcone are impractical for large multi‑server migrations.
- D: Drives/Snowcone are impractical for large multi‑server migrations.


---

## AWS Systems Manager

### Question #316

A company uses an Amazon EC2 instance to run a script to poll for and process messages in an Amazon Simple Queue Service (Amazon SQS) queue. The company wants to reduce operational costs while maintaining its ability to process a growing number of messages that are added to the queue. What should a solutions architect recommend to meet these requirements?

- A. Increase the size of the EC2 instance to process messages faster.

- B. Use Amazon EventBridge to turn off the EC2 instance when the instance is underutilized.

- C. Migrate the script on the EC2 instance to an AWS Lambda function with the appropriate runtime.

- D. Use AWS Systems Manager Run Command to run the script on demand.

**Correct:** C
**Why:** Replace the EC2 poller with a Lambda function subscribed to SQS; scales automatically, lower cost.

**Incorrect:**
- A: Larger instance, turning off, or Run Command do not reduce ops or scale on demand.
- B: Larger instance, turning off, or Run Command do not reduce ops or scale on demand.
- D: Larger instance, turning off, or Run Command do not reduce ops or scale on demand.


---

---

### Question #319

A company has hundreds of Amazon EC2 Linux-based instances in the AWS Cloud. Systems administrators have used shared SSH keys to manage the instances. After a recent audit, the company’s security team is mandating the removal of all shared keys. A solutions architect must design a solution that provides secure access to the EC2 instances. Which solution will meet this requirement with the LEAST amount of administrative overhead?

- A. Use AWS Systems Manager Session Manager to connect to the EC2 instances.

- B. Use AWS Security Token Service (AWS STS) to generate one-time SSH keys on demand.

- C. Allow shared SSH access to a set of bastion instances. Configure all other instances to allow only SSH access from the bastion instances.

- D. Use an Amazon Cognito custom authorizer to authenticate users. Invoke an AWS Lambda function to generate a temporary SSH key.

**Correct:** A
**Why:** Session Manager provides secure, auditable access without SSH keys or open ports.

**Incorrect:**
- B: STS one‑time keys, bastions, or custom auth add overhead.
- C: STS one‑time keys, bastions, or custom auth add overhead.
- D: STS one‑time keys, bastions, or custom auth add overhead.


---

---

### Question #329

A security audit reveals that Amazon EC2 instances are not being patched regularly. A solutions architect needs to provide a solution that will run regular security scans across a large eet of EC2 instances. The solution should also patch the EC2 instances on a regular schedule and provide a report of each instance’s patch status. Which solution will meet these requirements?

- A. Set up Amazon Macie to scan the EC2 instances for software vulnerabilities. Set up a cron job on each EC2 instance to patch the instance on a regular schedule.

- B. Turn on Amazon GuardDuty in the account. Configure GuardDuty to scan the EC2 instances for software vulnerabilities. Set up AWS Systems Manager Session Manager to patch the EC2 instances on a regular schedule.

- C. Set up Amazon Detective to scan the EC2 instances for software vulnerabilities. Set up an Amazon EventBridge scheduled rule to patch the EC2 instances on a regular schedule.

- D. Turn on Amazon Inspector in the account. Configure Amazon Inspector to scan the EC2 instances for software vulnerabilities. Set up AWS Systems Manager Patch Manager to patch the EC2 instances on a regular schedule.

**Correct:** D
**Why:** Inspector for vulnerability scanning and Systems Manager Patch Manager for scheduled patching with compliance reports.

**Incorrect:**
- A: Macie/GuardDuty/Detective do not patch or scan OS packages.
- B: Macie/GuardDuty/Detective do not patch or scan OS packages.
- C: Macie/GuardDuty/Detective do not patch or scan OS packages.


---

---

### Question #336

A company hosts a multi-tier web application that uses an Amazon Aurora MySQL DB cluster for storage. The application tier is hosted on Amazon EC2 instances. The company’s IT security guidelines mandate that the database credentials be encrypted and rotated every 14 days. What should a solutions architect do to meet this requirement with the LEAST operational effort?

- A. Create a new AWS Key Management Service (AWS KMS) encryption key. Use AWS Secrets Manager to create a new secret that uses the KMS key with the appropriate credentials. Associate the secret with the Aurora DB cluster. Configure a custom rotation period of 14 days.

- B. Create two parameters in AWS Systems Manager Parameter Store: one for the user name as a string parameter and one that uses the SecureString type for the password. Select AWS Key Management Service (AWS KMS) encryption for the password parameter, and load these parameters in the application tier. Implement an AWS Lambda function that rotates the password every 14 days.

- C. Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system in all EC2 instances of the application tier. Restrict the access to the file on the file system so that the application can read the file and that only super users can modify the file. Implement an AWS Lambda function that rotates the key in Aurora every 14 days and writes new credentials into the file.

- D. Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon S3 bucket that the application uses to load the credentials. Download the file to the application regularly to ensure that the correct credentials are used. Implement an AWS Lambda function that rotates the Aurora credentials every 14 days and uploads these credentials to the file in the S3 bucket.

**Correct:** A
**Why:** Secrets Manager with KMS‑encrypted secret and built‑in rotation integrates with Aurora; set 14‑day rotation.

**Incorrect:**
- B: Parameter Store/EFS/S3 solutions require more glue code and ops.
- C: Parameter Store/EFS/S3 solutions require more glue code and ops.
- D: Parameter Store/EFS/S3 solutions require more glue code and ops.


---

---

### Question #339

A company has a custom application with embedded credentials that retrieves information from an Amazon RDS MySQL DB instance. Management says the application must be made more secure with the least amount of programming effort. What should a solutions architect do to meet these requirements?

- A. Use AWS Key Management Service (AWS KMS) to create keys. Configure the application to load the database credentials from AWS KMS. Enable automatic key rotation.

- B. Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS Secrets Manager. Configure the application to load the database credentials from Secrets Manager. Create an AWS Lambda function that rotates the credentials in Secret Manager.

- C. Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS Secrets Manager. Configure the application to load the database credentials from Secrets Manager. Set up a credentials rotation schedule for the application user in the RDS for MySQL database using Secrets Manager.

- D. Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS Systems Manager Parameter Store. Configure the application to load the database credentials from Parameter Store. Set up a credentials rotation schedule for the application user in the RDS for MySQL database using Parameter Store.

**Correct:** C
**Why:** Store DB creds in Secrets Manager and enable automated rotation for RDS MySQL with minimal code changes.

**Incorrect:**
- A: KMS or Parameter Store lack integrated RDS rotation automation.
- B: Lambda rotation is not needed; use built‑in rotation.
- D: KMS or Parameter Store lack integrated RDS rotation automation.


---

---

### Question #390

A company hosts a three-tier ecommerce application on a eet of Amazon EC2 instances. The instances run in an Auto Scaling group behind an Application Load Balancer (ALB). All ecommerce data is stored in an Amazon RDS for MariaDB Multi-AZ DB instance. The company wants to optimize customer session management during transactions. The application must store session data durably. Which solutions will meet these requirements? (Choose two.)

- A. Turn on the sticky sessions feature (session anity) on the ALB.

- B. Use an Amazon DynamoDB table to store customer session information.

- C. Deploy an Amazon Cognito user pool to manage user session information.

- D. Deploy an Amazon ElastiCache for Redis cluster to store customer session information.

E. Use AWS Systems Manager Application Manager in the application to manage user session information.

**Correct:** A, B
**Why:** Sticky sessions reduce cross‑instance churn during transactions; store session data durably in DynamoDB to survive instance failure.

**Incorrect:**
- C: Cognito manages auth, not app transaction sessions.
- D: ElastiCache Redis alone is not durable.
- E: Systems Manager Application Manager is not for session storage.


---

---

### Question #428

A serverless application uses Amazon API Gateway, AWS Lambda, and Amazon DynamoDB. The Lambda function needs permissions to read and write to the DynamoDB table. Which solution will give the Lambda function access to the DynamoDB table MOST securely?

- A. Create an IAM user with programmatic access to the Lambda function. Attach a policy to the user that allows read and write access to the DynamoDB table. Store the access_key_id and secret_access_key parameters as part of the Lambda environment variables. Ensure that other AWS users do not have read and write access to the Lambda function configuration.

- B. Create an IAM role that includes Lambda as a trusted service. Attach a policy to the role that allows read and write access to the DynamoDB table. Update the configuration of the Lambda function to use the new role as the execution role.

- C. Create an IAM user with programmatic access to the Lambda function. Attach a policy to the user that allows read and write access to the DynamoDB table. Store the access_key_id and secret_access_key parameters in AWS Systems Manager Parameter Store as secure string parameters. Update the Lambda function code to retrieve the secure string parameters before connecting to the DynamoDB table.

- D. Create an IAM role that includes DynamoDB as a trusted service. Attach a policy to the role that allows read and write access from the Lambda function. Update the code of the Lambda function to attach to the new role as an execution role.

**Correct:** B
**Why:** Use an IAM role trusted by Lambda with a policy granting DynamoDB access; attach as the function’s execution role.

**Incorrect:**
- A: Do not use static IAM user creds in Lambda.
- C: Do not use static IAM user creds in Lambda.
- D: DynamoDB is not a trusted principal for the role.


---

---

### Question #454

A company has resources across multiple AWS Regions and accounts. A newly hired solutions architect discovers a previous employee did not provide details about the resources inventory. The solutions architect needs to build and map the relationship details of the various workloads across all accounts. Which solution will meet these requirements in the MOST operationally ecient way?

- A. Use AWS Systems Manager Inventory to generate a map view from the detailed view report.

- B. Use AWS Step Functions to collect workload details. Build architecture diagrams of the workloads manually.

- C. Use Workload Discovery on AWS to generate architecture diagrams of the workloads.

- D. Use AWS X-Ray to view the workload details. Build architecture diagrams with relationships.

**Correct:** C
**Why:** Workload Discovery on AWS automatically builds cross‑account, cross‑Region architecture diagrams with relationships.

**Incorrect:**
- A: Require more manual work or target different problems.
- B: Require more manual work or target different problems.
- D: Require more manual work or target different problems.


---

---

### Question #479

A company is making a prototype of the infrastructure for its new website by manually provisioning the necessary infrastructure. This infrastructure includes an Auto Scaling group, an Application Load Balancer and an Amazon RDS database. After the configuration has been thoroughly validated, the company wants the capability to immediately deploy the infrastructure for development and production use in two Availability Zones in an automated fashion. What should a solutions architect recommend to meet these requirements?

- A. Use AWS Systems Manager to replicate and provision the prototype infrastructure in two Availability Zones

- B. Dene the infrastructure as a template by using the prototype infrastructure as a guide. Deploy the infrastructure with AWS CloudFormation.

- C. Use AWS Cong to record the inventory of resources that are used in the prototype infrastructure. Use AWS Cong to deploy the prototype infrastructure into two Availability Zones.

- D. Use AWS Elastic Beanstalk and configure it to use an automated reference to the prototype infrastructure to automatically deploy new environments in two Availability Zones.

**Correct:** B
**Why:** Define the infra in CloudFormation templates and deploy to multiple AZs automatically.

**Incorrect:**
- A: Systems Manager/Config/Elastic Beanstalk are not ideal for full infra replication here.
- C: Systems Manager/Config/Elastic Beanstalk are not ideal for full infra replication here.
- D: Systems Manager/Config/Elastic Beanstalk are not ideal for full infra replication here.


---

---

### Question #492

A company has multiple AWS accounts for development work. Some staff consistently use oversized Amazon EC2 instances, which causes the company to exceed the yearly budget for the development accounts. The company wants to centrally restrict the creation of AWS resources in these accounts. Which solution will meet these requirements with the LEAST development effort?

- A. Develop AWS Systems Manager templates that use an approved EC2 creation process. Use the approved Systems Manager templates to provision EC2 instances.

- B. Use AWS Organizations to organize the accounts into organizational units (OUs). Dene and attach a service control policy (SCP) to control the usage of EC2 instance types.

- C. Configure an Amazon EventBridge rule that invokes an AWS Lambda function when an EC2 instance is created. Stop disallowed EC2 instance types.

- D. Set up AWS Service Catalog products for the staff to create the allowed EC2 instance types. Ensure that staff can deploy EC2 instances only by using the Service Catalog products.

**Correct:** B
**Why:** Organize accounts and attach an SCP that denies disallowed EC2 instance types (least effort, central control).

**Incorrect:**
- A: Heavier build or reactive after creation.
- C: Heavier build or reactive after creation.
- D: Heavier build or reactive after creation.


---

## AWS WAF & Shield

### Question #315

A company experienced a breach that affected several applications in its on-premises data center. The attacker took advantage of vulnerabilities in the custom applications that were running on the servers. The company is now migrating its applications to run on Amazon EC2 instances. The company wants to implement a solution that actively scans for vulnerabilities on the EC2 instances and sends a report that details the ndings. Which solution will meet these requirements?

- A. Deploy AWS Shield to scan the EC2 instances for vulnerabilities. Create an AWS Lambda function to log any ndings to AWS CloudTrail.

- B. Deploy Amazon Macie and AWS Lambda functions to scan the EC2 instances for vulnerabilities. Log any ndings to AWS CloudTrail.

- C. Turn on Amazon GuardDuty. Deploy the GuardDuty agents to the EC2 instances. Configure an AWS Lambda function to automate the generation and distribution of reports that detail the ndings.

- D. Turn on Amazon Inspector. Deploy the Amazon Inspector agent to the EC2 instances. Configure an AWS Lambda function to automate the generation and distribution of reports that detail the ndings.

**Correct:** D
**Why:** Amazon Inspector scans EC2 for vulnerabilities; use Lambda/automation to generate and distribute reports.

**Incorrect:**
- A: Shield/Macie/GuardDuty are not host vulnerability scanners.
- B: Shield/Macie/GuardDuty are not host vulnerability scanners.
- C: Shield/Macie/GuardDuty are not host vulnerability scanners.


---

---

### Question #327

A solutions architect must secure a VPC network that hosts Amazon EC2 instances. The EC2 instances contain highly sensitive data and run in a private subnet. According to company policy, the EC2 instances that run in the VPC can access only approved third-party software repositories on the internet for software product updates that use the third party’s URL. Other internet trac must be blocked. Which solution meets these requirements?

- A. Update the route table for the private subnet to route the outbound trac to an AWS Network Firewall firewall. Configure domain list rule groups.

- B. Set up an AWS WAF web ACL. Create a custom set of rules that filter trac requests based on source and destination IP address range sets.

- C. Implement strict inbound security group rules. Configure an outbound rule that allows trac only to the authorized software repositories on the internet by specifying the URLs.

- D. Configure an Application Load Balancer (ALB) in front of the EC2 instances. Direct all outbound trac to the ALB. Use a URL-based rule listener in the ALB’s target group for outbound access to the internet.

**Correct:** A
**Why:** AWS Network Firewall with domain list rules provides egress filtering to approved repositories from private subnets.

**Incorrect:**
- B: WAF is for HTTP(S) to apps; SGs cannot filter by domain/URL; ALB is inbound.
- C: WAF is for HTTP(S) to apps; SGs cannot filter by domain/URL; ALB is inbound.
- D: WAF is for HTTP(S) to apps; SGs cannot filter by domain/URL; ALB is inbound.


---

---

### Question #340

A media company hosts its website on AWS. The website application’s architecture includes a eet of Amazon EC2 instances behind an Application Load Balancer (ALB) and a database that is hosted on Amazon Aurora. The company’s cybersecurity team reports that the application is vulnerable to SQL injection. How should the company resolve this issue?

- A. Use AWS WAF in front of the ALB. Associate the appropriate web ACLs with AWS WAF.

- B. Create an ALB listener rule to reply to SQL injections with a xed response.

- C. Subscribe to AWS Shield Advanced to block all SQL injection attempts automatically.

- D. Set up Amazon Inspector to block all SQL injection attempts automatically.

**Correct:** A
**Why:** Put WAF in front of ALB and enable managed rules for SQLi to mitigate vulnerabilities quickly.

**Incorrect:**
- B: ALB listener responses or Shield/Inspector don’t address SQLi at the app layer.
- C: ALB listener responses or Shield/Inspector don’t address SQLi at the app layer.
- D: ALB listener responses or Shield/Inspector don’t address SQLi at the app layer.


---

---

### Question #366

A company’s web application consists of an Amazon API Gateway API in front of an AWS Lambda function and an Amazon DynamoDB database. The Lambda function handles the business logic, and the DynamoDB table hosts the data. The application uses Amazon Cognito user pools to identify the individual users of the application. A solutions architect needs to update the application so that only users who have a subscription can access premium content. Which solution will meet this requirement with the LEAST operational overhead?

- A. Enable API caching and throttling on the API Gateway API.

- B. Set up AWS WAF on the API Gateway API. Create a rule to filter users who have a subscription.

- C. Apply ne-grained IAM permissions to the premium content in the DynamoDB table.

- D. Implement API usage plans and API keys to limit the access of users who do not have a subscription.

**Correct:** D
**Why:** API Gateway usage plans and API keys provide an easy way to gate/limit access for non‑subscribers with low operational overhead.

**Incorrect:**
- A: Caching/throttling don’t enforce subscription access.
- B: WAF filters traffic patterns, not subscription entitlements.
- C: Fine‑grained IAM on DynamoDB is complex and not aligned with Cognito user pool tokens alone.


---

---

### Question #382

A company has a three-tier application on AWS that ingests sensor data from its users’ devices. The trac ows through a Network Load Balancer (NLB), then to Amazon EC2 instances for the web tier, and nally to EC2 instances for the application tier. The application tier makes calls to a database. What should a solutions architect do to improve the security of the data in transit?

- A. Configure a TLS listener. Deploy the server certicate on the NLB.

- B. Configure AWS Shield Advanced. Enable AWS WAF on the NLB.

- C. Change the load balancer to an Application Load Balancer (ALB). Enable AWS WAF on the ALB.

- D. Encrypt the Amazon Elastic Block Store (Amazon EBS) volume on the EC2 instances by using AWS Key Management Service (AWS KMS).

**Correct:** A
**Why:** Add a TLS listener and deploy certs on the NLB to encrypt data in transit from clients to the load balancer and to targets as needed.

**Incorrect:**
- B: Shield/WAF protect against attacks but don’t encrypt traffic.
- C: ALB is fine for HTTP/HTTPS but switching is unnecessary; TLS on NLB meets the need.
- D: EBS encryption is at rest, not in transit.


---

---

### Question #396

A company has implemented a self-managed DNS service on AWS. The solution consists of the following: • Amazon EC2 instances in different AWS Regions • Endpoints of a standard accelerator in AWS Global Accelerator The company wants to protect the solution against DDoS attacks. What should a solutions architect do to meet this requirement?

- A. Subscribe to AWS Shield Advanced. Add the accelerator as a resource to protect.

- B. Subscribe to AWS Shield Advanced. Add the EC2 instances as resources to protect.

- C. Create an AWS WAF web ACL that includes a rate-based rule. Associate the web ACL with the accelerator.

- D. Create an AWS WAF web ACL that includes a rate-based rule. Associate the web ACL with the EC2 instances.

**Correct:** A
**Why:** Shield Advanced protects Global Accelerator resources against DDoS; add the accelerator as a protected resource.

**Incorrect:**
- B: Protecting individual EC2 instances misses the accelerator edge.
- C: AWS WAF rate rules do not mitigate volumetric DDoS at the network layer.
- D: AWS WAF rate rules do not mitigate volumetric DDoS at the network layer.


---

---

### Question #399

A nancial company hosts a web application on AWS. The application uses an Amazon API Gateway Regional API endpoint to give users the ability to retrieve current stock prices. The company’s security team has noticed an increase in the number of API requests. The security team is concerned that HTTP ood attacks might take the application oine. A solutions architect must design a solution to protect the application from this type of attack. Which solution meets these requirements with the LEAST operational overhead?

- A. Create an Amazon CloudFront distribution in front of the API Gateway Regional API endpoint with a maximum TTL of 24 hours.

- B. Create a Regional AWS WAF web ACL with a rate-based rule. Associate the web ACL with the API Gateway stage.

- C. Use Amazon CloudWatch metrics to monitor the Count metric and alert the security team when the predened rate is reached.

- D. Create an Amazon CloudFront distribution with Lambda@Edge in front of the API Gateway Regional API endpoint. Create an AWS Lambda function to block requests from IP addresses that exceed the predened rate.

**Correct:** B
**Why:** A Regional WAF web ACL with a rate‑based rule attached to the API Gateway stage mitigates HTTP flood attacks with low ops.

**Incorrect:**
- A: CloudFront alone doesn’t handle rate limiting for Regional APIs.
- C: Monitoring doesn’t mitigate attacks.
- D: Lambda@Edge scripting is more complex and operationally heavy.


---

---

### Question #437

A company operates an ecommerce website on Amazon EC2 instances behind an Application Load Balancer (ALB) in an Auto Scaling group. The site is experiencing performance issues related to a high request rate from illegitimate external systems with changing IP addresses. The security team is worried about potential DDoS attacks against the website. The company must block the illegitimate incoming requests in a way that has a minimal impact on legitimate users. What should a solutions architect recommend?

- A. Deploy Amazon Inspector and associate it with the ALB.

- B. Deploy AWS WAF, associate it with the ALB, and configure a rate-limiting rule.

- C. Deploy rules to the network ACLs associated with the ALB to block the incomingtrac.

- D. Deploy Amazon GuardDuty and enable rate-limiting protection when conguring GuardDuty.

**Correct:** B
**Why:** Attach AWS WAF to the ALB and configure rate‑based rules to throttle illegitimate request floods while allowing legitimate users.

**Incorrect:**
- A: Inspector/GuardDuty do not mitigate at the edge.
- C: NACLs are coarse and static; IPs change.
- D: Inspector/GuardDuty do not mitigate at the edge.


---

---

### Question #473

A company hosts a website on Amazon EC2 instances behind an Application Load Balancer (ALB). The website serves static content. Website trac is increasing, and the company is concerned about a potential increase in cost.

- A. Create an Amazon CloudFront distribution to cache state files at edge locations

- B. Create an Amazon ElastiCache cluster. Connect the ALB to the ElastiCache cluster to serve cached files

- C. Create an AWS WAF web ACL and associate it with the ALB. Add a rule to the web ACL to cache static files

- D. Create a second ALB in an alternative AWS Region. Route user trac to the closest Region to minimize data transfer costs

**Correct:** A
**Why:** CloudFront caches static files at edge locations, reducing ALB/EC2 load and cost.

**Incorrect:**
- B: ALB cannot connect directly to ElastiCache to serve files.
- C: WAF is for filtering, not caching.
- D: Adding a second ALB/Region doesn’t address caching/cost.


---

## Amazon API Gateway

### Question #272

A company serves a dynamic website from a eet of Amazon EC2 instances behind an Application Load Balancer (ALB). The website needs to support multiple languages to serve customers around the world. The website’s architecture is running in the us-west-1 Region and is exhibiting high request latency for users that are located in other parts of the world. The website needs to serve requests quickly and eciently regardless of a user’s location. However, the company does not want to recreate the existing architecture across multiple Regions. What should a solutions architect do to meet these requirements?

- A. Replace the existing architecture with a website that is served from an Amazon S3 bucket. Configure an Amazon CloudFront distribution with the S3 bucket as the origin. Set the cache behavior settings to cache based on the Accept-Language request header.

- B. Configure an Amazon CloudFront distribution with the ALB as the origin. Set the cache behavior settings to cache based on the Accept- Language request header.

- C. Create an Amazon API Gateway API that is integrated with the ALB. Configure the API to use the HTTP integration type. Set up an API Gateway stage to enable the API cache based on the Accept-Language request header.

- D. Launch an EC2 instance in each additional Region and configure NGINX to act as a cache server for that Region. Put all the EC2 instances and the ALB behind an Amazon Route 53 record set with a geolocation routing policy.

**Correct:** B
**Why:** CloudFront with ALB origin and cache based on Accept‑Language provides low latency globally without multi‑Region.

**Incorrect:**
- A: S3 static site or regional NGINX cache adds complexity or doesn’t fit dynamic origin.
- C: S3 static site or regional NGINX cache adds complexity or doesn’t fit dynamic origin.
- D: S3 static site or regional NGINX cache adds complexity or doesn’t fit dynamic origin.


---

---

### Question #285

A company hosts its static website by using Amazon S3. The company wants to add a contact form to its webpage. The contact form will have dynamic server-side components for users to input their name, email address, phone number, and user message. The company anticipates that there will be fewer than 100 site visits each month. Which solution will meet these requirements MOST cost-effectively?

- A. Host a dynamic contact form page in Amazon Elastic Container Service (Amazon ECS). Set up Amazon Simple Email Service (Amazon SES) to connect to any third-party email provider.

- B. Create an Amazon API Gateway endpoint with an AWS Lambda backend that makes a call to Amazon Simple Email Service (Amazon SES).

- C. Convert the static webpage to dynamic by deploying Amazon Lightsail. Use client-side scripting to build the contact form. Integrate the form with Amazon WorkMail.

- D. Create a t2.micro Amazon EC2 instance. Deploy a LAMP (Linux, Apache, MySQL, PHP/Perl/Python) stack to host the webpage. Use client- side scripting to build the contact form. Integrate the form with Amazon WorkMail.

**Correct:** B
**Why:** API Gateway + Lambda → SES adds minimal serverless backend to a static S3 site.

**Incorrect:**
- A: ECS/Lightsail/EC2 add cost and ops for very low traffic.
- C: ECS/Lightsail/EC2 add cost and ops for very low traffic.
- D: ECS/Lightsail/EC2 add cost and ops for very low traffic.


---

---

### Question #323

A company’s facility has badge readers at every entrance throughout the building. When badges are scanned, the readers send a message over HTTPS to indicate who attempted to access that particular entrance. A solutions architect must design a system to process these messages from the sensors. The solution must be highly available, and the results must be made available for the company’s security team to analyze. Which system architecture should the solutions architect recommend?

- A. Launch an Amazon EC2 instance to serve as the HTTPS endpoint and to process the messages. Configure the EC2 instance to save the results to an Amazon S3 bucket.

- B. Create an HTTPS endpoint in Amazon API Gateway. Configure the API Gateway endpoint to invoke an AWS Lambda function to process the messages and save the results to an Amazon DynamoDB table.

- C. Use Amazon Route 53 to direct incoming sensor messages to an AWS Lambda function. Configure the Lambda function to process the messages and save the results to an Amazon DynamoDB table.

- D. Create a gateway VPC endpoint for Amazon S3. Configure a Site-to-Site VPN connection from the facility network to the VPC so that sensor data can be written directly to an S3 bucket by way of the VPC endpoint.

**Correct:** B
**Why:** API Gateway HTTPS endpoint invoking Lambda, storing to DynamoDB, is highly available and serverless.

**Incorrect:**
- A: EC2 endpoint or direct S3 via VPN increases ops/complexity.
- C: Route 53 cannot directly invoke code.
- D: EC2 endpoint or direct S3 via VPN increases ops/complexity.


---

---

### Question #354

A company hosts a serverless application on AWS. The application uses Amazon API Gateway, AWS Lambda, and an Amazon RDS for PostgreSQL database. The company notices an increase in application errors that result from database connection timeouts during times of peak trac or unpredictable trac. The company needs a solution that reduces the application failures with the least amount of change to the code. What should a solutions architect do to meet these requirements?

- A. Reduce the Lambda concurrency rate.

- B. Enable RDS Proxy on the RDS DB instance.

- C. Resize the RDS DB instance class to accept more connections.

- D. Migrate the database to Amazon DynamoDB with on-demand scaling.

**Correct:** B
**Why:** RDS Proxy pools and reuses connections for Lambda, preventing connection storms/timeouts during bursts with minimal code change.

**Incorrect:**
- A: Reducing Lambda concurrency throttles the app and does not fix connection pooling.
- C: Upsizing may delay but not solve connection exhaustion.
- D: Migrating to DynamoDB is a large rewrite.


---

---

### Question #360

A company uses Amazon API Gateway to run a private gateway with two REST APIs in the same VPC. The BuyStock RESTful web service calls the CheckFunds RESTful web service to ensure that enough funds are available before a stock can be purchased. The company has noticed in the VPC flow logs that the BuyStock RESTful web service calls the CheckFunds RESTful web service over the internet instead of through the VPC. A solutions architect must implement a solution so that the APIs communicate through the VPC. Which solution will meet these requirements with the FEWEST changes to the code?

- A. Add an X-API-Key header in the HTTP header for authorization.

- B. Use an interface endpoint.

- C. Use a gateway endpoint.

- D. Add an Amazon Simple Queue Service (Amazon SQS) queue between the two REST APIs.

**Correct:** B
**Why:** An interface VPC endpoint for API Gateway ensures private communication between private APIs through the VPC with minimal code changes.

**Incorrect:**
- A: API keys don’t change the network path.
- C: Gateway endpoints are for S3/DynamoDB, not API Gateway.
- D: Adding SQS changes the design and still doesn’t ensure private API‑to‑API calls.


---

---

### Question #366

A company’s web application consists of an Amazon API Gateway API in front of an AWS Lambda function and an Amazon DynamoDB database. The Lambda function handles the business logic, and the DynamoDB table hosts the data. The application uses Amazon Cognito user pools to identify the individual users of the application. A solutions architect needs to update the application so that only users who have a subscription can access premium content. Which solution will meet this requirement with the LEAST operational overhead?

- A. Enable API caching and throttling on the API Gateway API.

- B. Set up AWS WAF on the API Gateway API. Create a rule to filter users who have a subscription.

- C. Apply ne-grained IAM permissions to the premium content in the DynamoDB table.

- D. Implement API usage plans and API keys to limit the access of users who do not have a subscription.

**Correct:** D
**Why:** API Gateway usage plans and API keys provide an easy way to gate/limit access for non‑subscribers with low operational overhead.

**Incorrect:**
- A: Caching/throttling don’t enforce subscription access.
- B: WAF filters traffic patterns, not subscription entitlements.
- C: Fine‑grained IAM on DynamoDB is complex and not aligned with Cognito user pool tokens alone.


---

---

### Question #379

A company hosts a frontend application that uses an Amazon API Gateway API backend that is integrated with AWS Lambda. When the API receives requests, the Lambda function loads many libraries. Then the Lambda function connects to an Amazon RDS database, processes the data, and returns the data to the frontend application. The company wants to ensure that response latency is as low as possible for all its users with the fewest number of changes to the company's operations. Which solution will meet these requirements?

- A. Establish a connection between the frontend application and the database to make queries faster by bypassing the API.

- B. Configure provisioned concurrency for the Lambda function that handles the requests.

- C. Cache the results of the queries in Amazon S3 for faster retrieval of similar datasets.

- D. Increase the size of the database to increase the number of connections Lambda can establish at one time.

**Correct:** B
**Why:** Provisioned concurrency keeps Lambda execution environments warm, reducing cold starts (library load time) and latency with minimal changes.

**Incorrect:**
- A: Direct DB access from frontend bypasses API and is insecure.
- C: Caching S3 results doesn’t address dynamic queries and connections.
- D: Upsizing DB does not remove Lambda cold starts.


---

---

### Question #397

An ecommerce company needs to run a scheduled daily job to aggregate and filter sales records for analytics. The company stores the sales records in an Amazon S3 bucket. Each object can be up to 10 GB in size. Based on the number of sales events, the job can take up to an hour to complete. The CPU and memory usage of the job are constant and are known in advance. A solutions architect needs to minimize the amount of operational effort that is needed for the job to run. Which solution meets these requirements?

- A. Create an AWS Lambda function that has an Amazon EventBridge notification. Schedule the EventBridge event to run once a day.

- B. Create an AWS Lambda function. Create an Amazon API Gateway HTTP API, and integrate the API with the function. Create an Amazon EventBridge scheduled event that calls the API and invokes the function.

- C. Create an Amazon Elastic Container Service (Amazon ECS) cluster with an AWS Fargate launch type. Create an Amazon EventBridge scheduled event that launches an ECS task on the cluster to run the job.

- D. Create an Amazon Elastic Container Service (Amazon ECS) cluster with an Amazon EC2 launch type and an Auto Scaling group with at least one EC2 instance. Create an Amazon EventBridge scheduled event that launches an ECS task on the cluster to run the job.

**Correct:** C
**Why:** ECS on Fargate with an EventBridge schedule runs the containerized job with known CPU/memory, minimizing ops (no servers to manage).

**Incorrect:**
- A: Lambda is not suitable for up to 10 GB objects and 1‑hour jobs.
- B: Lambda is not suitable for up to 10 GB objects and 1‑hour jobs.
- D: EC2 launch type requires managing instances/ASG.


---

---

### Question #399

A nancial company hosts a web application on AWS. The application uses an Amazon API Gateway Regional API endpoint to give users the ability to retrieve current stock prices. The company’s security team has noticed an increase in the number of API requests. The security team is concerned that HTTP ood attacks might take the application oine. A solutions architect must design a solution to protect the application from this type of attack. Which solution meets these requirements with the LEAST operational overhead?

- A. Create an Amazon CloudFront distribution in front of the API Gateway Regional API endpoint with a maximum TTL of 24 hours.

- B. Create a Regional AWS WAF web ACL with a rate-based rule. Associate the web ACL with the API Gateway stage.

- C. Use Amazon CloudWatch metrics to monitor the Count metric and alert the security team when the predened rate is reached.

- D. Create an Amazon CloudFront distribution with Lambda@Edge in front of the API Gateway Regional API endpoint. Create an AWS Lambda function to block requests from IP addresses that exceed the predened rate.

**Correct:** B
**Why:** A Regional WAF web ACL with a rate‑based rule attached to the API Gateway stage mitigates HTTP flood attacks with low ops.

**Incorrect:**
- A: CloudFront alone doesn’t handle rate limiting for Regional APIs.
- C: Monitoring doesn’t mitigate attacks.
- D: Lambda@Edge scripting is more complex and operationally heavy.


---

---

### Question #427

A solutions architect is implementing a complex Java application with a MySQL database. The Java application must be deployed on Apache Tomcat and must be highly available. What should the solutions architect do to meet these requirements?

- A. Deploy the application in AWS Lambda. Configure an Amazon API Gateway API to connect with the Lambda functions.

- B. Deploy the application by using AWS Elastic Beanstalk. Configure a load-balanced environment and a rolling deployment policy.

- C. Migrate the database to Amazon ElastiCache. Configure the ElastiCache security group to allow access from the application.

- D. Launch an Amazon EC2 instance. Install a MySQL server on the EC2 instance. Configure the application on the server. Create an AMI. Use the AMI to create a launch template with an Auto Scaling group.

**Correct:** B
**Why:** Elastic Beanstalk for Tomcat with load‑balanced, rolling deployments meets HA needs with low ops; use RDS for MySQL.

**Incorrect:**
- A: Lambda is not for Tomcat Java web apps.
- C: ElastiCache is not a database.
- D: DIY EC2/ASG raises operational burden.


---

---

### Question #428

A serverless application uses Amazon API Gateway, AWS Lambda, and Amazon DynamoDB. The Lambda function needs permissions to read and write to the DynamoDB table. Which solution will give the Lambda function access to the DynamoDB table MOST securely?

- A. Create an IAM user with programmatic access to the Lambda function. Attach a policy to the user that allows read and write access to the DynamoDB table. Store the access_key_id and secret_access_key parameters as part of the Lambda environment variables. Ensure that other AWS users do not have read and write access to the Lambda function configuration.

- B. Create an IAM role that includes Lambda as a trusted service. Attach a policy to the role that allows read and write access to the DynamoDB table. Update the configuration of the Lambda function to use the new role as the execution role.

- C. Create an IAM user with programmatic access to the Lambda function. Attach a policy to the user that allows read and write access to the DynamoDB table. Store the access_key_id and secret_access_key parameters in AWS Systems Manager Parameter Store as secure string parameters. Update the Lambda function code to retrieve the secure string parameters before connecting to the DynamoDB table.

- D. Create an IAM role that includes DynamoDB as a trusted service. Attach a policy to the role that allows read and write access from the Lambda function. Update the code of the Lambda function to attach to the new role as an execution role.

**Correct:** B
**Why:** Use an IAM role trusted by Lambda with a policy granting DynamoDB access; attach as the function’s execution role.

**Incorrect:**
- A: Do not use static IAM user creds in Lambda.
- C: Do not use static IAM user creds in Lambda.
- D: DynamoDB is not a trusted principal for the role.


---

---

### Question #441

A company hosts a multi-tier web application on Amazon Linux Amazon EC2 instances behind an Application Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The company observes that the Auto Scaling group launches more On-Demand Instances when the application's end users access high volumes of static web content. The company wants to optimize cost. What should a solutions architect do to redesign the application MOST cost-effectively?

- A. Update the Auto Scaling group to use Reserved Instances instead of On-Demand Instances.

- B. Update the Auto Scaling group to scale by launching Spot Instances instead of On-Demand Instances.

- C. Create an Amazon CloudFront distribution to host the static web contents from an Amazon S3 bucket.

- D. Create an AWS Lambda function behind an Amazon API Gateway API to host the static website contents.

**Correct:** C
**Why:** Serve static content from S3 via CloudFront to reduce load on EC2/ALB and lower cost.

**Incorrect:**
- A: Instance pricing choices don’t remove static load driver.
- B: Instance pricing choices don’t remove static load driver.
- D: Lambda+API Gateway is unnecessary for static hosting.


---

---

### Question #444

A company has hired a solutions architect to design a reliable architecture for its application. The application consists of one Amazon RDS DB instance and two manually provisioned Amazon EC2 instances that run web servers. The EC2 instances are located in a single Availability Zone. An employee recently deleted the DB instance, and the application was unavailable for 24 hours as a result. The company is concerned with the overall reliability of its environment. What should the solutions architect do to maximize reliability of the application's infrastructure?

- A. Delete one EC2 instance and enable termination protection on the other EC2 instance. Update the DB instance to be Multi-AZ, and enable deletion protection.

- B. Update the DB instance to be Multi-AZ, and enable deletion protection. Place the EC2 instances behind an Application Load Balancer, and run them in an EC2 Auto Scaling group across multiple Availability Zones.

- C. Create an additional DB instance along with an Amazon API Gateway and an AWS Lambda function. Configure the application to invoke the Lambda function through API Gateway. Have the Lambda function write the data to the two DB instances.

- D. Place the EC2 instances in an EC2 Auto Scaling group that has multiple subnets located in multiple Availability Zones. Use Spot Instances instead of On-Demand Instances. Set up Amazon CloudWatch alarms to monitor the health of the instances Update the DB instance to be Multi-AZ, and enable deletion protection.

**Correct:** B
**Why:** Protect RDS with Multi‑AZ and deletion protection; place EC2 in ALB+Auto Scaling across AZs to maximize reliability.

**Incorrect:**
- A: Either reduce availability or add unnecessary components/cost.
- C: Either reduce availability or add unnecessary components/cost.
- D: Either reduce availability or add unnecessary components/cost.


---

---

### Question #447

A company has a stateless web application that runs on AWS Lambda functions that are invoked by Amazon API Gateway. The company wants to deploy the application across multiple AWS Regions to provide Regional failover capabilities. What should a solutions architect do to route trac to multiple Regions?

- A. Create Amazon Route 53 health checks for each Region. Use an active-active failover configuration.

- B. Create an Amazon CloudFront distribution with an origin for each Region. Use CloudFront health checks to route trac.

- C. Create a transit gateway. Attach the transit gateway to the API Gateway endpoint in each Region. Configure the transit gateway to route requests.

- D. Create an Application Load Balancer in the primary Region. Set the target group to point to the API Gateway endpoint hostnames in each Region.

**Correct:** A
**Why:** Use Route 53 health checks with active‑active routing across Regions for serverless endpoints.

**Incorrect:**
- B: CloudFront is for HTTP caching, not API multi‑Region routing.
- C: Transit gateway/ALB cannot directly route to API Gateway across Regions as required.
- D: Transit gateway/ALB cannot directly route to API Gateway across Regions as required.


---

---

### Question #458

A solutions architect is designing a RESTAPI in Amazon API Gateway for a cash payback service. The application requires 1 GB of memory and 2 GB of storage for its computation resources. The application will require that the data is in a relational format. Which additional combination ofAWS services will meet these requirements with the LEAST administrative effort? (Choose two.)

- A. Amazon EC2

- B. AWS Lambda

- C. Amazon RDS

- D. Amazon DynamoDB

E. Amazon Elastic Kubernetes Services (Amazon EKS)

**Correct:** B, C
**Why:** Lambda can be sized to 1 GB memory and sufficient ephemeral storage; use Amazon RDS for relational storage—minimal admin.

**Incorrect:**
- A: EC2/EKS add operational overhead.
- D: DynamoDB is non‑relational.
- E: EC2/EKS add operational overhead.


---

---

### Question #468

A company is developing a microservices application that will provide a search catalog for customers. The company must use REST APIs to present the frontend of the application to users. The REST APIs must access the backend services that the company hosts in containers in private VPC subnets. Which solution will meet these requirements?

- A. Design a WebSocket API by using Amazon API Gateway. Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet. Create a private VPC link for API Gateway to access Amazon ECS.

- B. Design a REST API by using Amazon API Gateway. Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet. Create a private VPC link for API Gateway to access Amazon ECS.

- C. Design a WebSocket API by using Amazon API Gateway. Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet. Create a security group for API Gateway to access Amazon ECS.

- D. Design a REST API by using Amazon API Gateway. Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet. Create a security group for API Gateway to access Amazon ECS.

**Correct:** B
**Why:** API Gateway REST API with a VPC Link privately connects to ECS services in private subnets.

**Incorrect:**
- A: WebSocket API or security groups alone don’t provide private integration.
- C: WebSocket API or security groups alone don’t provide private integration.
- D: WebSocket API or security groups alone don’t provide private integration.


---

## Amazon Athena

### Question #258

A company has an application that places hundreds of .csv files into an Amazon S3 bucket every hour. The files are 1 GB in size. Each time a file is uploaded, the company needs to convert the file to Apache Parquet format and place the output file into an S3 bucket. Which solution will meet these requirements with the LEAST operational overhead?

- A. Create an AWS Lambda function to download the .csv files, convert the files to Parquet format, and place the output files in an S3 bucket. Invoke the Lambda function for each S3 PUT event.

- B. Create an Apache Spark job to read the .csv files, convert the files to Parquet format, and place the output files in an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the Spark job.

- C. Create an AWS Glue table and an AWS Glue crawler for the S3 bucket where the application places the .csv files. Schedule an AWS Lambda function to periodically use Amazon Athena to query the AWS Glue table, convert the query results into Parquet format, and place the output files into an S3 bucket.

- D. Create an AWS Glue extract, transform, and load (ETL) job to convert the .csv files to Parquet format and place the output files into an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the ETL job.

**Correct:** D
**Why:** AWS Glue ETL is managed and scalable; invoke job on S3 PUT via Lambda for timely conversion to Parquet.

**Incorrect:**
- A: Lambda alone is inefficient for 1‑GB files at scale.
- B: EMR or Athena‑based conversions add more ops/indirection.
- C: EMR or Athena‑based conversions add more ops/indirection.


---

---

### Question #267

A company has one million users that use its mobile app. The company must analyze the data usage in near-real time. The company also must encrypt the data in near-real time and must store the data in a centralized location in Apache Parquet format for further processing. Which solution will meet these requirements with the LEAST operational overhead?

- A. Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon Kinesis Data Analytics application to analyze the data. Invoke an AWS Lambda function to send the data to the Kinesis Data Analytics application.

- B. Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon EMR cluster to analyze the data. Invoke an AWS Lambda function to send the data to the EMR cluster.

- C. Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create an Amazon EMR cluster to analyze the data.

- D. Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create an Amazon Kinesis Data Analytics application to analyze the data.

**Correct:** A
**Why:** Kinesis Data Streams → Kinesis Data Analytics for near real‑time analytics, Firehose to S3 with Parquet conversion; Athena for SQL.

**Incorrect:**
- B: Do not provide both real‑time analytics and Parquet delivery with least ops as cleanly.
- C: Do not provide both real‑time analytics and Parquet delivery with least ops as cleanly.
- D: Do not provide both real‑time analytics and Parquet delivery with least ops as cleanly.


---

---

### Question #278

A company wants to create an application to store employee data in a hierarchical structured relationship. The company needs a minimum-latency response to high-trac queries for the employee data and must protect any sensitive data. The company also needs to receive monthly email messages if any nancial information is present in the employee data. Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)

- A. Use Amazon Redshift to store the employee data in hierarchies. Unload the data to Amazon S3 every month.

- B. Use Amazon DynamoDB to store the employee data in hierarchies. Export the data to Amazon S3 every month.

- C. Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly events to AWS Lambda.

- D. Use Amazon Athena to analyze the employee data in Amazon S3. Integrate Athena with Amazon QuickSight to publish analysis dashboards and share the dashboards with users.

E. Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly notications through an Amazon Simple Notification Service (Amazon SNS) subscription.

**Correct:** B, E
**Why:** DynamoDB suits hierarchical, low‑latency queries; Macie + EventBridge → SNS can notify monthly when financial data is detected.

**Incorrect:**
- A: Redshift/Athena aren’t optimized for low‑latency hierarchical queries.
- C: Lambda trigger pattern is unnecessary for monthly notifications.
- D: Redshift/Athena aren’t optimized for low‑latency hierarchical queries.


---

---

### Question #280

A company is using Amazon CloudFront with its website. The company has enabled logging on the CloudFront distribution, and logs are saved in one of the company’s Amazon S3 buckets. The company needs to perform advanced analyses on the logs and build visualizations. What should a solutions architect do to meet these requirements?

- A. Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket. Visualize the results with AWS Glue.

- B. Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket. Visualize the results with Amazon QuickSight.

- C. Use standard SQL queries in Amazon DynamoDB to analyze the CloudFront logs in the S3 bucket. Visualize the results with AWS Glue.

- D. Use standard SQL queries in Amazon DynamoDB to analyze the CloudFront logs in the S3 bucket. Visualize the results with Amazon QuickSight.

**Correct:** B
**Why:** Use Athena to query CloudFront logs in S3 and QuickSight for visualizations.

**Incorrect:**
- A: Glue/DynamoDB are not needed for log analysis/visualization here.
- C: Glue/DynamoDB are not needed for log analysis/visualization here.
- D: Glue/DynamoDB are not needed for log analysis/visualization here.


---

---

### Question #284

As part of budget planning, management wants a report of AWS billed items listed by user. The data will be used to create department budgets. A solutions architect needs to determine the most ecient way to obtain this report information. Which solution meets these requirements?

- A. Run a query with Amazon Athena to generate the report.

- B. Create a report in Cost Explorer and download the report.

- C. Access the bill details from the billing dashboard and download the bill.

- D. Modify a cost budget in AWS Budgets to alert with Amazon Simple Email Service (Amazon SES).

**Correct:** B
**Why:** Cost Explorer can produce user‑level billed items report efficiently.

**Incorrect:**
- A: Athena/console bills/Budgets alerts don’t directly produce the required per‑user billing report.
- C: Athena/console bills/Budgets alerts don’t directly produce the required per‑user billing report.
- D: Athena/console bills/Budgets alerts don’t directly produce the required per‑user billing report.


---

---

### Question #292

A company is preparing a new data platform that will ingest real-time streaming data from multiple sources. The company needs to transform the data before writing the data to Amazon S3. The company needs the ability to use SQL to query the transformed data. Which solutions will meet these requirements? (Choose two.)

- A. Use Amazon Kinesis Data Streams to stream the data. Use Amazon Kinesis Data Analytics to transform the data. Use Amazon Kinesis Data Firehose to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.

- B. Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use AWS Glue to transform the data and to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.

- C. Use AWS Database Migration Service (AWS DMS) to ingest the data. Use Amazon EMR to transform the data and to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.

- D. Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use Amazon Kinesis Data Analytics to transform the data and to write the data to Amazon S3. Use the Amazon RDS query editor to query the transformed data from Amazon S3.

E. Use Amazon Kinesis Data Streams to stream the data. Use AWS Glue to transform the data. Use Amazon Kinesis Data Firehose to write the data to Amazon S3. Use the Amazon RDS query editor to query the transformed data from Amazon S3.

**Correct:** A, B
**Why:** Kinesis Data Streams + KDA + Firehose + S3 + Athena, or MSK + Glue + S3 + Athena both satisfy transform to S3 and SQL on data.

**Incorrect:**
- C: DMS/Incorrect query endpoints do not meet requirements fully.
- D: DMS/Incorrect query endpoints do not meet requirements fully.
- E: DMS/Incorrect query endpoints do not meet requirements fully.


---

---

### Question #309

A solutions architect needs to optimize storage costs. The solutions architect must identify any Amazon S3 buckets that are no longer being accessed or are rarely accessed. Which solution will accomplish this goal with the LEAST operational overhead?

- A. Analyze bucket access patterns by using the S3 Storage Lens dashboard for advanced activity metrics.

- B. Analyze bucket access patterns by using the S3 dashboard in the AWS Management Console.

- C. Turn on the Amazon CloudWatch BucketSizeBytes metric for buckets. Analyze bucket access patterns by using the metrics data with Amazon Athena.

- D. Turn on AWS CloudTrail for S3 object monitoring. Analyze bucket access patterns by using CloudTrail logs that are integrated with Amazon CloudWatch Logs.

**Correct:** A
**Why:** S3 Storage Lens advanced metrics identify buckets with low/rare access with least ops.

**Incorrect:**
- B: Console metrics, CloudWatch, or CloudTrail require more work and are less precise.
- C: Console metrics, CloudWatch, or CloudTrail require more work and are less precise.
- D: Console metrics, CloudWatch, or CloudTrail require more work and are less precise.


---

---

### Question #320

A company is using a eet of Amazon EC2 instances to ingest data from on-premises data sources. The data is in JSON format and ingestion rates can be as high as 1 MB/s. When an EC2 instance is rebooted, the data in-ight is lost. The company’s data science team wants to query ingested data in near-real time. Which solution provides near-real-time data querying that is scalable with minimal data loss?

- A. Publish data to Amazon Kinesis Data Streams, Use Kinesis Data Analytics to query the data.

- B. Publish data to Amazon Kinesis Data Firehose with Amazon Redshift as the destination. Use Amazon Redshift to query the data.

- C. Store ingested data in an EC2 instance store. Publish data to Amazon Kinesis Data Firehose with Amazon S3 as the destination. Use Amazon Athena to query the data.

- D. Store ingested data in an Amazon Elastic Block Store (Amazon EBS) volume. Publish data to Amazon ElastiCache for Redis. Subscribe to the Redis channel to query the data.

**Correct:** A
**Why:** Kinesis Data Streams with Kinesis Data Analytics provides near‑real‑time querying with minimal loss and scalability.

**Incorrect:**
- B: Firehose→Redshift has more latency; instance store/EBS risk data loss; Redis pub/sub isn’t durable.
- C: Firehose→Redshift has more latency; instance store/EBS risk data loss; Redis pub/sub isn’t durable.
- D: Firehose→Redshift has more latency; instance store/EBS risk data loss; Redis pub/sub isn’t durable.


---

---

### Question #341

A company has an Amazon S3 data lake that is governed by AWS Lake Formation. The company wants to create a visualization in Amazon QuickSight by joining the data in the data lake with operational data that is stored in an Amazon Aurora MySQL database. The company wants to enforce column-level authorization so that the company’s marketing team can access only a subset of columns in the database. Which solution will meet these requirements with the LEAST operational overhead?

- A. Use Amazon EMR to ingest the data directly from the database to the QuickSight SPICE engine. Include only the required columns.

- B. Use AWS Glue Studio to ingest the data from the database to the S3 data lake. Attach an IAM policy to the QuickSight users to enforce column-level access control. Use Amazon S3 as the data source in QuickSight.

- C. Use AWS Glue Elastic Views to create a materialized view for the database in Amazon S3. Create an S3 bucket policy to enforce column- level access control for the QuickSight users. Use Amazon S3 as the data source in QuickSight.

- D. Use a Lake Formation blueprint to ingest the data from the database to the S3 data lake. Use Lake Formation to enforce column-level access control for the QuickSight users. Use Amazon Athena as the data source in QuickSight.

**Correct:** D
**Why:** Lake Formation ingestion and column‑level permissions with Athena as the query engine integrate cleanly with QuickSight.

**Incorrect:**
- A: EMR/Glue Studio or Elastic Views/S3 policies don’t provide column‑level governance end‑to‑end as simply.
- B: EMR/Glue Studio or Elastic Views/S3 policies don’t provide column‑level governance end‑to‑end as simply.
- C: EMR/Glue Studio or Elastic Views/S3 policies don’t provide column‑level governance end‑to‑end as simply.


---

---

### Question #361

A company hosts a multiplayer gaming application on AWS. The company wants the application to read data with sub-millisecond latency and run one-time queries on historical data. Which solution will meet these requirements with the LEAST operational overhead?

- A. Use Amazon RDS for data that is frequently accessed. Run a periodic custom script to export the data to an Amazon S3 bucket.

- B. Store the data directly in an Amazon S3 bucket. Implement an S3 Lifecycle policy to move older data to S3 Glacier Deep Archive for long- term storage. Run one-time queries on the data in Amazon S3 by using Amazon Athena.

- C. Use Amazon DynamoDB with DynamoDB Accelerator (DAX) for data that is frequently accessed. Export the data to an Amazon S3 bucket by using DynamoDB table export. Run one-time queries on the data in Amazon S3 by using Amazon Athena.

- D. Use Amazon DynamoDB for data that is frequently accessed. Turn on streaming to Amazon Kinesis Data Streams. Use Amazon Kinesis Data Firehose to read the data from Kinesis Data Streams. Store the records in an Amazon S3 bucket.

**Correct:** C
**Why:** DynamoDB + DAX provides sub‑millisecond reads; export to S3 and query ad hoc with Athena for historical analysis with low ops.

**Incorrect:**
- A: Adds more moving parts and does not give sub‑ms reads as simply as DAX.
- B: S3 alone lacks sub‑ms latency for frequently accessed data.
- D: Adds more moving parts and does not give sub‑ms reads as simply as DAX.


---

---

### Question #495

A company is conducting an internal audit. The company wants to ensure that the data in an Amazon S3 bucket that is associated with the company’s AWS Lake Formation data lake does not contain sensitive customer or employee data. The company wants to discover personally identiable information (PII) or nancial information, including passport numbers and credit card numbers. Which solution will meet these requirements?

- A. Configure AWS Audit Manager on the account. Select the Payment Card Industry Data Security Standards (PCI DSS) for auditing.

- B. Configure Amazon S3 Inventory on the S3 bucket Configure Amazon Athena to query the inventory.

- C. Configure Amazon Macie to run a data discovery job that uses managed identiers for the required data types.

- D. Use Amazon S3 Select to run a report across the S3 bucket.

**Correct:** C
**Why:** Amazon Macie can scan S3 for PII/financial data with managed identifiers and produce findings.

**Incorrect:**
- A: Audit Manager/S3 Inventory/S3 Select do not detect PII patterns as Macie does.
- B: Audit Manager/S3 Inventory/S3 Select do not detect PII patterns as Macie does.
- D: Audit Manager/S3 Inventory/S3 Select do not detect PII patterns as Macie does.


---

## Amazon Aurora

### Question #273

A rapidly growing ecommerce company is running its workloads in a single AWS Region. A solutions architect must create a disaster recovery (DR) strategy that includes a different AWS Region. The company wants its database to be up to date in the DR Region with the least possible latency. The remaining infrastructure in the DR Region needs to run at reduced capacity and must be able to scale up if necessary. Which solution will meet these requirements with the LOWEST recovery time objective (RTO)?

- A. Use an Amazon Aurora global database with a pilot light deployment.

- B. Use an Amazon Aurora global database with a warm standby deployment.

- C. Use an Amazon RDS Multi-AZ DB instance with a pilot light deployment.

- D. Use an Amazon RDS Multi-AZ DB instance with a warm standby deployment.

**Correct:** B
**Why:** Aurora Global Database with a warm standby (scaled‑down stack) in DR Region offers the lowest RTO and near‑real‑time replication.

**Incorrect:**
- A: Pilot light increases RTO.
- C: RDS Multi‑AZ/snapshots won’t meet lowest RTO cross‑Region.
- D: RDS Multi‑AZ/snapshots won’t meet lowest RTO cross‑Region.


---

---

### Question #276

A company has a multi-tier application deployed on several Amazon EC2 instances in an Auto Scaling group. An Amazon RDS for Oracle instance is the application’ s data layer that uses Oracle-specic PL/SQL functions. Trac to the application has been steadily increasing. This is causing the EC2 instances to become overloaded and the RDS instance to run out of storage. The Auto Scaling group does not have any scaling metrics and denes the minimum healthy instance count only. The company predicts that trac will continue to increase at a steady but unpredictable rate before leveling off. What should a solutions architect do to ensure the system can automatically scale for the increased trac? (Choose two.)

- A. Configure storage Auto Scaling on the RDS for Oracle instance.

- B. Migrate the database to Amazon Aurora to use Auto Scaling storage.

- C. Configure an alarm on the RDS for Oracle instance for low free storage space.

- D. Configure the Auto Scaling group to use the average CPU as the scaling metric.

E. Configure the Auto Scaling group to use the average free memory as the scaling metric.

**Correct:** A, D
**Why:** Enable RDS storage autoscaling and scale EC2 fleet on average CPU to handle increasing traffic automatically.

**Incorrect:**
- B: Aurora migration is heavier change.
- C: Alarm only or memory metric alone is insufficient as primary scaler.
- E: Alarm only or memory metric alone is insufficient as primary scaler.


---

---

### Question #300

A company needs to migrate a legacy application from an on-premises data center to the AWS Cloud because of hardware capacity constraints. The application runs 24 hours a day, 7 days a week. The application’s database storage continues to grow over time. What should a solutions architect do to meet these requirements MOST cost-effectively?

- A. Migrate the application layer to Amazon EC2 Spot Instances. Migrate the data storage layer to Amazon S3.

- B. Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer to Amazon RDS On-Demand Instances.

- C. Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer to Amazon Aurora Reserved Instances.

- D. Migrate the application layer to Amazon EC2 On-Demand Instances. Migrate the data storage layer to Amazon RDS Reserved Instances.

**Correct:** C
**Why:** 24/7 app → EC2 RIs; Aurora RIs for growing database with storage autoscaling; cost‑effective for continuous use.

**Incorrect:**
- A: Spot unsuitable for 24/7.
- B: On‑Demand for DB or app increases cost.
- D: On‑Demand for DB or app increases cost.


---

---

### Question #314

A company has an on-premises MySQL database used by the global sales team with infrequent access patterns. The sales team requires the database to have minimal downtime. A database administrator wants to migrate this database to AWS without selecting a particular instance type in anticipation of more users in the future. Which service should a solutions architect recommend?

- A. Amazon Aurora MySQL

- B. Amazon Aurora Serverless for MySQL

- C. Amazon Redshift Spectrum

- D. Amazon RDS for MySQL

**Correct:** B
**Why:** Aurora Serverless for MySQL scales capacity automatically; supports minimal downtime migration and infrequent usage.

**Incorrect:**
- A: Fixed instances require capacity planning.
- C: Redshift Spectrum is for analytics, not OLTP.
- D: Fixed instances require capacity planning.


---

---

### Question #336

A company hosts a multi-tier web application that uses an Amazon Aurora MySQL DB cluster for storage. The application tier is hosted on Amazon EC2 instances. The company’s IT security guidelines mandate that the database credentials be encrypted and rotated every 14 days. What should a solutions architect do to meet this requirement with the LEAST operational effort?

- A. Create a new AWS Key Management Service (AWS KMS) encryption key. Use AWS Secrets Manager to create a new secret that uses the KMS key with the appropriate credentials. Associate the secret with the Aurora DB cluster. Configure a custom rotation period of 14 days.

- B. Create two parameters in AWS Systems Manager Parameter Store: one for the user name as a string parameter and one that uses the SecureString type for the password. Select AWS Key Management Service (AWS KMS) encryption for the password parameter, and load these parameters in the application tier. Implement an AWS Lambda function that rotates the password every 14 days.

- C. Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system in all EC2 instances of the application tier. Restrict the access to the file on the file system so that the application can read the file and that only super users can modify the file. Implement an AWS Lambda function that rotates the key in Aurora every 14 days and writes new credentials into the file.

- D. Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon S3 bucket that the application uses to load the credentials. Download the file to the application regularly to ensure that the correct credentials are used. Implement an AWS Lambda function that rotates the Aurora credentials every 14 days and uploads these credentials to the file in the S3 bucket.

**Correct:** A
**Why:** Secrets Manager with KMS‑encrypted secret and built‑in rotation integrates with Aurora; set 14‑day rotation.

**Incorrect:**
- B: Parameter Store/EFS/S3 solutions require more glue code and ops.
- C: Parameter Store/EFS/S3 solutions require more glue code and ops.
- D: Parameter Store/EFS/S3 solutions require more glue code and ops.


---

---

### Question #337

A company has deployed a web application on AWS. The company hosts the backend database on Amazon RDS for MySQL with a primary DB instance and ve read replicas to support scaling needs. The read replicas must lag no more than 1 second behind the primary DB instance. The database routinely runs scheduled stored procedures. As trac on the website increases, the replicas experience additional lag during periods of peak load. A solutions architect must reduce the replication lag as much as possible. The solutions architect must minimize changes to the application code and must minimize ongoing operational overhead. Which solution will meet these requirements?

- A. Migrate the database to Amazon Aurora MySQL. Replace the read replicas with Aurora Replicas, and configure Aurora Auto Scaling. Replace the stored procedures with Aurora MySQL native functions.

- B. Deploy an Amazon ElastiCache for Redis cluster in front of the database. Modify the application to check the cache before the application queries the database. Replace the stored procedures with AWS Lambda functions.

- C. Migrate the database to a MySQL database that runs on Amazon EC2 instances. Choose large, compute optimized EC2 instances for all replica nodes. Maintain the stored procedures on the EC2 instances.

- D. Migrate the database to Amazon DynamoDB. Provision a large number of read capacity units (RCUs) to support the required throughput, and configure on-demand capacity scaling. Replace the stored procedures with DynamoDB streams.

**Correct:** A
**Why:** Aurora MySQL with Aurora Replicas and Auto Scaling reduces replica lag and operational overhead.

**Incorrect:**
- B: Cache/EC2 self‑managed or DynamoDB require more changes.
- C: Cache/EC2 self‑managed or DynamoDB require more changes.
- D: Cache/EC2 self‑managed or DynamoDB require more changes.


---

---

### Question #338

A solutions architect must create a disaster recovery (DR) plan for a high-volume software as a service (SaaS) platform. All data for the platform is stored in an Amazon Aurora MySQL DB cluster. The DR plan must replicate data to a secondary AWS Region. Which solution will meet these requirements MOST cost-effectively?

- A. Use MySQL binary log replication to an Aurora cluster in the secondary Region. Provision one DB instance for the Aurora cluster in the secondary Region.

- B. Set up an Aurora global database for the DB cluster. When setup is complete, remove the DB instance from the secondary Region.

- C. Use AWS Database Migration Service (AWS DMS) to continuously replicate data to an Aurora cluster in the secondary Region. Remove the DB instance from the secondary Region.

- D. Set up an Aurora global database for the DB cluster. Specify a minimum of one DB instance in the secondary Region.

**Correct:** D
**Why:** Aurora Global Database replicates data to a secondary Region with minimal lag; requires at least one instance there.

**Incorrect:**
- A: MySQL binlog/DMS add ops and cost without native Aurora benefits.
- B: Removing the instance breaks the global cluster.
- C: MySQL binlog/DMS add ops and cost without native Aurora benefits.


---

---

### Question #340

A media company hosts its website on AWS. The website application’s architecture includes a eet of Amazon EC2 instances behind an Application Load Balancer (ALB) and a database that is hosted on Amazon Aurora. The company’s cybersecurity team reports that the application is vulnerable to SQL injection. How should the company resolve this issue?

- A. Use AWS WAF in front of the ALB. Associate the appropriate web ACLs with AWS WAF.

- B. Create an ALB listener rule to reply to SQL injections with a xed response.

- C. Subscribe to AWS Shield Advanced to block all SQL injection attempts automatically.

- D. Set up Amazon Inspector to block all SQL injection attempts automatically.

**Correct:** A
**Why:** Put WAF in front of ALB and enable managed rules for SQLi to mitigate vulnerabilities quickly.

**Incorrect:**
- B: ALB listener responses or Shield/Inspector don’t address SQLi at the app layer.
- C: ALB listener responses or Shield/Inspector don’t address SQLi at the app layer.
- D: ALB listener responses or Shield/Inspector don’t address SQLi at the app layer.


---

---

### Question #341

A company has an Amazon S3 data lake that is governed by AWS Lake Formation. The company wants to create a visualization in Amazon QuickSight by joining the data in the data lake with operational data that is stored in an Amazon Aurora MySQL database. The company wants to enforce column-level authorization so that the company’s marketing team can access only a subset of columns in the database. Which solution will meet these requirements with the LEAST operational overhead?

- A. Use Amazon EMR to ingest the data directly from the database to the QuickSight SPICE engine. Include only the required columns.

- B. Use AWS Glue Studio to ingest the data from the database to the S3 data lake. Attach an IAM policy to the QuickSight users to enforce column-level access control. Use Amazon S3 as the data source in QuickSight.

- C. Use AWS Glue Elastic Views to create a materialized view for the database in Amazon S3. Create an S3 bucket policy to enforce column- level access control for the QuickSight users. Use Amazon S3 as the data source in QuickSight.

- D. Use a Lake Formation blueprint to ingest the data from the database to the S3 data lake. Use Lake Formation to enforce column-level access control for the QuickSight users. Use Amazon Athena as the data source in QuickSight.

**Correct:** D
**Why:** Lake Formation ingestion and column‑level permissions with Athena as the query engine integrate cleanly with QuickSight.

**Incorrect:**
- A: EMR/Glue Studio or Elastic Views/S3 policies don’t provide column‑level governance end‑to‑end as simply.
- B: EMR/Glue Studio or Elastic Views/S3 policies don’t provide column‑level governance end‑to‑end as simply.
- C: EMR/Glue Studio or Elastic Views/S3 policies don’t provide column‑level governance end‑to‑end as simply.


---

---

### Question #343

A solutions architect is designing a company’s disaster recovery (DR) architecture. The company has a MySQL database that runs on an Amazon EC2 instance in a private subnet with scheduled backup. The DR design needs to include multiple AWS Regions. Which solution will meet these requirements with the LEAST operational overhead?

- A. Migrate the MySQL database to multiple EC2 instances. Configure a standby EC2 instance in the DR Region. Turn on replication.

- B. Migrate the MySQL database to Amazon RDS. Use a Multi-AZ deployment. Turn on read replication for the primary DB instance in the different Availability Zones.

- C. Migrate the MySQL database to an Amazon Aurora global database. Host the primary DB cluster in the primary Region. Host the secondary DB cluster in the DR Region.

- D. Store the scheduled backup of the MySQL database in an Amazon S3 bucket that is congured for S3 Cross-Region Replication (CRR). Use the data backup to restore the database in the DR Region.

**Correct:** C
**Why:** Aurora Global Database provides multi‑Region design with low ops and built‑in replication; best for DR across Regions.

**Incorrect:**
- A: EC2 self‑managed, single‑Region RDS Multi‑AZ, or CRR of backups offer higher RTO/RPO or more ops.
- B: EC2 self‑managed, single‑Region RDS Multi‑AZ, or CRR of backups offer higher RTO/RPO or more ops.
- D: EC2 self‑managed, single‑Region RDS Multi‑AZ, or CRR of backups offer higher RTO/RPO or more ops.


---

---

### Question #349

A company stores condential data in an Amazon Aurora PostgreSQL database in the ap-southeast-3 Region. The database is encrypted with an AWS Key Management Service (AWS KMS) customer managed key. The company was recently acquired and must securely share a backup of the database with the acquiring company’s AWS account in ap-southeast-3. What should a solutions architect do to meet these requirements?

- A. Create a database snapshot. Copy the snapshot to a new unencrypted snapshot. Share the new snapshot with the acquiring company’s AWS account.

- B. Create a database snapshot. Add the acquiring company’s AWS account to the KMS key policy. Share the snapshot with the acquiring company’s AWS account.

- C. Create a database snapshot that uses a different AWS managed KMS key. Add the acquiring company’s AWS account to the KMS key alias. Share the snapshot with the acquiring company's AWS account.

- D. Create a database snapshot. Download the database snapshot. Upload the database snapshot to an Amazon S3 bucket. Update the S3 bucket policy to allow access from the acquiring company’s AWS account.

**Correct:** B
**Why:** Share encrypted snapshot by adding the other account to the CMK key policy and sharing the snapshot.

**Incorrect:**
- A: Unencrypted copies, different KMS aliases, or S3 export are unnecessary/less secure.
- C: Unencrypted copies, different KMS aliases, or S3 export are unnecessary/less secure.
- D: Unencrypted copies, different KMS aliases, or S3 export are unnecessary/less secure.


---

---

### Question #378

A company is developing a real-time multiplayer game that uses UDP for communications between the client and servers in an Auto Scaling group. Spikes in demand are anticipated during the day, so the game server platform must adapt accordingly. Developers want to store gamer scores and other non-relational data in a database solution that will scale without intervention. Which solution should a solutions architect recommend?

- A. Use Amazon Route 53 for trac distribution and Amazon Aurora Serverless for data storage.

- B. Use a Network Load Balancer for trac distribution and Amazon DynamoDB on-demand for data storage.

- C. Use a Network Load Balancer for trac distribution and Amazon Aurora Global Database for data storage.

- D. Use an Application Load Balancer for trac distribution and Amazon DynamoDB global tables for data storage.

**Correct:** B
**Why:** NLB supports UDP for game traffic; DynamoDB on‑demand scales automatically for storing scores/non‑relational data.

**Incorrect:**
- A: Aurora is relational and requires capacity planning.
- C: Aurora is relational and requires capacity planning.
- D: ALB does not support UDP.


---

---

### Question #381

A company hosts a three-tier web application that includes a PostgreSQL database. The database stores the metadata from documents. The company searches the metadata for key terms to retrieve documents that the company reviews in a report each month. The documents are stored in Amazon S3. The documents are usually written only once, but they are updated frequently. The reporting process takes a few hours with the use of relational queries. The reporting process must not prevent any document modications or the addition of new documents. A solutions architect needs to implement a solution to speed up the reporting process. Which solution will meet these requirements with the LEAST amount of change to the application code?

- A. Set up a new Amazon DocumentDB (with MongoDB compatibility) cluster that includes a read replica. Scale the read replica to generate the reports.

- B. Set up a new Amazon Aurora PostgreSQL DB cluster that includes an Aurora Replica. Issue queries to the Aurora Replica to generate the reports.

- C. Set up a new Amazon RDS for PostgreSQL Multi-AZ DB instance. Configure the reporting module to query the secondary RDS node so that the reporting module does not affect the primary node.

- D. Set up a new Amazon DynamoDB table to store the documents. Use a xed write capacity to support new document entries. Automatically scale the read capacity to support the reports.

**Correct:** B
**Why:** Aurora PostgreSQL with an Aurora Replica lets reports run against the replica without blocking writes; minimal code change (same engine).

**Incorrect:**
- A: DocumentDB or RDS Multi‑AZ do not solve read scaling as effectively.
- C: DocumentDB or RDS Multi‑AZ do not solve read scaling as effectively.
- D: DynamoDB would require a redesign.


---

---

### Question #411

A company has a web application with sporadic usage patterns. There is heavy usage at the beginning of each month, moderate usage at the start of each week, and unpredictable usage during the week. The application consists of a web server and a MySQL database server running inside the data center. The company would like to move the application to the AWS Cloud, and needs to select a cost-effective database platform that will not require database modications. Which solution will meet these requirements?

- A. Amazon DynamoDB

- B. Amazon RDS for MySQL

- C. MySQL-compatible Amazon Aurora Serverless

- D. MySQL deployed on Amazon EC2 in an Auto Scaling group

**Correct:** B
**Why:** Amazon RDS for MySQL is a managed, cost‑effective, compatible drop‑in with no application or schema changes.

**Incorrect:**
- A: DynamoDB would require a rewrite.
- C: Aurora Serverless can be cost‑effective but may require migration/compatibility testing; RDS MySQL is simpler.
- D: Self-managed MySQL on EC2 increases ops.


---

---

### Question #440

A company used an Amazon RDS for MySQL DB instance during application testing. Before terminating the DB instance at the end of the test cycle, a solutions architect created two backups. The solutions architect created the first backup by using the mysqldump utility to create a database dump. The solutions architect created the second backup by enabling the nal DB snapshot option on RDS termination. The company is now planning for a new test cycle and wants to create a new DB instance from the most recent backup. The company has chosen a MySQL-compatible edition ofAmazon Aurora to host the DB instance. Which solutions will create the new DB instance? (Choose two.)

- A. Import the RDS snapshot directly into Aurora.

- B. Upload the RDS snapshot to Amazon S3. Then import the RDS snapshot into Aurora.

- C. Upload the database dump to Amazon S3. Then import the database dump into Aurora.

- D. Use AWS Database Migration Service (AWS DMS) to import the RDS snapshot into Aurora.

E. Upload the database dump to Amazon S3. Then use AWS Database Migration Service (AWS DMS) to import the database dump into Aurora.

**Correct:** C, E
**Why:** Import from mysqldump in S3 to Aurora or use DMS to load from the dump; you cannot import an RDS snapshot directly into Aurora.

**Incorrect:**
- A: Snapshots are not directly importable to Aurora; DMS ingests data, not snapshots.
- B: Snapshots are not directly importable to Aurora; DMS ingests data, not snapshots.
- D: Snapshots are not directly importable to Aurora; DMS ingests data, not snapshots.


---

---

### Question #462

A company has an application that processes customer orders. The company hosts the application on an Amazon EC2 instance that saves the orders to an Amazon Aurora database. Occasionally when trac is high the workload does not process orders fast enough. What should a solutions architect do to write the orders reliably to the database as quickly as possible?

- A. Increase the instance size of the EC2 instance when trac is high. Write orders to Amazon Simple Notification Service (Amazon SNS). Subscribe the database endpoint to the SNS topic.

- B. Write orders to an Amazon Simple Queue Service (Amazon SQS) queue. Use EC2 instances in an Auto Scaling group behind an Application Load Balancer to read from the SQS queue and process orders into the database.

- C. Write orders to Amazon Simple Notification Service (Amazon SNS). Subscribe the database endpoint to the SNS topic. Use EC2 instances in an Auto Scaling group behind an Application Load Balancer to read from the SNS topic.

- D. Write orders to an Amazon Simple Queue Service (Amazon SQS) queue when the EC2 instance reaches CPU threshold limits. Use scheduled scaling of EC2 instances in an Auto Scaling group behind an Application Load Balancer to read from the SQS queue and process orders into the database.

**Correct:** B
**Why:** Buffer orders to SQS and scale consumers behind an ALB/ASG to write quickly and reliably to Aurora.

**Incorrect:**
- A: SNS lacks durable queue semantics; scaling only on CPU misses spikes; D is reactive.
- C: SNS lacks durable queue semantics; scaling only on CPU misses spikes; D is reactive.
- D: SNS lacks durable queue semantics; scaling only on CPU misses spikes; D is reactive.


---

## Amazon CloudFront

### Question #261

A company recently announced the deployment of its retail website to a global audience. The website runs on multiple Amazon EC2 instances behind an Elastic Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The company wants to provide its customers with different versions of content based on the devices that the customers use to access the website. Which combination of actions should a solutions architect take to meet these requirements? (Choose two.)

- A. Configure Amazon CloudFront to cache multiple versions of the content.

- B. Configure a host header in a Network Load Balancer to forward trac to different instances.

- C. Configure a Lambda@Edge function to send specic objects to users based on the User-Agent header.

- D. Configure AWS Global Accelerator. Forward requests to a Network Load Balancer (NLB). Configure the NLB to set up host-based routing to different EC2 instances.

E. Configure AWS Global Accelerator. Forward requests to a Network Load Balancer (NLB). Configure the NLB to set up path-based routing to different EC2 instances.

**Correct:** A, C
**Why:** Use CloudFront for global caching and Lambda@Edge to vary content based on User‑Agent header.

**Incorrect:**
- B: NLB host/path routing or Global Accelerator aren’t needed for device‑specific content at the edge.
- D: NLB host/path routing or Global Accelerator aren’t needed for device‑specific content at the edge.
- E: NLB host/path routing or Global Accelerator aren’t needed for device‑specific content at the edge.


---

---

### Question #264

A company has a web application hosted over 10 Amazon EC2 instances with trac directed by Amazon Route 53. The company occasionally experiences a timeout error when attempting to browse the application. The networking team nds that some DNS queries return IP addresses of unhealthy instances, resulting in the timeout error. What should a solutions architect implement to overcome these timeout errors?

- A. Create a Route 53 simple routing policy record for each EC2 instance. Associate a health check with each record.

- B. Create a Route 53 failover routing policy record for each EC2 instance. Associate a health check with each record.

- C. Create an Amazon CloudFront distribution with EC2 instances as its origin. Associate a health check with the EC2 instances.

- D. Create an Application Load Balancer (ALB) with a health check in front of the EC2 instances. Route to the ALB from Route 53.

**Correct:** D
**Why:** Put an ALB with health checks in front of instances and route DNS to the ALB to avoid returning unhealthy instance IPs.

**Incorrect:**
- A: Route 53 alone still returns unhealthy A records without ALB health integration.
- B: Route 53 alone still returns unhealthy A records without ALB health integration.
- C: CloudFront is for CDN, not origin health for EC2 fleet.


---

---

### Question #265

A solutions architect needs to design a highly available application consisting of web, application, and database tiers. HTTPS content delivery should be as close to the edge as possible, with the least delivery time. Which solution meets these requirements and is MOST secure?

- A. Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in public subnets. Configure Amazon CloudFront to deliver HTTPS content using the public ALB as the origin.

- B. Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in private subnets. Configure Amazon CloudFront to deliver HTTPS content using the EC2 instances as the origin.

- C. Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in private subnets. Configure Amazon CloudFront to deliver HTTPS content using the public ALB as the origin.

- D. Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in public subnets. Configure Amazon CloudFront to deliver HTTPS content using the EC2 instances as the origin.

**Correct:** C
**Why:** Place EC2 in private subnets behind a public ALB; put CloudFront in front for HTTPS at the edge.

**Incorrect:**
- A: Either expose EC2 publicly or skip CloudFront edge TLS offload.
- B: Either expose EC2 publicly or skip CloudFront edge TLS offload.
- D: Either expose EC2 publicly or skip CloudFront edge TLS offload.


---

---

### Question #266

A company has a popular gaming platform running on AWS. The application is sensitive to latency because latency can impact the user experience and introduce unfair advantages to some players. The application is deployed in every AWS Region. It runs on Amazon EC2 instances that are part of Auto Scaling groups congured behind Application Load Balancers (ALBs). A solutions architect needs to implement a mechanism to monitor the health of the application and redirect trac to healthy endpoints. Which solution meets these requirements?

- A. Configure an accelerator in AWS Global Accelerator. Add a listener for the port that the application listens on, and attach it to a Regional endpoint in each Region. Add the ALB as the endpoint.

- B. Create an Amazon CloudFront distribution and specify the ALB as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the trac.

- C. Create an Amazon CloudFront distribution and specify Amazon S3 as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the trac.

- D. Configure an Amazon DynamoDB database to serve as the data store for the application. Create a DynamoDB Accelerator (DAX) cluster to act as the in-memory cache for DynamoDB hosting the application data.

**Correct:** A
**Why:** Global Accelerator provides health checks and intelligent routing to the nearest healthy Regional endpoint (ALB).

**Incorrect:**
- B: CloudFront/S3 or DynamoDB/DAX don’t meet UDP/TCP app health redirect needs across Regions.
- C: CloudFront/S3 or DynamoDB/DAX don’t meet UDP/TCP app health redirect needs across Regions.
- D: CloudFront/S3 or DynamoDB/DAX don’t meet UDP/TCP app health redirect needs across Regions.


---

---

### Question #272

A company serves a dynamic website from a eet of Amazon EC2 instances behind an Application Load Balancer (ALB). The website needs to support multiple languages to serve customers around the world. The website’s architecture is running in the us-west-1 Region and is exhibiting high request latency for users that are located in other parts of the world. The website needs to serve requests quickly and eciently regardless of a user’s location. However, the company does not want to recreate the existing architecture across multiple Regions. What should a solutions architect do to meet these requirements?

- A. Replace the existing architecture with a website that is served from an Amazon S3 bucket. Configure an Amazon CloudFront distribution with the S3 bucket as the origin. Set the cache behavior settings to cache based on the Accept-Language request header.

- B. Configure an Amazon CloudFront distribution with the ALB as the origin. Set the cache behavior settings to cache based on the Accept- Language request header.

- C. Create an Amazon API Gateway API that is integrated with the ALB. Configure the API to use the HTTP integration type. Set up an API Gateway stage to enable the API cache based on the Accept-Language request header.

- D. Launch an EC2 instance in each additional Region and configure NGINX to act as a cache server for that Region. Put all the EC2 instances and the ALB behind an Amazon Route 53 record set with a geolocation routing policy.

**Correct:** B
**Why:** CloudFront with ALB origin and cache based on Accept‑Language provides low latency globally without multi‑Region.

**Incorrect:**
- A: S3 static site or regional NGINX cache adds complexity or doesn’t fit dynamic origin.
- C: S3 static site or regional NGINX cache adds complexity or doesn’t fit dynamic origin.
- D: S3 static site or regional NGINX cache adds complexity or doesn’t fit dynamic origin.


---

---

### Question #280

A company is using Amazon CloudFront with its website. The company has enabled logging on the CloudFront distribution, and logs are saved in one of the company’s Amazon S3 buckets. The company needs to perform advanced analyses on the logs and build visualizations. What should a solutions architect do to meet these requirements?

- A. Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket. Visualize the results with AWS Glue.

- B. Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket. Visualize the results with Amazon QuickSight.

- C. Use standard SQL queries in Amazon DynamoDB to analyze the CloudFront logs in the S3 bucket. Visualize the results with AWS Glue.

- D. Use standard SQL queries in Amazon DynamoDB to analyze the CloudFront logs in the S3 bucket. Visualize the results with Amazon QuickSight.

**Correct:** B
**Why:** Use Athena to query CloudFront logs in S3 and QuickSight for visualizations.

**Incorrect:**
- A: Glue/DynamoDB are not needed for log analysis/visualization here.
- C: Glue/DynamoDB are not needed for log analysis/visualization here.
- D: Glue/DynamoDB are not needed for log analysis/visualization here.


---

---

### Question #286

A company has a static website that is hosted on Amazon CloudFront in front of Amazon S3. The static website uses a database backend. The company notices that the website does not reect updates that have been made in the website’s Git repository. The company checks the continuous integration and continuous delivery (CI/CD) pipeline between the Git repository and Amazon S3. The company veries that the webhooks are congured properly and that the CI/CD pipeline is sending messages that indicate successful deployments. A solutions architect needs to implement a solution that displays the updates on the website. Which solution will meet these requirements?

- A. Add an Application Load Balancer.

- B. Add Amazon ElastiCache for Redis or Memcached to the database layer of the web application.

- C. Invalidate the CloudFront cache.

- D. Use AWS Certicate Manager (ACM) to validate the website’s SSL certicate.

**Correct:** C
**Why:** Invalidate the CloudFront cache so the CDN serves the latest content from S3.

**Incorrect:**
- A: ALB/ElastiCache/ACM do not address CDN cache staleness.
- B: ALB/ElastiCache/ACM do not address CDN cache staleness.
- D: ALB/ElastiCache/ACM do not address CDN cache staleness.


---

---

### Question #288

A company is migrating a Linux-based web server group to AWS. The web servers must access files in a shared file store for some content. The company must not make any changes to the application. What should a solutions architect do to meet these requirements?

- A. Create an Amazon S3 Standard bucket with access to the web servers.

- B. Configure an Amazon CloudFront distribution with an Amazon S3 bucket as the origin.

- C. Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on all web servers.

- D. Configure a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume to all web servers.

**Correct:** C
**Why:** EFS provides a POSIX shared file system for Linux without app changes.

**Incorrect:**
- A: S3/CloudFront are object/CDN; EBS is single‑attach.
- B: S3/CloudFront are object/CDN; EBS is single‑attach.
- D: S3/CloudFront are object/CDN; EBS is single‑attach.


---

---

### Question #291

A media company uses Amazon CloudFront for its publicly available streaming video content. The company wants to secure the video content that is hosted in Amazon S3 by controlling who has access. Some of the company’s users are using a custom HTTP client that does not support cookies. Some of the company’s users are unable to change the hardcoded URLs that they are using for access. Which services or methods will meet these requirements with the LEAST impact to the users? (Choose two.)

- A. Signed cookies

- B. Signed URLs

- C. AWS AppSync

- D. JSON Web Token (JWT)

E. AWS Secrets Manager

**Correct:** A, B
**Why:** Use signed cookies where clients support cookies and signed URLs where they don’t or URLs are hardcoded.

**Incorrect:**
- C: AppSync/JWT/Secrets Manager do not solve CloudFront authorization distribution.
- D: AppSync/JWT/Secrets Manager do not solve CloudFront authorization distribution.
- E: AppSync/JWT/Secrets Manager do not solve CloudFront authorization distribution.


---

---

### Question #302

A company wants to create a mobile app that allows users to stream slow-motion video clips on their mobile devices. Currently, the app captures video clips and uploads the video clips in raw format into an Amazon S3 bucket. The app retrieves these video clips directly from the S3 bucket. However, the videos are large in their raw format. Users are experiencing issues with buffering and playback on mobile devices. The company wants to implement solutions to maximize the performance and scalability of the app while minimizing operational overhead. Which combination of solutions will meet these requirements? (Choose two.)

- A. Deploy Amazon CloudFront for content delivery and caching.

- B. Use AWS DataSync to replicate the video files across AW'S Regions in other S3 buckets.

- C. Use Amazon Elastic Transcoder to convert the video files to more appropriate formats.

- D. Deploy an Auto Sealing group of Amazon EC2 instances in Local Zones for content delivery and caching.

E. Deploy an Auto Scaling group of Amazon EC2 instances to convert the video files to more appropriate formats.

**Correct:** A, C
**Why:** CloudFront accelerates delivery and caching; Elastic Transcoder (or AWS Elemental MediaConvert) converts to mobile‑friendly formats.

**Incorrect:**
- B: Cross‑Region replication or EC2 fleets add ops without solving core playback/format issues.
- D: Cross‑Region replication or EC2 fleets add ops without solving core playback/format issues.
- E: Cross‑Region replication or EC2 fleets add ops without solving core playback/format issues.


---

---

### Question #310

A company sells datasets to customers who do research in articial intelligence and machine learning (AI/ML). The datasets are large, formatted files that are stored in an Amazon S3 bucket in the us-east-1 Region. The company hosts a web application that the customers use to purchase access to a given dataset. The web application is deployed on multiple Amazon EC2 instances behind an Application Load Balancer. After a purchase is made, customers receive an S3 signed URL that allows access to the files. The customers are distributed across North America and Europe. The company wants to reduce the cost that is associated with data transfers and wants to maintain or improve performance. What should a solutions architect do to meet these requirements?

- A. Configure S3 Transfer Acceleration on the existing S3 bucket. Direct customer requests to the S3 Transfer Acceleration endpoint. Continue to use S3 signed URLs for access control.

- B. Deploy an Amazon CloudFront distribution with the existing S3 bucket as the origin. Direct customer requests to the CloudFront URL. Switch to CloudFront signed URLs for access control.

- C. Set up a second S3 bucket in the eu-central-1 Region with S3 Cross-Region Replication between the buckets. Direct customer requests to the closest Region. Continue to use S3 signed URLs for access control.

- D. Modify the web application to enable streaming of the datasets to end users. Configure the web application to read the data from the existing S3 bucket. Implement access control directly in the application.

**Correct:** B
**Why:** CloudFront in front of S3 reduces egress cost (regional edge caches/edge), improves performance; use signed URLs for access control.

**Incorrect:**
- A: Transfer Acceleration/CRR/streaming app changes are less optimal or higher ops.
- C: Transfer Acceleration/CRR/streaming app changes are less optimal or higher ops.
- D: Transfer Acceleration/CRR/streaming app changes are less optimal or higher ops.


---

---

### Question #313

A company is building a mobile app on AWS. The company wants to expand its reach to millions of users. The company needs to build a platform so that authorized users can watch the company’s content on their mobile devices. What should a solutions architect recommend to meet these requirements?

- A. Publish content to a public Amazon S3 bucket. Use AWS Key Management Service (AWS KMS) keys to stream content.

- B. Set up IPsec VPN between the mobile app and the AWS environment to stream content.

- C. Use Amazon CloudFront. Provide signed URLs to stream content.

- D. Set up AWS Client VPN between the mobile app and the AWS environment to stream content.

**Correct:** C
**Why:** CloudFront with signed URLs securely streams to authorized users at scale on mobile devices.

**Incorrect:**
- A: S3 public/KMS, VPN/Client VPN are not suitable for mass content distribution.
- B: S3 public/KMS, VPN/Client VPN are not suitable for mass content distribution.
- D: S3 public/KMS, VPN/Client VPN are not suitable for mass content distribution.


---

---

### Question #328

A company is hosting a three-tier ecommerce application in the AWS Cloud. The company hosts the website on Amazon S3 and integrates the website with an API that handles sales requests. The company hosts the API on three Amazon EC2 instances behind an Application Load Balancer (ALB). The API consists of static and dynamic front-end content along with backend workers that process sales requests asynchronously. The company is expecting a signicant and sudden increase in the number of sales requests during events for the launch of new products. What should a solutions architect recommend to ensure that all the requests are processed successfully?

- A. Add an Amazon CloudFront distribution for the dynamic content. Increase the number of EC2 instances to handle the increase in trac.

- B. Add an Amazon CloudFront distribution for the static content. Place the EC2 instances in an Auto Scaling group to launch new instances based on network trac.

- C. Add an Amazon CloudFront distribution for the dynamic content. Add an Amazon ElastiCache instance in front of the ALB to reduce trac for the API to handle.

- D. Add an Amazon CloudFront distribution for the static content. Add an Amazon Simple Queue Service (Amazon SQS) queue to receive requests from the website for later processing by the EC2 instances.

**Correct:** D
**Why:** Cache static content with CloudFront and use SQS to buffer backend requests to ensure all are processed.

**Incorrect:**
- A: Dynamic CloudFront or ElastiCache alone won’t handle sudden async workload spikes.
- B: Without queueing, backend can still be overwhelmed.
- C: Dynamic CloudFront or ElastiCache alone won’t handle sudden async workload spikes.


---

---

### Question #333

A company’s application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. On the first day of every month at midnight, the application becomes much slower when the month-end nancial calculation batch runs. This causes the CPU utilization of the EC2 instances to immediately peak to 100%, which disrupts the application. What should a solutions architect recommend to ensure the application is able to handle the workload and avoid downtime?

- A. Configure an Amazon CloudFront distribution in front of the ALB.

- B. Configure an EC2 Auto Scaling simple scaling policy based on CPU utilization.

- C. Configure an EC2 Auto Scaling scheduled scaling policy based on the monthly schedule.

- D. Configure Amazon ElastiCache to remove some of the workload from the EC2 instances.

**Correct:** C
**Why:** Scheduled scaling handles known monthly spikes at midnight without downtime.

**Incorrect:**
- A: CDN or reactive scaling alone may be too late; caching doesn’t fix CPU‑bound computation.
- B: CDN or reactive scaling alone may be too late; caching doesn’t fix CPU‑bound computation.
- D: CDN or reactive scaling alone may be too late; caching doesn’t fix CPU‑bound computation.


---

---

### Question #345

A company wants to restrict access to the content of one of its main web applications and to protect the content by using authorization techniques available on AWS. The company wants to implement a serverless architecture and an authentication solution for fewer than 100 users. The solution needs to integrate with the main web application and serve web content globally. The solution must also scale as the company's user base grows while providing the lowest login latency possible. Which solution will meet these requirements MOST cost-effectively?

- A. Use Amazon Cognito for authentication. Use Lambda@Edge for authorization. Use Amazon CloudFront to serve the web application globally.

- B. Use AWS Directory Service for Microsoft Active Directory for authentication. Use AWS Lambda for authorization. Use an Application Load Balancer to serve the web application globally.

- C. Use Amazon Cognito for authentication. Use AWS Lambda for authorization. Use Amazon S3 Transfer Acceleration to serve the web application globally.

- D. Use AWS Directory Service for Microsoft Active Directory for authentication. Use Lambda@Edge for authorization. Use AWS Elastic Beanstalk to serve the web application globally.

**Correct:** A
**Why:** Cognito for auth, Lambda@Edge for authorization, and CloudFront for global delivery provide a serverless, low‑latency solution.

**Incorrect:**
- B: Directory Service and Beanstalk/ALB add ops;
- C: S3 Transfer Acceleration isn’t for auth.
- D: Directory Service and Beanstalk/ALB add ops;


---

---

### Question #352

A company is designing the network for an online multi-player game. The game uses the UDP networking protocol and will be deployed in eight AWS Regions. The network architecture needs to minimize latency and packet loss to give end users a high-quality gaming experience. Which solution will meet these requirements?

- A. Setup a transit gateway in each Region. Create inter-Region peering attachments between each transit gateway.

- B. Set up AWS Global Accelerator with UDP listeners and endpoint groups in each Region.

- C. Set up Amazon CloudFront with UDP turned on. Configure an origin in each Region.

- D. Set up a VPC peering mesh between each Region. Turn on UDP for each VPC.

**Correct:** B
**Why:** AWS Global Accelerator supports UDP and provides static anycast IPs with global edge network to reduce latency and packet loss across Regions.

**Incorrect:**
- A: Transit gateways and peering do not optimize internet edge-to-Region latency for end users.
- C: CloudFront does not proxy UDP.
- D: VPC peering is for inter-VPC traffic; end users still traverse the internet without edge acceleration.


---

---

### Question #357

A gaming company is moving its public scoreboard from a data center to the AWS Cloud. The company uses Amazon EC2 Windows Server instances behind an Application Load Balancer to host its dynamic application. The company needs a highly available storage solution for the application. The application consists of static files and dynamic server-side code. Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)

- A. Store the static files on Amazon S3. Use Amazon CloudFront to cache objects at the edge.

- B. Store the static files on Amazon S3. Use Amazon ElastiCache to cache objects at the edge.

- C. Store the server-side code on Amazon Elastic File System (Amazon EFS). Mount the EFS volume on each EC2 instance to share the files.

- D. Store the server-side code on Amazon FSx for Windows File Server. Mount the FSx for Windows File Server volume on each EC2 instance to share the files.

E. Store the server-side code on a General Purpose SSD (gp2) Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume on each EC2 instance to share the files.

**Correct:** A, D
**Why:** Store static files in S3 and cache with CloudFront; use FSx for Windows File Server to share server‑side code across Windows EC2 instances.

**Incorrect:**
- B: ElastiCache is not an edge CDN for static files.
- C: EFS is POSIX/NFS (Linux), not ideal for Windows.
- E: EBS volumes are single‑instance; not shareable across instances.


---

---

### Question #358

A social media company runs its application on Amazon EC2 instances behind an Application Load Balancer (ALB). The ALB is the origin for an Amazon CloudFront distribution. The application has more than a billion images stored in an Amazon S3 bucket and processes thousands of images each second. The company wants to resize the images dynamically and serve appropriate formats to clients. Which solution will meet these requirements with the LEAST operational overhead?

- A. Install an external image management library on an EC2 instance. Use the image management library to process the images.

- B. Create a CloudFront origin request policy. Use the policy to automatically resize images and to serve the appropriate format based on the User-Agent HTTP header in the request.

- C. Use a Lambda@Edge function with an external image management library. Associate the Lambda@Edge function with the CloudFront behaviors that serve the images.

- D. Create a CloudFront response headers policy. Use the policy to automatically resize images and to serve the appropriate format based on the User-Agent HTTP header in the request.

**Correct:** C
**Why:** Lambda@Edge can dynamically transform/resize images at the edge with minimal ops and latency.

**Incorrect:**
- A: Managing EC2 image service increases ops and scale burden.
- B: CloudFront policies/headers cannot perform image processing by themselves.
- D: CloudFront policies/headers cannot perform image processing by themselves.


---

---

### Question #367

A company is using Amazon Route 53 latency-based routing to route requests to its UDP-based application for users around the world. The application is hosted on redundant servers in the company's on-premises data centers in the United States, Asia, and Europe. The company’s compliance requirements state that the application must be hosted on premises. The company wants to improve the performance and availability of the application. What should a solutions architect do to meet these requirements?

- A. Configure three Network Load Balancers (NLBs) in the three AWS Regions to address the on-premises endpoints. Create an accelerator by using AWS Global Accelerator, and register the NLBs as its endpoints. Provide access to the application by using a CNAME that points to the accelerator DNS.

- B. Configure three Application Load Balancers (ALBs) in the three AWS Regions to address the on-premises endpoints. Create an accelerator by using AWS Global Accelerator, and register the ALBs as its endpoints. Provide access to the application by using a CNAME that points to the accelerator DNS.

- C. Configure three Network Load Balancers (NLBs) in the three AWS Regions to address the on-premises endpoints. In Route 53, create a latency-based record that points to the three NLBs, and use it as an origin for an Amazon CloudFront distribution. Provide access to the application by using a CNAME that points to the CloudFront DNS.

- D. Configure three Application Load Balancers (ALBs) in the three AWS Regions to address the on-premises endpoints. In Route 53, create a latency-based record that points to the three ALBs, and use it as an origin for an Amazon CloudFront distribution. Provide access to the application by using a CNAME that points to the CloudFront DNS.

**Correct:** A
**Why:** Place NLBs in three Regions that point to on‑prem endpoints; use Global Accelerator to improve performance/availability for UDP traffic while keeping hosting on‑prem.

**Incorrect:**
- B: ALB does not support UDP.
- C: CloudFront is HTTP/HTTPS and not for UDP; also adds unnecessary layers.
- D: CloudFront is HTTP/HTTPS and not for UDP; also adds unnecessary layers.


---

---

### Question #399

A nancial company hosts a web application on AWS. The application uses an Amazon API Gateway Regional API endpoint to give users the ability to retrieve current stock prices. The company’s security team has noticed an increase in the number of API requests. The security team is concerned that HTTP ood attacks might take the application oine. A solutions architect must design a solution to protect the application from this type of attack. Which solution meets these requirements with the LEAST operational overhead?

- A. Create an Amazon CloudFront distribution in front of the API Gateway Regional API endpoint with a maximum TTL of 24 hours.

- B. Create a Regional AWS WAF web ACL with a rate-based rule. Associate the web ACL with the API Gateway stage.

- C. Use Amazon CloudWatch metrics to monitor the Count metric and alert the security team when the predened rate is reached.

- D. Create an Amazon CloudFront distribution with Lambda@Edge in front of the API Gateway Regional API endpoint. Create an AWS Lambda function to block requests from IP addresses that exceed the predened rate.

**Correct:** B
**Why:** A Regional WAF web ACL with a rate‑based rule attached to the API Gateway stage mitigates HTTP flood attacks with low ops.

**Incorrect:**
- A: CloudFront alone doesn’t handle rate limiting for Regional APIs.
- C: Monitoring doesn’t mitigate attacks.
- D: Lambda@Edge scripting is more complex and operationally heavy.


---

---

### Question #416

A rapidly growing global ecommerce company is hosting its web application on AWS. The web application includes static content and dynamic content. The website stores online transaction processing (OLTP) data in an Amazon RDS database The website’s users are experiencing slow page loads. Which combination of actions should a solutions architect take to resolve this issue? (Choose two.)

- A. Configure an Amazon Redshift cluster.

- B. Set up an Amazon CloudFront distribution.

- C. Host the dynamic web content in Amazon S3.

- D. Create a read replica for the RDS DB instance.

E. Configure a Multi-AZ deployment for the RDS DB instance.

**Correct:** B, D
**Why:** CloudFront caches static/dynamic content at edge; RDS read replica offloads reads, improving page load.

**Incorrect:**
- A: Redshift is for OLAP, not OLTP page rendering.
- C: Dynamic content doesn’t belong in S3.
- E: Multi‑AZ is HA, not performance.


---

---

### Question #431

A company has developed a new video game as a web application. The application is in a three-tier architecture in a VPC with Amazon RDS for MySQL in the database layer. Several players will compete concurrently online. The game’s developers want to display a top-10 scoreboard in near- real time and offer the ability to stop and restore the game while preserving the current scores. What should a solutions architect do to meet these requirements?

- A. Set up an Amazon ElastiCache for Memcached cluster to cache the scores for the web application to display.

- B. Set up an Amazon ElastiCache for Redis cluster to compute and cache the scores for the web application to display.

- C. Place an Amazon CloudFront distribution in front of the web application to cache the scoreboard in a section of the application.

- D. Create a read replica on Amazon RDS for MySQL to run queries to compute the scoreboard and serve the read trac to the web application.

**Correct:** B
**Why:** ElastiCache for Redis supports sorted sets for near‑real‑time leaderboards and can persist to preserve scores.

**Incorrect:**
- A: Memcached lacks persistence and sorted‑set operations.
- C: CloudFront caches static content, not compute leaderboards.
- D: RDS read replica queries add latency and load.


---

---

### Question #441

A company hosts a multi-tier web application on Amazon Linux Amazon EC2 instances behind an Application Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The company observes that the Auto Scaling group launches more On-Demand Instances when the application's end users access high volumes of static web content. The company wants to optimize cost. What should a solutions architect do to redesign the application MOST cost-effectively?

- A. Update the Auto Scaling group to use Reserved Instances instead of On-Demand Instances.

- B. Update the Auto Scaling group to scale by launching Spot Instances instead of On-Demand Instances.

- C. Create an Amazon CloudFront distribution to host the static web contents from an Amazon S3 bucket.

- D. Create an AWS Lambda function behind an Amazon API Gateway API to host the static website contents.

**Correct:** C
**Why:** Serve static content from S3 via CloudFront to reduce load on EC2/ALB and lower cost.

**Incorrect:**
- A: Instance pricing choices don’t remove static load driver.
- B: Instance pricing choices don’t remove static load driver.
- D: Lambda+API Gateway is unnecessary for static hosting.


---

---

### Question #443

A company wants to host a scalable web application on AWS. The application will be accessed by users from different geographic regions of the world. Application users will be able to download and upload unique data up to gigabytes in size. The development team wants a cost-effective solution to minimize upload and download latency and maximize performance. What should a solutions architect do to accomplish this?

- A. Use Amazon S3 with Transfer Acceleration to host the application.

- B. Use Amazon S3 with CacheControl headers to host the application.

- C. Use Amazon EC2 with Auto Scaling and Amazon CloudFront to host the application.

- D. Use Amazon EC2 with Auto Scaling and Amazon ElastiCache to host the application.

**Correct:** A
**Why:** S3 Transfer Acceleration provides globally distributed edge ingress/egress to minimize latency for large uploads/downloads.

**Incorrect:**
- B: CacheControl headers do not improve upload latency.
- C: EC2 adds ops and may not minimize global transfer latency.
- D: EC2 adds ops and may not minimize global transfer latency.


---

---

### Question #447

A company has a stateless web application that runs on AWS Lambda functions that are invoked by Amazon API Gateway. The company wants to deploy the application across multiple AWS Regions to provide Regional failover capabilities. What should a solutions architect do to route trac to multiple Regions?

- A. Create Amazon Route 53 health checks for each Region. Use an active-active failover configuration.

- B. Create an Amazon CloudFront distribution with an origin for each Region. Use CloudFront health checks to route trac.

- C. Create a transit gateway. Attach the transit gateway to the API Gateway endpoint in each Region. Configure the transit gateway to route requests.

- D. Create an Application Load Balancer in the primary Region. Set the target group to point to the API Gateway endpoint hostnames in each Region.

**Correct:** A
**Why:** Use Route 53 health checks with active‑active routing across Regions for serverless endpoints.

**Incorrect:**
- B: CloudFront is for HTTP caching, not API multi‑Region routing.
- C: Transit gateway/ALB cannot directly route to API Gateway across Regions as required.
- D: Transit gateway/ALB cannot directly route to API Gateway across Regions as required.


---

---

### Question #461

A company is developing a mobile gaming app in a single AWS Region. The app runs on multiple Amazon EC2 instances in an Auto Scaling group. The company stores the app data in Amazon DynamoDB. The app communicates by using TCP trac and UDP trac between the users and the servers. The application will be used globally. The company wants to ensure the lowest possible latency for all users. Which solution will meet these requirements?

- A. Use AWS Global Accelerator to create an accelerator. Create an Application Load Balancer (ALB) behind an accelerator endpoint that uses Global Accelerator integration and listening on the TCP and UDP ports. Update the Auto Scaling group to register instances on the ALB.

- B. Use AWS Global Accelerator to create an accelerator. Create a Network Load Balancer (NLB) behind an accelerator endpoint that uses Global Accelerator integration and listening on the TCP and UDP ports. Update the Auto Scaling group to register instances on the NLB.

- C. Create an Amazon CloudFront content delivery network (CDN) endpoint. Create a Network Load Balancer (NLB) behind the endpoint and listening on the TCP and UDP ports. Update the Auto Scaling group to register instances on the NLB. Update CloudFront to use the NLB as the origin.

- D. Create an Amazon CloudFront content delivery network (CDN) endpoint. Create an Application Load Balancer (ALB) behind the endpoint and listening on the TCP and UDP ports. Update the Auto Scaling group to register instances on the ALB. Update CloudFront to use the ALB as the origin.

**Correct:** B
**Why:** Global Accelerator with an NLB supports both TCP and UDP and provides the lowest latency globally via the AWS edge.

**Incorrect:**
- A: ALB doesn’t support UDP; CloudFront is not for arbitrary TCP/UDP.
- C: ALB doesn’t support UDP; CloudFront is not for arbitrary TCP/UDP.
- D: ALB doesn’t support UDP; CloudFront is not for arbitrary TCP/UDP.


---

---

### Question #473

A company hosts a website on Amazon EC2 instances behind an Application Load Balancer (ALB). The website serves static content. Website trac is increasing, and the company is concerned about a potential increase in cost.

- A. Create an Amazon CloudFront distribution to cache state files at edge locations

- B. Create an Amazon ElastiCache cluster. Connect the ALB to the ElastiCache cluster to serve cached files

- C. Create an AWS WAF web ACL and associate it with the ALB. Add a rule to the web ACL to cache static files

- D. Create a second ALB in an alternative AWS Region. Route user trac to the closest Region to minimize data transfer costs

**Correct:** A
**Why:** CloudFront caches static files at edge locations, reducing ALB/EC2 load and cost.

**Incorrect:**
- B: ALB cannot connect directly to ElastiCache to serve files.
- C: WAF is for filtering, not caching.
- D: Adding a second ALB/Region doesn’t address caching/cost.


---

---

### Question #486

A company is building a three-tier application on AWS. The presentation tier will serve a static website The logic tier is a containerized application. This application will store data in a relational database. The company wants to simplify deployment and to reduce operational costs. Which solution will meet these requirements?

- A. Use Amazon S3 to host static content. Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for compute power. Use a managed Amazon RDS cluster for the database.

- B. Use Amazon CloudFront to host static content. Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 for compute power. Use a managed Amazon RDS cluster for the database.

- C. Use Amazon S3 to host static content. Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for compute power. Use a managed Amazon RDS cluster for the database.

- D. Use Amazon EC2 Reserved Instances to host static content. Use Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 for compute power. Use a managed Amazon RDS cluster for the database.

**Correct:** A
**Why:** S3 static hosting + ECS Fargate for containers + managed RDS minimizes ops and simplifies deployment.

**Incorrect:**
- B: CloudFront alone doesn’t host; EKS/EC2 increase ops.
- C: CloudFront alone doesn’t host; EKS/EC2 increase ops.
- D: CloudFront alone doesn’t host; EKS/EC2 increase ops.


---

## Amazon CloudWatch & EventBridge

### Question #253

A solutions architect has created two IAM policies: Policy1 and Policy2. Both policies are attached to an IAM group. A cloud engineer is added as an IAM user to the IAM group. The policy documents are:

Policy 1
{
	"Version": "2012-10-17",
	"Statement": [
		{
			"Effect": "Allow",
			"Action": [
				"iam:Get*",
				"iam:List*",
				"kms:List*",
				"ec2:*",
				"ds:*",
				"logs:Get*",
				"logs:Describe*"
			],
			"Resource": "*"
		}
	]
}

Policy 2
{
	"Version": "2012-10-17",
	"Statement": [
		{
			"Effect": "Deny",
			"Action": "ds:Delete*",
			"Resource": "*"
		}
	]
}

Which action will the cloud engineer be able to perform?

- A. Deleting IAM users

- B. Deleting directories

- C. Deleting Amazon EC2 instances

- D. Deleting logs from Amazon CloudWatch Logs

**Correct:** C
**Why:** Policy 1 explicitly allows ec2:* so terminating/deleting EC2 instances is allowed. Policy 2 adds an explicit Deny only for ds:Delete*, which overrides ds:* but does not affect EC2. Deleting IAM users and CloudWatch Logs are not covered by the allowed actions (only iam:Get*/List* and logs:Get*/Describe* are allowed).

**Incorrect:**
- A: Not allowed. Only read-only IAM permissions (iam:Get*/List*); DeleteUser requires iam:DeleteUser.
- B: Explicitly denied by Policy 2 (ds:Delete*), which overrides any Allow.
- D: Not allowed. Only logs:Get*/Describe* are allowed; deletions require logs:Delete* permissions.


---

---

### Question #257

A company is building a solution that will report Amazon EC2 Auto Scaling events across all the applications in an AWS account. The company needs to use a serverless solution to store the EC2 Auto Scaling status data in Amazon S3. The company then will use the data in Amazon S3 to provide near-real-time updates in a dashboard. The solution must not affect the speed of EC2 instance launches. How should the company move the data to Amazon S3 to meet these requirements?

- A. Use an Amazon CloudWatch metric stream to send the EC2 Auto Scaling status data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.

- B. Launch an Amazon EMR cluster to collect the EC2 Auto Scaling status data and send the data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.

- C. Create an Amazon EventBridge rule to invoke an AWS Lambda function on a schedule. Configure the Lambda function to send the EC2 Auto Scaling status data directly to Amazon S3.

- D. Use a bootstrap script during the launch of an EC2 instance to install Amazon Kinesis Agent. Configure Kinesis Agent to collect the EC2 Auto Scaling status data and send the data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.

**Correct:** A
**Why:** CloudWatch metric streams → Kinesis Data Firehose → S3 is serverless and near real time without impacting launches.

**Incorrect:**
- B: EMR/Kinesis Agent add unnecessary infrastructure.
- C: Scheduled polling adds latency and complexity.
- D: EMR/Kinesis Agent add unnecessary infrastructure.


---

---

### Question #259

A company is implementing new data retention policies for all databases that run on Amazon RDS DB instances. The company must retain daily backups for a minimum period of 2 years. The backups must be consistent and restorable. Which solution should a solutions architect recommend to meet these requirements?

- A. Create a backup vault in AWS Backup to retain RDS backups. Create a new backup plan with a daily schedule and an expiration period of 2 years after creation. Assign the RDS DB instances to the backup plan.

- B. Configure a backup window for the RDS DB instances for daily snapshots. Assign a snapshot retention policy of 2 years to each RDS DB instance. Use Amazon Data Lifecycle Manager (Amazon DLM) to schedule snapshot deletions.

- C. Configure database transaction logs to be automatically backed up to Amazon CloudWatch Logs with an expiration period of 2 years.

- D. Configure an AWS Database Migration Service (AWS DMS) replication task. Deploy a replication instance, and configure a change data capture (CDC) task to stream database changes to Amazon S3 as the target. Configure S3 Lifecycle policies to delete the snapshots after 2 years.

**Correct:** A
**Why:** AWS Backup can enforce daily backups and retain for 2 years with minimal ops.

**Incorrect:**
- B: DLM is for EBS; RDS snapshots retention isn’t managed that way.
- C: CloudWatch logs/DMS exports are not consistent RDS backups.
- D: CloudWatch logs/DMS exports are not consistent RDS backups.


---

---

### Question #278

A company wants to create an application to store employee data in a hierarchical structured relationship. The company needs a minimum-latency response to high-trac queries for the employee data and must protect any sensitive data. The company also needs to receive monthly email messages if any nancial information is present in the employee data. Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)

- A. Use Amazon Redshift to store the employee data in hierarchies. Unload the data to Amazon S3 every month.

- B. Use Amazon DynamoDB to store the employee data in hierarchies. Export the data to Amazon S3 every month.

- C. Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly events to AWS Lambda.

- D. Use Amazon Athena to analyze the employee data in Amazon S3. Integrate Athena with Amazon QuickSight to publish analysis dashboards and share the dashboards with users.

E. Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly notications through an Amazon Simple Notification Service (Amazon SNS) subscription.

**Correct:** B, E
**Why:** DynamoDB suits hierarchical, low‑latency queries; Macie + EventBridge → SNS can notify monthly when financial data is detected.

**Incorrect:**
- A: Redshift/Athena aren’t optimized for low‑latency hierarchical queries.
- C: Lambda trigger pattern is unnecessary for monthly notifications.
- D: Redshift/Athena aren’t optimized for low‑latency hierarchical queries.


---

---

### Question #279

A company has an application that is backed by an Amazon DynamoDB table. The company’s compliance requirements specify that database backups must be taken every month, must be available for 6 months, and must be retained for 7 years. Which solution will meet these requirements?

- A. Create an AWS Backup plan to back up the DynamoDB table on the first day of each month. Specify a lifecycle policy that transitions the backup to cold storage after 6 months. Set the retention period for each backup to 7 years.

- B. Create a DynamoDB on-demand backup of the DynamoDB table on the first day of each month. Transition the backup to Amazon S3 Glacier Flexible Retrieval after 6 months. Create an S3 Lifecycle policy to delete backups that are older than 7 years.

- C. Use the AWS SDK to develop a script that creates an on-demand backup of the DynamoDB table. Set up an Amazon EventBridge rule that runs the script on the first day of each month. Create a second script that will run on the second day of each month to transition DynamoDB backups that are older than 6 months to cold storage and to delete backups that are older than 7 years.

- D. Use the AWS CLI to create an on-demand backup of the DynamoDB table. Set up an Amazon EventBridge rule that runs the command on the first day of each month with a cron expression. Specify in the command to transition the backups to cold storage after 6 months and to delete the backups after 7 years.

**Correct:** A
**Why:** AWS Backup monthly backup plan with lifecycle to cold storage after 6 months and 7‑year retention meets requirements.

**Incorrect:**
- B: Manual or partial solutions add ops and risk consistency.
- C: Manual or partial solutions add ops and risk consistency.
- D: Manual or partial solutions add ops and risk consistency.


---

---

### Question #297

A company deploys an application on ve Amazon EC2 instances. An Application Load Balancer (ALB) distributes trac to the instances by using a target group. The average CPU usage on each of the instances is below 10% most of the time, with occasional surges to 65%. A solutions architect needs to implement a solution to automate the scalability of the application. The solution must optimize the cost of the architecture and must ensure that the application has enough CPU resources when surges occur. Which solution will meet these requirements?

- A. Create an Amazon CloudWatch alarm that enters the ALARM state when the CPUUtilization metric is less than 20%. Create an AWS Lambda function that the CloudWatch alarm invokes to terminate one of the EC2 instances in the ALB target group.

- B. Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing target group as the target group. Set a target tracking scaling policy that is based on the ASGAverageCPUUtilization metric. Set the minimum instances to 2, the desired capacity to 3, the maximum instances to 6, and the target value to 50%. Add the EC2 instances to the Auto Scaling group.

- C. Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing target group as the target group. Set the minimum instances to 2, the desired capacity to 3, and the maximum instances to 6. Add the EC2 instances to the Auto Scaling group.

- D. Create two Amazon CloudWatch alarms. Configure the first CloudWatch alarm to enter the ALARM state when the average CPUUtilization metric is below 20%. Configure the second CloudWatch alarm to enter the ALARM state when the average CPUUtilization matric is above 50%. Configure the alarms to publish to an Amazon Simple Notification Service (Amazon SNS) topic to send an email message. After receiving the message, log in to decrease or increase the number of EC2 instances that are running.

**Correct:** B
**Why:** Create an Auto Scaling group with target tracking on CPU and attach to existing ALB with appropriate min/desired/max.

**Incorrect:**
- A: Manual alarms/emails are not automation.
- C: Missing scaling policy.
- D: Manual alarms/emails are not automation.


---

---

### Question #303

A company is launching a new application deployed on an Amazon Elastic Container Service (Amazon ECS) cluster and is using the Fargate launch type for ECS tasks. The company is monitoring CPU and memory usage because it is expecting high trac to the application upon its launch. However, the company wants to reduce costs when utilization decreases. What should a solutions architect recommend?

- A. Use Amazon EC2 Auto Scaling to scale at certain periods based on previous trac patterns.

- B. Use an AWS Lambda function to scale Amazon ECS based on metric breaches that trigger an Amazon CloudWatch alarm.

- C. Use Amazon EC2 Auto Scaling with simple scaling policies to scale when ECS metric breaches trigger an Amazon CloudWatch alarm.

- D. Use AWS Application Auto Scaling with target tracking policies to scale when ECS metric breaches trigger an Amazon CloudWatch alarm.

**Correct:** D
**Why:** Use Application Auto Scaling target tracking for ECS on Fargate based on CPU/memory metrics.

**Incorrect:**
- A: EC2 Auto Scaling not applicable to Fargate tasks.
- B: Lambda intermediates are unnecessary.
- C: EC2 Auto Scaling not applicable to Fargate tasks.


---

---

### Question #309

A solutions architect needs to optimize storage costs. The solutions architect must identify any Amazon S3 buckets that are no longer being accessed or are rarely accessed. Which solution will accomplish this goal with the LEAST operational overhead?

- A. Analyze bucket access patterns by using the S3 Storage Lens dashboard for advanced activity metrics.

- B. Analyze bucket access patterns by using the S3 dashboard in the AWS Management Console.

- C. Turn on the Amazon CloudWatch BucketSizeBytes metric for buckets. Analyze bucket access patterns by using the metrics data with Amazon Athena.

- D. Turn on AWS CloudTrail for S3 object monitoring. Analyze bucket access patterns by using CloudTrail logs that are integrated with Amazon CloudWatch Logs.

**Correct:** A
**Why:** S3 Storage Lens advanced metrics identify buckets with low/rare access with least ops.

**Incorrect:**
- B: Console metrics, CloudWatch, or CloudTrail require more work and are less precise.
- C: Console metrics, CloudWatch, or CloudTrail require more work and are less precise.
- D: Console metrics, CloudWatch, or CloudTrail require more work and are less precise.


---

---

### Question #316

A company uses an Amazon EC2 instance to run a script to poll for and process messages in an Amazon Simple Queue Service (Amazon SQS) queue. The company wants to reduce operational costs while maintaining its ability to process a growing number of messages that are added to the queue. What should a solutions architect recommend to meet these requirements?

- A. Increase the size of the EC2 instance to process messages faster.

- B. Use Amazon EventBridge to turn off the EC2 instance when the instance is underutilized.

- C. Migrate the script on the EC2 instance to an AWS Lambda function with the appropriate runtime.

- D. Use AWS Systems Manager Run Command to run the script on demand.

**Correct:** C
**Why:** Replace the EC2 poller with a Lambda function subscribed to SQS; scales automatically, lower cost.

**Incorrect:**
- A: Larger instance, turning off, or Run Command do not reduce ops or scale on demand.
- B: Larger instance, turning off, or Run Command do not reduce ops or scale on demand.
- D: Larger instance, turning off, or Run Command do not reduce ops or scale on demand.


---

---

### Question #317

A company uses a legacy application to produce data in CSV format. The legacy application stores the output data in Amazon S3. The company is deploying a new commercial off-the-shelf (COTS) application that can perform complex SQL queries to analyze data that is stored in Amazon Redshift and Amazon S3 only. However, the COTS application cannot process the .csv files that the legacy application produces. The company cannot update the legacy application to produce data in another format. The company needs to implement a solution so that the COTS application can use the data that the legacy application produces. Which solution will meet these requirements with the LEAST operational overhead?

- A. Create an AWS Glue extract, transform, and load (ETL) job that runs on a schedule. Configure the ETL job to process the .csv files and store the processed data in Amazon Redshift.

- B. Develop a Python script that runs on Amazon EC2 instances to convert the .csv files to .sql files. Invoke the Python script on a cron schedule to store the output files in Amazon S3.

- C. Create an AWS Lambda function and an Amazon DynamoDB table. Use an S3 event to invoke the Lambda function. Configure the Lambda function to perform an extract, transform, and load (ETL) job to process the .csv files and store the processed data in the DynamoDB table.

- D. Use Amazon EventBridge to launch an Amazon EMR cluster on a weekly schedule. Configure the EMR cluster to perform an extract, transform, and load (ETL) job to process the .csv files and store the processed data in an Amazon Redshift table.

**Correct:** A
**Why:** Glue ETL job on a schedule to process CSVs and load into Redshift with least ops.

**Incorrect:**
- B: EC2 scripts, DynamoDB, or EMR add ops for the use case.
- C: EC2 scripts, DynamoDB, or EMR add ops for the use case.
- D: EC2 scripts, DynamoDB, or EMR add ops for the use case.


---

---

### Question #329

A security audit reveals that Amazon EC2 instances are not being patched regularly. A solutions architect needs to provide a solution that will run regular security scans across a large eet of EC2 instances. The solution should also patch the EC2 instances on a regular schedule and provide a report of each instance’s patch status. Which solution will meet these requirements?

- A. Set up Amazon Macie to scan the EC2 instances for software vulnerabilities. Set up a cron job on each EC2 instance to patch the instance on a regular schedule.

- B. Turn on Amazon GuardDuty in the account. Configure GuardDuty to scan the EC2 instances for software vulnerabilities. Set up AWS Systems Manager Session Manager to patch the EC2 instances on a regular schedule.

- C. Set up Amazon Detective to scan the EC2 instances for software vulnerabilities. Set up an Amazon EventBridge scheduled rule to patch the EC2 instances on a regular schedule.

- D. Turn on Amazon Inspector in the account. Configure Amazon Inspector to scan the EC2 instances for software vulnerabilities. Set up AWS Systems Manager Patch Manager to patch the EC2 instances on a regular schedule.

**Correct:** D
**Why:** Inspector for vulnerability scanning and Systems Manager Patch Manager for scheduled patching with compliance reports.

**Incorrect:**
- A: Macie/GuardDuty/Detective do not patch or scan OS packages.
- B: Macie/GuardDuty/Detective do not patch or scan OS packages.
- C: Macie/GuardDuty/Detective do not patch or scan OS packages.


---

---

### Question #335

A company is experiencing sudden increases in demand. The company needs to provision large Amazon EC2 instances from an Amazon Machine Image (AMI). The instances will run in an Auto Scaling group. The company needs a solution that provides minimum initialization latency to meet the demand. Which solution meets these requirements?

- A. Use the aws ec2 register-image command to create an AMI from a snapshot. Use AWS Step Functions to replace the AMI in the Auto Scaling group.

- B. Enable Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot. Provision an AMI by using the snapshot. Replace the AMI in the Auto Scaling group with the new AMI.

- C. Enable AMI creation and dene lifecycle rules in Amazon Data Lifecycle Manager (Amazon DLM). Create an AWS Lambda function that modies the AMI in the Auto Scaling group.

- D. Use Amazon EventBridge to invoke AWS Backup lifecycle policies that provision AMIs. Configure Auto Scaling group capacity limits as an event source in EventBridge.

**Correct:** B
**Why:** Enable EBS Fast Snapshot Restore on the AMI’s snapshot to minimize initialization latency during scale‑out.

**Incorrect:**
- A: Register‑image/DLM/Backup events don’t directly reduce restore latency.
- C: Register‑image/DLM/Backup events don’t directly reduce restore latency.
- D: Register‑image/DLM/Backup events don’t directly reduce restore latency.


---

---

### Question #342

A transaction processing company has weekly scripted batch jobs that run on Amazon EC2 instances. The EC2 instances are in an Auto Scaling group. The number of transactions can vary, but the baseline CPU utilization that is noted on each run is at least 60%. The company needs to provision the capacity 30 minutes before the jobs run. Currently, engineers complete this task by manually modifying the Auto Scaling group parameters. The company does not have the resources to analyze the required capacity trends for the Auto Scaling group counts. The company needs an automated way to modify the Auto Scaling group’s desired capacity. Which solution will meet these requirements with the LEAST operational overhead?

- A. Create a dynamic scaling policy for the Auto Scaling group. Configure the policy to scale based on the CPU utilization metric. Set the target value for the metric to 60%.

- B. Create a scheduled scaling policy for the Auto Scaling group. Set the appropriate desired capacity, minimum capacity, and maximum capacity. Set the recurrence to weekly. Set the start time to 30 minutes before the batch jobs run.

- C. Create a predictive scaling policy for the Auto Scaling group. Configure the policy to scale based on forecast. Set the scaling metric to CPU utilization. Set the target value for the metric to 60%. In the policy, set the instances to pre-launch 30 minutes before the jobs run.

- D. Create an Amazon EventBridge event to invoke an AWS Lambda function when the CPU utilization metric value for the Auto Scaling group reaches 60%. Configure the Lambda function to increase the Auto Scaling group’s desired capacity and maximum capacity by 20%.

**Correct:** C
**Why:** Predictive scaling can forecast and pre‑launch capacity 30 minutes before the weekly batch run.

**Incorrect:**
- A: Dynamic/scheduled without forecasting or EventBridge on metric threshold won’t proactively pre‑launch as needed.
- B: Dynamic/scheduled without forecasting or EventBridge on metric threshold won’t proactively pre‑launch as needed.
- D: Dynamic/scheduled without forecasting or EventBridge on metric threshold won’t proactively pre‑launch as needed.


---

---

### Question #344

A company has a Java application that uses Amazon Simple Queue Service (Amazon SQS) to parse messages. The application cannot parse messages that are larger than 256 KB in size. The company wants to implement a solution to give the application the ability to parse messages as large as 50 MB. Which solution will meet these requirements with the FEWEST changes to the code?

- A. Use the Amazon SQS Extended Client Library for Java to host messages that are larger than 256 KB in Amazon S3.

- B. Use Amazon EventBridge to post large messages from the application instead of Amazon SQS.

- C. Change the limit in Amazon SQS to handle messages that are larger than 256 KB.

- D. Store messages that are larger than 256 KB in Amazon Elastic File System (Amazon EFS). Configure Amazon SQS to reference this location in the messages.

**Correct:** A
**Why:** SQS Extended Client Library stores message payloads in S3 to overcome the 256 KB limit up to 2–50 MB.

**Incorrect:**
- B: EventBridge isn’t a queue; SQS cannot raise native limit; EFS indirection isn’t integrated.
- C: EventBridge isn’t a queue; SQS cannot raise native limit; EFS indirection isn’t integrated.
- D: EventBridge isn’t a queue; SQS cannot raise native limit; EFS indirection isn’t integrated.


---

---

### Question #351

A company is moving its data management application to AWS. The company wants to transition to an event-driven architecture. The architecture needs to be more distributed and to use serverless concepts while performing the different aspects of the workow. The company also wants to minimize operational overhead. Which solution will meet these requirements?

- A. Build out the workow in AWS Glue. Use AWS Glue to invoke AWS Lambda functions to process the workow steps.

- B. Build out the workow in AWS Step Functions. Deploy the application on Amazon EC2 instances. Use Step Functions to invoke the workow steps on the EC2 instances.

- C. Build out the workow in Amazon EventBridge. Use EventBridge to invoke AWS Lambda functions on a schedule to process the workow steps.

- D. Build out the workow in AWS Step Functions. Use Step Functions to create a state machine. Use the state machine to invoke AWS Lambda functions to process the workow steps.

**Correct:** D
**Why:** Step Functions orchestrates serverless, event-driven workflows with a state machine that invokes Lambda for steps, minimizing ops and providing retries and branching.

**Incorrect:**
- A: AWS Glue is for ETL, not general workflow orchestration; invoking Lambda from Glue adds complexity.
- B: Orchestrating EC2 increases operational overhead and is not serverless.
- C: EventBridge schedules are time-based, not a workflow engine; you still need orchestration and step control.


---

---

### Question #363

A company is building a game system that needs to send unique events to separate leaderboard, matchmaking, and authentication services concurrently. The company needs an AWS event-driven system that guarantees the order of the events. Which solution will meet these requirements?

- A. Amazon EventBridge event bus

- B. Amazon Simple Notification Service (Amazon SNS) FIFO topics

- C. Amazon Simple Notification Service (Amazon SNS) standard topics

- D. Amazon Simple Queue Service (Amazon SQS) FIFO queues

**Correct:** B
**Why:** SNS FIFO topics provide ordered, exactly‑once message fanout to multiple subscribers, meeting concurrency with order guarantees.

**Incorrect:**
- A: EventBridge does not guarantee strict ordering across targets.
- C: SNS standard topics are at‑least‑once and unordered.
- D: A single SQS FIFO queue cannot fan out to multiple services concurrently without extra components.


---

---

### Question #368

A solutions architect wants all new users to have specic complexity requirements and mandatory rotation periods for IAM user passwords. What should the solutions architect do to accomplish this?

- A. Set an overall password policy for the entire AWS account.

- B. Set a password policy for each IAM user in the AWS account.

- C. Use third-party vendor software to set password requirements.

- D. Attach an Amazon CloudWatch rule to the Create_newuser event to set the password with the appropriate requirements.

**Correct:** A
**Why:** The account‑level IAM password policy enforces complexity and rotation for all new IAM users.

**Incorrect:**
- B: Per‑user policies are not scalable.
- C: Third‑party tools are unnecessary.
- D: CloudWatch rule cannot enforce password policy on user creation.


---

---

### Question #369

A company has migrated an application to Amazon EC2 Linux instances. One of these EC2 instances runs several 1-hour tasks on a schedule. These tasks were written by different teams and have no common programming language. The company is concerned about performance and scalability while these tasks run on a single instance. A solutions architect needs to implement a solution to resolve these concerns. Which solution will meet these requirements with the LEAST operational overhead?

- A. Use AWS Batch to run the tasks as jobs. Schedule the jobs by using Amazon EventBridge (Amazon CloudWatch Events).

- B. Convert the EC2 instance to a container. Use AWS App Runner to create the container on demand to run the tasks as jobs.

- C. Copy the tasks into AWS Lambda functions. Schedule the Lambda functions by using Amazon EventBridge (Amazon CloudWatch Events).

- D. Create an Amazon Machine Image (AMI) of the EC2 instance that runs the tasks. Create an Auto Scaling group with the AMI to run multiple copies of the instance.

**Correct:** A
**Why:** AWS Batch runs heterogeneous jobs (any language) as managed jobs and can be scheduled with EventBridge, improving scalability with minimal ops.

**Incorrect:**
- B: App Runner targets web services/containers, not scheduled arbitrary jobs as simply.
- C: Rewriting tasks into Lambda functions is heavy and may hit limits.
- D: Cloning instances increases cost/ops and doesn’t address scheduling/queuing.


---

---

### Question #375

An ecommerce company is building a distributed application that involves several serverless functions and AWS services to complete order- processing tasks. These tasks require manual approvals as part of the workow. A solutions architect needs to design an architecture for the order-processing application. The solution must be able to combine multiple AWS Lambda functions into responsive serverless applications. The solution also must orchestrate data and services that run on Amazon EC2 instances, containers, or on-premises servers. Which solution will meet these requirements with the LEAST operational overhead?

- A. Use AWS Step Functions to build the application.

- B. Integrate all the application components in an AWS Glue job.

- C. Use Amazon Simple Queue Service (Amazon SQS) to build the application.

- D. Use AWS Lambda functions and Amazon EventBridge events to build the application.

**Correct:** A
**Why:** Step Functions orchestrates serverless and non‑serverless tasks, supports human approval steps, and simplifies complex workflows.

**Incorrect:**
- B: AWS Glue is for data ETL, not general orchestration with approvals.
- C: SQS/EventBridge alone do not provide orchestration/state management.
- D: SQS/EventBridge alone do not provide orchestration/state management.


---

---

### Question #380

A company is migrating its on-premises workload to the AWS Cloud. The company already uses several Amazon EC2 instances and Amazon RDS DB instances. The company wants a solution that automatically starts and stops the EC2 instances and DB instances outside of business hours. The solution must minimize cost and infrastructure maintenance. Which solution will meet these requirements?

- A. Scale the EC2 instances by using elastic resize. Scale the DB instances to zero outside of business hours.

- B. Explore AWS Marketplace for partner solutions that will automatically start and stop the EC2 instances and DB instances on a schedule.

- C. Launch another EC2 instance. Configure a crontab schedule to run shell scripts that will start and stop the existing EC2 instances and DB instances on a schedule.

- D. Create an AWS Lambda function that will start and stop the EC2 instances and DB instances. Configure Amazon EventBridge to invoke the Lambda function on a schedule.

**Correct:** D
**Why:** An EventBridge‑scheduled Lambda can start/stop EC2 and RDS on a schedule with minimal cost and ops.

**Incorrect:**
- A: EC2 “elastic resize” and RDS scale‑to‑zero do not apply.
- B: Marketplace adds cost/complexity.
- C: Managing a cron EC2 instance adds ops.


---

---

### Question #393

A payment processing company records all voice communication with its customers and stores the audio files in an Amazon S3 bucket. The company needs to capture the text from the audio files. The company must remove from the text any personally identiable information (PII) that belongs to customers. What should a solutions architect do to meet these requirements?

- A. Process the audio files by using Amazon Kinesis Video Streams. Use an AWS Lambda function to scan for known PII patterns.

- B. When an audio file is uploaded to the S3 bucket, invoke an AWS Lambda function to start an Amazon Textract task to analyze the call recordings.

- C. Configure an Amazon Transcribe transcription job with PII redaction turned on. When an audio file is uploaded to the S3 bucket, invoke an AWS Lambda function to start the transcription job. Store the output in a separate S3 bucket.

- D. Create an Amazon Connect contact flow that ingests the audio files with transcription turned on. Embed an AWS Lambda function to scan for known PII patterns. Use Amazon EventBridge to start the contact flow when an audio file is uploaded to the S3 bucket.

**Correct:** C
**Why:** Amazon Transcribe supports PII redaction; invoke jobs on upload and store sanitized text output separately.

**Incorrect:**
- A: Not appropriate services for transcription and PII redaction here.
- B: Textract is for documents, not audio.
- D: Not appropriate services for transcription and PII redaction here.


---

---

### Question #394

A company is running a multi-tier ecommerce web application in the AWS Cloud. The application runs on Amazon EC2 instances with an Amazon RDS for MySQL Multi-AZ DB instance. Amazon RDS is congured with the latest generation DB instance with 2,000 GB of storage in a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume. The database performance affects the application during periods of high demand. A database administrator analyzes the logs in Amazon CloudWatch Logs and discovers that the application performance always degrades when the number of read and write IOPS is higher than 20,000. What should a solutions architect do to improve the application performance?

- A. Replace the volume with a magnetic volume.

- B. Increase the number of IOPS on the gp3 volume.

- C. Replace the volume with a Provisioned IOPS SSD (io2) volume.

- D. Replace the 2,000 GB gp3 volume with two 1,000 GB gp3 volumes.

**Correct:** C
**Why:** Move to Provisioned IOPS SSD (io2) to exceed 20K IOPS limits of gp3 and ensure consistent high IOPS for peak periods.

**Incorrect:**
- A: Magnetic volumes are low performance.
- B: gp3 max IOPS is 16K—insufficient for >20K IOPS.
- D: Splitting gp3 volumes does not increase single‑volume IOPS for RDS.


---

---

### Question #397

An ecommerce company needs to run a scheduled daily job to aggregate and filter sales records for analytics. The company stores the sales records in an Amazon S3 bucket. Each object can be up to 10 GB in size. Based on the number of sales events, the job can take up to an hour to complete. The CPU and memory usage of the job are constant and are known in advance. A solutions architect needs to minimize the amount of operational effort that is needed for the job to run. Which solution meets these requirements?

- A. Create an AWS Lambda function that has an Amazon EventBridge notification. Schedule the EventBridge event to run once a day.

- B. Create an AWS Lambda function. Create an Amazon API Gateway HTTP API, and integrate the API with the function. Create an Amazon EventBridge scheduled event that calls the API and invokes the function.

- C. Create an Amazon Elastic Container Service (Amazon ECS) cluster with an AWS Fargate launch type. Create an Amazon EventBridge scheduled event that launches an ECS task on the cluster to run the job.

- D. Create an Amazon Elastic Container Service (Amazon ECS) cluster with an Amazon EC2 launch type and an Auto Scaling group with at least one EC2 instance. Create an Amazon EventBridge scheduled event that launches an ECS task on the cluster to run the job.

**Correct:** C
**Why:** ECS on Fargate with an EventBridge schedule runs the containerized job with known CPU/memory, minimizing ops (no servers to manage).

**Incorrect:**
- A: Lambda is not suitable for up to 10 GB objects and 1‑hour jobs.
- B: Lambda is not suitable for up to 10 GB objects and 1‑hour jobs.
- D: EC2 launch type requires managing instances/ASG.


---

---

### Question #399

A nancial company hosts a web application on AWS. The application uses an Amazon API Gateway Regional API endpoint to give users the ability to retrieve current stock prices. The company’s security team has noticed an increase in the number of API requests. The security team is concerned that HTTP ood attacks might take the application oine. A solutions architect must design a solution to protect the application from this type of attack. Which solution meets these requirements with the LEAST operational overhead?

- A. Create an Amazon CloudFront distribution in front of the API Gateway Regional API endpoint with a maximum TTL of 24 hours.

- B. Create a Regional AWS WAF web ACL with a rate-based rule. Associate the web ACL with the API Gateway stage.

- C. Use Amazon CloudWatch metrics to monitor the Count metric and alert the security team when the predened rate is reached.

- D. Create an Amazon CloudFront distribution with Lambda@Edge in front of the API Gateway Regional API endpoint. Create an AWS Lambda function to block requests from IP addresses that exceed the predened rate.

**Correct:** B
**Why:** A Regional WAF web ACL with a rate‑based rule attached to the API Gateway stage mitigates HTTP flood attacks with low ops.

**Incorrect:**
- A: CloudFront alone doesn’t handle rate limiting for Regional APIs.
- C: Monitoring doesn’t mitigate attacks.
- D: Lambda@Edge scripting is more complex and operationally heavy.


---

---

### Question #433

A company is running its production and nonproduction environment workloads in multiple AWS accounts. The accounts are in an organization in AWS Organizations. The company needs to design a solution that will prevent the modication of cost usage tags. Which solution will meet these requirements?

- A. Create a custom AWS Cong rule to prevent tag modication except by authorized principals.

- B. Create a custom trail in AWS CloudTrail to prevent tag modication.

- C. Create a service control policy (SCP) to prevent tag modication except by authorized principals.

- D. Create custom Amazon CloudWatch logs to prevent tag modication.

**Correct:** C
**Why:** Use an SCP in Organizations to centrally prevent tag modification except for authorized principals.

**Incorrect:**
- A: Config/CloudTrail/CloudWatch cannot enforce prevention like SCPs.
- B: Config/CloudTrail/CloudWatch cannot enforce prevention like SCPs.
- D: Config/CloudTrail/CloudWatch cannot enforce prevention like SCPs.


---

---

### Question #434

A company hosts its application in the AWS Cloud. The application runs on Amazon EC2 instances behind an Elastic Load Balancer in an Auto Scaling group and with an Amazon DynamoDB table. The company wants to ensure the application can be made available in anotherAWS Region with minimal downtime. What should a solutions architect do to meet these requirements with the LEAST amount of downtime?

- A. Create an Auto Scaling group and a load balancer in the disaster recovery Region. Configure the DynamoDB table as a global table. Configure DNS failover to point to the new disaster recovery Region's load balancer.

- B. Create an AWS CloudFormation template to create EC2 instances, load balancers, and DynamoDB tables to be launched when needed Configure DNS failover to point to the new disaster recovery Region's load balancer.

- C. Create an AWS CloudFormation template to create EC2 instances and a load balancer to be launched when needed. Configure the DynamoDB table as a global table. Configure DNS failover to point to the new disaster recovery Region's load balancer.

- D. Create an Auto Scaling group and load balancer in the disaster recovery Region. Configure the DynamoDB table as a global table. Create an Amazon CloudWatch alarm to trigger an AWS Lambda function that updates Amazon Route 53 pointing to the disaster recovery load balancer.

**Correct:** A
**Why:** Pre‑provision ALB/ASG in DR Region and use DynamoDB global tables; DNS failover minimizes downtime.

**Incorrect:**
- B: Delayed provisioning increases downtime.
- C: Delayed provisioning increases downtime.
- D: Extra Lambda automation is unnecessary.


---

---

### Question #444

A company has hired a solutions architect to design a reliable architecture for its application. The application consists of one Amazon RDS DB instance and two manually provisioned Amazon EC2 instances that run web servers. The EC2 instances are located in a single Availability Zone. An employee recently deleted the DB instance, and the application was unavailable for 24 hours as a result. The company is concerned with the overall reliability of its environment. What should the solutions architect do to maximize reliability of the application's infrastructure?

- A. Delete one EC2 instance and enable termination protection on the other EC2 instance. Update the DB instance to be Multi-AZ, and enable deletion protection.

- B. Update the DB instance to be Multi-AZ, and enable deletion protection. Place the EC2 instances behind an Application Load Balancer, and run them in an EC2 Auto Scaling group across multiple Availability Zones.

- C. Create an additional DB instance along with an Amazon API Gateway and an AWS Lambda function. Configure the application to invoke the Lambda function through API Gateway. Have the Lambda function write the data to the two DB instances.

- D. Place the EC2 instances in an EC2 Auto Scaling group that has multiple subnets located in multiple Availability Zones. Use Spot Instances instead of On-Demand Instances. Set up Amazon CloudWatch alarms to monitor the health of the instances Update the DB instance to be Multi-AZ, and enable deletion protection.

**Correct:** B
**Why:** Protect RDS with Multi‑AZ and deletion protection; place EC2 in ALB+Auto Scaling across AZs to maximize reliability.

**Incorrect:**
- A: Either reduce availability or add unnecessary components/cost.
- C: Either reduce availability or add unnecessary components/cost.
- D: Either reduce availability or add unnecessary components/cost.


---

---

### Question #452

A company runs a Java-based job on an Amazon EC2 instance. The job runs every hour and takes 10 seconds to run. The job runs on a scheduled interval and consumes 1 GB of memory. The CPU utilization of the instance is low except for short surges during which the job uses the maximum CPU available. The company wants to optimize the costs to run the job. Which solution will meet these requirements?

- A. Use AWS App2Container (A2C) to containerize the job. Run the job as an Amazon Elastic Container Service (Amazon ECS) task on AWS Fargate with 0.5 virtual CPU (vCPU) and 1 GB of memory.

- B. Copy the code into an AWS Lambda function that has 1 GB of memory. Create an Amazon EventBridge scheduled rule to run the code each hour.

- C. Use AWS App2Container (A2C) to containerize the job. Install the container in the existing Amazon Machine Image (AMI). Ensure that the schedule stops the container when the task nishes.

- D. Configure the existing schedule to stop the EC2 instance at the completion of the job and restart the EC2 instance when the next job starts.

**Correct:** B
**Why:** Lambda with 1 GB memory and an EventBridge schedule runs for ~10 seconds/hour at the lowest cost; no servers to manage.

**Incorrect:**
- A: Containerize/EC2 approaches incur more baseline cost and management for such a short job.
- C: Containerize/EC2 approaches incur more baseline cost and management for such a short job.
- D: Containerize/EC2 approaches incur more baseline cost and management for such a short job.


---

---

### Question #483

A company containerized a Windows job that runs on .NET 6 Framework under a Windows container. The company wants to run this job in the AWS Cloud. The job runs every 10 minutes. The job’s runtime varies between 1 minute and 3 minutes. Which solution will meet these requirements MOST cost-effectively?

- A. Create an AWS Lambda function based on the container image of the job. Configure Amazon EventBridge to invoke the function every 10 minutes.

- B. Use AWS Batch to create a job that uses AWS Fargate resources. Configure the job scheduling to run every 10 minutes.

- C. Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate to run the job. Create a scheduled task based on the container image of the job to run every 10 minutes.

- D. Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate to run the job. Create a standalone task based on the container image of the job. Use Windows task scheduler to run the job every 10 minutes.

**Correct:** C
**Why:** ECS on Fargate scheduled tasks run Windows containers cost‑effectively for short, periodic jobs; Lambda doesn’t support Windows containers.

**Incorrect:**
- A: Lambda cannot run Windows containers.
- B: Batch is heavier for a fixed, short schedule.
- D: Standalone task + Windows scheduler increases ops.


---

---

### Question #492

A company has multiple AWS accounts for development work. Some staff consistently use oversized Amazon EC2 instances, which causes the company to exceed the yearly budget for the development accounts. The company wants to centrally restrict the creation of AWS resources in these accounts. Which solution will meet these requirements with the LEAST development effort?

- A. Develop AWS Systems Manager templates that use an approved EC2 creation process. Use the approved Systems Manager templates to provision EC2 instances.

- B. Use AWS Organizations to organize the accounts into organizational units (OUs). Dene and attach a service control policy (SCP) to control the usage of EC2 instance types.

- C. Configure an Amazon EventBridge rule that invokes an AWS Lambda function when an EC2 instance is created. Stop disallowed EC2 instance types.

- D. Set up AWS Service Catalog products for the staff to create the allowed EC2 instance types. Ensure that staff can deploy EC2 instances only by using the Service Catalog products.

**Correct:** B
**Why:** Organize accounts and attach an SCP that denies disallowed EC2 instance types (least effort, central control).

**Incorrect:**
- A: Heavier build or reactive after creation.
- C: Heavier build or reactive after creation.
- D: Heavier build or reactive after creation.


---

## Amazon DynamoDB

### Question #266

A company has a popular gaming platform running on AWS. The application is sensitive to latency because latency can impact the user experience and introduce unfair advantages to some players. The application is deployed in every AWS Region. It runs on Amazon EC2 instances that are part of Auto Scaling groups congured behind Application Load Balancers (ALBs). A solutions architect needs to implement a mechanism to monitor the health of the application and redirect trac to healthy endpoints. Which solution meets these requirements?

- A. Configure an accelerator in AWS Global Accelerator. Add a listener for the port that the application listens on, and attach it to a Regional endpoint in each Region. Add the ALB as the endpoint.

- B. Create an Amazon CloudFront distribution and specify the ALB as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the trac.

- C. Create an Amazon CloudFront distribution and specify Amazon S3 as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the trac.

- D. Configure an Amazon DynamoDB database to serve as the data store for the application. Create a DynamoDB Accelerator (DAX) cluster to act as the in-memory cache for DynamoDB hosting the application data.

**Correct:** A
**Why:** Global Accelerator provides health checks and intelligent routing to the nearest healthy Regional endpoint (ALB).

**Incorrect:**
- B: CloudFront/S3 or DynamoDB/DAX don’t meet UDP/TCP app health redirect needs across Regions.
- C: CloudFront/S3 or DynamoDB/DAX don’t meet UDP/TCP app health redirect needs across Regions.
- D: CloudFront/S3 or DynamoDB/DAX don’t meet UDP/TCP app health redirect needs across Regions.


---

---

### Question #268

A gaming company has a web application that displays scores. The application runs on Amazon EC2 instances behind an Application Load Balancer. The application stores data in an Amazon RDS for MySQL database. Users are starting to experience long delays and interruptions that are caused by database read performance. The company wants to improve the user experience while minimizing changes to the application’s architecture. What should a solutions architect do to meet these requirements?

- A. Use Amazon ElastiCache in front of the database.

- B. Use RDS Proxy between the application and the database.

- C. Migrate the application from EC2 instances to AWS Lambda.

- D. Migrate the database from Amazon RDS for MySQL to Amazon DynamoDB.

**Correct:** A
**Why:** Add ElastiCache in front of RDS to reduce read pressure without major architecture change.

**Incorrect:**
- B: RDS Proxy assists connections, not read latency.
- C: Lambda/DynamoDB are re‑architectures.
- D: Lambda/DynamoDB are re‑architectures.


---

---

### Question #269

An ecommerce company has noticed performance degradation of its Amazon RDS based web application. The performance degradation is attributed to an increase in the number of read-only SQL queries triggered by business analysts. A solutions architect needs to solve the problem with minimal changes to the existing web application. What should the solutions architect recommend?

- A. Export the data to Amazon DynamoDB and have the business analysts run their queries.

- B. Load the data into Amazon ElastiCache and have the business analysts run their queries.

- C. Create a read replica of the primary database and have the business analysts run their queries.

- D. Copy the data into an Amazon Redshift cluster and have the business analysts run their queries.

**Correct:** C
**Why:** Offload analysts’ queries to a read replica to protect primary performance with minimal changes.

**Incorrect:**
- A: DynamoDB/ElastiCache/Redshift require more changes.
- B: DynamoDB/ElastiCache/Redshift require more changes.
- D: DynamoDB/ElastiCache/Redshift require more changes.


---

---

### Question #278

A company wants to create an application to store employee data in a hierarchical structured relationship. The company needs a minimum-latency response to high-trac queries for the employee data and must protect any sensitive data. The company also needs to receive monthly email messages if any nancial information is present in the employee data. Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)

- A. Use Amazon Redshift to store the employee data in hierarchies. Unload the data to Amazon S3 every month.

- B. Use Amazon DynamoDB to store the employee data in hierarchies. Export the data to Amazon S3 every month.

- C. Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly events to AWS Lambda.

- D. Use Amazon Athena to analyze the employee data in Amazon S3. Integrate Athena with Amazon QuickSight to publish analysis dashboards and share the dashboards with users.

E. Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly notications through an Amazon Simple Notification Service (Amazon SNS) subscription.

**Correct:** B, E
**Why:** DynamoDB suits hierarchical, low‑latency queries; Macie + EventBridge → SNS can notify monthly when financial data is detected.

**Incorrect:**
- A: Redshift/Athena aren’t optimized for low‑latency hierarchical queries.
- C: Lambda trigger pattern is unnecessary for monthly notifications.
- D: Redshift/Athena aren’t optimized for low‑latency hierarchical queries.


---

---

### Question #279

A company has an application that is backed by an Amazon DynamoDB table. The company’s compliance requirements specify that database backups must be taken every month, must be available for 6 months, and must be retained for 7 years. Which solution will meet these requirements?

- A. Create an AWS Backup plan to back up the DynamoDB table on the first day of each month. Specify a lifecycle policy that transitions the backup to cold storage after 6 months. Set the retention period for each backup to 7 years.

- B. Create a DynamoDB on-demand backup of the DynamoDB table on the first day of each month. Transition the backup to Amazon S3 Glacier Flexible Retrieval after 6 months. Create an S3 Lifecycle policy to delete backups that are older than 7 years.

- C. Use the AWS SDK to develop a script that creates an on-demand backup of the DynamoDB table. Set up an Amazon EventBridge rule that runs the script on the first day of each month. Create a second script that will run on the second day of each month to transition DynamoDB backups that are older than 6 months to cold storage and to delete backups that are older than 7 years.

- D. Use the AWS CLI to create an on-demand backup of the DynamoDB table. Set up an Amazon EventBridge rule that runs the command on the first day of each month with a cron expression. Specify in the command to transition the backups to cold storage after 6 months and to delete the backups after 7 years.

**Correct:** A
**Why:** AWS Backup monthly backup plan with lifecycle to cold storage after 6 months and 7‑year retention meets requirements.

**Incorrect:**
- B: Manual or partial solutions add ops and risk consistency.
- C: Manual or partial solutions add ops and risk consistency.
- D: Manual or partial solutions add ops and risk consistency.


---

---

### Question #280

A company is using Amazon CloudFront with its website. The company has enabled logging on the CloudFront distribution, and logs are saved in one of the company’s Amazon S3 buckets. The company needs to perform advanced analyses on the logs and build visualizations. What should a solutions architect do to meet these requirements?

- A. Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket. Visualize the results with AWS Glue.

- B. Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket. Visualize the results with Amazon QuickSight.

- C. Use standard SQL queries in Amazon DynamoDB to analyze the CloudFront logs in the S3 bucket. Visualize the results with AWS Glue.

- D. Use standard SQL queries in Amazon DynamoDB to analyze the CloudFront logs in the S3 bucket. Visualize the results with Amazon QuickSight.

**Correct:** B
**Why:** Use Athena to query CloudFront logs in S3 and QuickSight for visualizations.

**Incorrect:**
- A: Glue/DynamoDB are not needed for log analysis/visualization here.
- C: Glue/DynamoDB are not needed for log analysis/visualization here.
- D: Glue/DynamoDB are not needed for log analysis/visualization here.


---

---

### Question #295

An ecommerce company stores terabytes of customer data in the AWS Cloud. The data contains personally identiable information (PII). The company wants to use the data in three applications. Only one of the applications needs to process the PII. The PII must be removed before the other two applications process the data. Which solution will meet these requirements with the LEAST operational overhead?

- A. Store the data in an Amazon DynamoDB table. Create a proxy application layer to intercept and process the data that each application requests.

- B. Store the data in an Amazon S3 bucket. Process and transform the data by using S3 Object Lambda before returning the data to the requesting application.

- C. Process the data and store the transformed data in three separate Amazon S3 buckets so that each application has its own custom dataset. Point each application to its respective S3 bucket.

- D. Process the data and store the transformed data in three separate Amazon DynamoDB tables so that each application has its own custom dataset. Point each application to its respective DynamoDB table.

**Correct:** B
**Why:** S3 Object Lambda can redact or transform object content (e.g., remove PII) per requester while preserving original data.

**Incorrect:**
- A: Proxy/app‑level or separate datasets add ops and duplication.
- C: Proxy/app‑level or separate datasets add ops and duplication.
- D: Proxy/app‑level or separate datasets add ops and duplication.


---

---

### Question #317

A company uses a legacy application to produce data in CSV format. The legacy application stores the output data in Amazon S3. The company is deploying a new commercial off-the-shelf (COTS) application that can perform complex SQL queries to analyze data that is stored in Amazon Redshift and Amazon S3 only. However, the COTS application cannot process the .csv files that the legacy application produces. The company cannot update the legacy application to produce data in another format. The company needs to implement a solution so that the COTS application can use the data that the legacy application produces. Which solution will meet these requirements with the LEAST operational overhead?

- A. Create an AWS Glue extract, transform, and load (ETL) job that runs on a schedule. Configure the ETL job to process the .csv files and store the processed data in Amazon Redshift.

- B. Develop a Python script that runs on Amazon EC2 instances to convert the .csv files to .sql files. Invoke the Python script on a cron schedule to store the output files in Amazon S3.

- C. Create an AWS Lambda function and an Amazon DynamoDB table. Use an S3 event to invoke the Lambda function. Configure the Lambda function to perform an extract, transform, and load (ETL) job to process the .csv files and store the processed data in the DynamoDB table.

- D. Use Amazon EventBridge to launch an Amazon EMR cluster on a weekly schedule. Configure the EMR cluster to perform an extract, transform, and load (ETL) job to process the .csv files and store the processed data in an Amazon Redshift table.

**Correct:** A
**Why:** Glue ETL job on a schedule to process CSVs and load into Redshift with least ops.

**Incorrect:**
- B: EC2 scripts, DynamoDB, or EMR add ops for the use case.
- C: EC2 scripts, DynamoDB, or EMR add ops for the use case.
- D: EC2 scripts, DynamoDB, or EMR add ops for the use case.


---

---

### Question #323

A company’s facility has badge readers at every entrance throughout the building. When badges are scanned, the readers send a message over HTTPS to indicate who attempted to access that particular entrance. A solutions architect must design a system to process these messages from the sensors. The solution must be highly available, and the results must be made available for the company’s security team to analyze. Which system architecture should the solutions architect recommend?

- A. Launch an Amazon EC2 instance to serve as the HTTPS endpoint and to process the messages. Configure the EC2 instance to save the results to an Amazon S3 bucket.

- B. Create an HTTPS endpoint in Amazon API Gateway. Configure the API Gateway endpoint to invoke an AWS Lambda function to process the messages and save the results to an Amazon DynamoDB table.

- C. Use Amazon Route 53 to direct incoming sensor messages to an AWS Lambda function. Configure the Lambda function to process the messages and save the results to an Amazon DynamoDB table.

- D. Create a gateway VPC endpoint for Amazon S3. Configure a Site-to-Site VPN connection from the facility network to the VPC so that sensor data can be written directly to an S3 bucket by way of the VPC endpoint.

**Correct:** B
**Why:** API Gateway HTTPS endpoint invoking Lambda, storing to DynamoDB, is highly available and serverless.

**Incorrect:**
- A: EC2 endpoint or direct S3 via VPN increases ops/complexity.
- C: Route 53 cannot directly invoke code.
- D: EC2 endpoint or direct S3 via VPN increases ops/complexity.


---

---

### Question #337

A company has deployed a web application on AWS. The company hosts the backend database on Amazon RDS for MySQL with a primary DB instance and ve read replicas to support scaling needs. The read replicas must lag no more than 1 second behind the primary DB instance. The database routinely runs scheduled stored procedures. As trac on the website increases, the replicas experience additional lag during periods of peak load. A solutions architect must reduce the replication lag as much as possible. The solutions architect must minimize changes to the application code and must minimize ongoing operational overhead. Which solution will meet these requirements?

- A. Migrate the database to Amazon Aurora MySQL. Replace the read replicas with Aurora Replicas, and configure Aurora Auto Scaling. Replace the stored procedures with Aurora MySQL native functions.

- B. Deploy an Amazon ElastiCache for Redis cluster in front of the database. Modify the application to check the cache before the application queries the database. Replace the stored procedures with AWS Lambda functions.

- C. Migrate the database to a MySQL database that runs on Amazon EC2 instances. Choose large, compute optimized EC2 instances for all replica nodes. Maintain the stored procedures on the EC2 instances.

- D. Migrate the database to Amazon DynamoDB. Provision a large number of read capacity units (RCUs) to support the required throughput, and configure on-demand capacity scaling. Replace the stored procedures with DynamoDB streams.

**Correct:** A
**Why:** Aurora MySQL with Aurora Replicas and Auto Scaling reduces replica lag and operational overhead.

**Incorrect:**
- B: Cache/EC2 self‑managed or DynamoDB require more changes.
- C: Cache/EC2 self‑managed or DynamoDB require more changes.
- D: Cache/EC2 self‑managed or DynamoDB require more changes.


---

---

### Question #348

A company collects data from a large number of participants who use wearable devices. The company stores the data in an Amazon DynamoDB table and uses applications to analyze the data. The data workload is constant and predictable. The company wants to stay at or below its forecasted budget for DynamoDB. Which solution will meet these requirements MOST cost-effectively?

- A. Use provisioned mode and DynamoDB Standard-Infrequent Access (DynamoDB Standard-IA). Reserve capacity for the forecasted workload.

- B. Use provisioned mode. Specify the read capacity units (RCUs) and write capacity units (WCUs).

- C. Use on-demand mode. Set the read capacity units (RCUs) and write capacity units (WCUs) high enough to accommodate changes in the workload.

- D. Use on-demand mode. Specify the read capacity units (RCUs) and write capacity units (WCUs) with reserved capacity.

**Correct:** A
**Why:** Provisioned mode with reserved capacity for predictable workload and Standard‑IA for rarely accessed items reduces cost.

**Incorrect:**
- B: Plain provisioned lacks reservations; on‑demand can exceed budget and doesn’t support reserved capacity.
- C: Plain provisioned lacks reservations; on‑demand can exceed budget and doesn’t support reserved capacity.
- D: Plain provisioned lacks reservations; on‑demand can exceed budget and doesn’t support reserved capacity.


---

---

### Question #354

A company hosts a serverless application on AWS. The application uses Amazon API Gateway, AWS Lambda, and an Amazon RDS for PostgreSQL database. The company notices an increase in application errors that result from database connection timeouts during times of peak trac or unpredictable trac. The company needs a solution that reduces the application failures with the least amount of change to the code. What should a solutions architect do to meet these requirements?

- A. Reduce the Lambda concurrency rate.

- B. Enable RDS Proxy on the RDS DB instance.

- C. Resize the RDS DB instance class to accept more connections.

- D. Migrate the database to Amazon DynamoDB with on-demand scaling.

**Correct:** B
**Why:** RDS Proxy pools and reuses connections for Lambda, preventing connection storms/timeouts during bursts with minimal code change.

**Incorrect:**
- A: Reducing Lambda concurrency throttles the app and does not fix connection pooling.
- C: Upsizing may delay but not solve connection exhaustion.
- D: Migrating to DynamoDB is a large rewrite.


---

---

### Question #361

A company hosts a multiplayer gaming application on AWS. The company wants the application to read data with sub-millisecond latency and run one-time queries on historical data. Which solution will meet these requirements with the LEAST operational overhead?

- A. Use Amazon RDS for data that is frequently accessed. Run a periodic custom script to export the data to an Amazon S3 bucket.

- B. Store the data directly in an Amazon S3 bucket. Implement an S3 Lifecycle policy to move older data to S3 Glacier Deep Archive for long- term storage. Run one-time queries on the data in Amazon S3 by using Amazon Athena.

- C. Use Amazon DynamoDB with DynamoDB Accelerator (DAX) for data that is frequently accessed. Export the data to an Amazon S3 bucket by using DynamoDB table export. Run one-time queries on the data in Amazon S3 by using Amazon Athena.

- D. Use Amazon DynamoDB for data that is frequently accessed. Turn on streaming to Amazon Kinesis Data Streams. Use Amazon Kinesis Data Firehose to read the data from Kinesis Data Streams. Store the records in an Amazon S3 bucket.

**Correct:** C
**Why:** DynamoDB + DAX provides sub‑millisecond reads; export to S3 and query ad hoc with Athena for historical analysis with low ops.

**Incorrect:**
- A: Adds more moving parts and does not give sub‑ms reads as simply as DAX.
- B: S3 alone lacks sub‑ms latency for frequently accessed data.
- D: Adds more moving parts and does not give sub‑ms reads as simply as DAX.


---

---

### Question #362

A company uses a payment processing system that requires messages for a particular payment ID to be received in the same order that they were sent. Otherwise, the payments might be processed incorrectly. Which actions should a solutions architect take to meet this requirement? (Choose two.)

- A. Write the messages to an Amazon DynamoDB table with the payment ID as the partition key.

- B. Write the messages to an Amazon Kinesis data stream with the payment ID as the partition key.

- C. Write the messages to an Amazon ElastiCache for Memcached cluster with the payment ID as the key.

- D. Write the messages to an Amazon Simple Queue Service (Amazon SQS) queue. Set the message attribute to use the payment ID.

E. Write the messages to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the message group to use the payment ID.

**Correct:** B, E
**Why:** Kinesis shards preserve ordering by partition key; SQS FIFO preserves message order per message group.

**Incorrect:**
- A: DynamoDB does not enforce arrival/processing order.
- C: ElastiCache is not a message bus with ordering guarantees.
- D: Standard SQS queues do not guarantee ordering.


---

---

### Question #366

A company’s web application consists of an Amazon API Gateway API in front of an AWS Lambda function and an Amazon DynamoDB database. The Lambda function handles the business logic, and the DynamoDB table hosts the data. The application uses Amazon Cognito user pools to identify the individual users of the application. A solutions architect needs to update the application so that only users who have a subscription can access premium content. Which solution will meet this requirement with the LEAST operational overhead?

- A. Enable API caching and throttling on the API Gateway API.

- B. Set up AWS WAF on the API Gateway API. Create a rule to filter users who have a subscription.

- C. Apply ne-grained IAM permissions to the premium content in the DynamoDB table.

- D. Implement API usage plans and API keys to limit the access of users who do not have a subscription.

**Correct:** D
**Why:** API Gateway usage plans and API keys provide an easy way to gate/limit access for non‑subscribers with low operational overhead.

**Incorrect:**
- A: Caching/throttling don’t enforce subscription access.
- B: WAF filters traffic patterns, not subscription entitlements.
- C: Fine‑grained IAM on DynamoDB is complex and not aligned with Cognito user pool tokens alone.


---

---

### Question #372

A company wants to migrate an Oracle database to AWS. The database consists of a single table that contains millions of geographic information systems (GIS) images that are high resolution and are identied by a geographic code. When a natural disaster occurs, tens of thousands of images get updated every few minutes. Each geographic code has a single image or row that is associated with it. The company wants a solution that is highly available and scalable during such events. Which solution meets these requirements MOST cost-effectively?

- A. Store the images and geographic codes in a database table. Use Oracle running on an Amazon RDS Multi-AZ DB instance.

- B. Store the images in Amazon S3 buckets. Use Amazon DynamoDB with the geographic code as the key and the image S3 URL as the value.

- C. Store the images and geographic codes in an Amazon DynamoDB table. Configure DynamoDB Accelerator (DAX) during times of high load.

- D. Store the images in Amazon S3 buckets. Store geographic codes and image S3 URLs in a database table. Use Oracle running on an Amazon RDS Multi-AZ DB instance.

**Correct:** B
**Why:** Store images in S3 and use DynamoDB keyed by geographic code with S3 URLs; scales and is highly available during spikes.

**Incorrect:**
- A: RDS Oracle will struggle to scale writes/size cost‑effectively.
- C: Storing large images directly in DynamoDB is not appropriate.
- D: RDS Oracle will struggle to scale writes/size cost‑effectively.


---

---

### Question #378

A company is developing a real-time multiplayer game that uses UDP for communications between the client and servers in an Auto Scaling group. Spikes in demand are anticipated during the day, so the game server platform must adapt accordingly. Developers want to store gamer scores and other non-relational data in a database solution that will scale without intervention. Which solution should a solutions architect recommend?

- A. Use Amazon Route 53 for trac distribution and Amazon Aurora Serverless for data storage.

- B. Use a Network Load Balancer for trac distribution and Amazon DynamoDB on-demand for data storage.

- C. Use a Network Load Balancer for trac distribution and Amazon Aurora Global Database for data storage.

- D. Use an Application Load Balancer for trac distribution and Amazon DynamoDB global tables for data storage.

**Correct:** B
**Why:** NLB supports UDP for game traffic; DynamoDB on‑demand scales automatically for storing scores/non‑relational data.

**Incorrect:**
- A: Aurora is relational and requires capacity planning.
- C: Aurora is relational and requires capacity planning.
- D: ALB does not support UDP.


---

---

### Question #381

A company hosts a three-tier web application that includes a PostgreSQL database. The database stores the metadata from documents. The company searches the metadata for key terms to retrieve documents that the company reviews in a report each month. The documents are stored in Amazon S3. The documents are usually written only once, but they are updated frequently. The reporting process takes a few hours with the use of relational queries. The reporting process must not prevent any document modications or the addition of new documents. A solutions architect needs to implement a solution to speed up the reporting process. Which solution will meet these requirements with the LEAST amount of change to the application code?

- A. Set up a new Amazon DocumentDB (with MongoDB compatibility) cluster that includes a read replica. Scale the read replica to generate the reports.

- B. Set up a new Amazon Aurora PostgreSQL DB cluster that includes an Aurora Replica. Issue queries to the Aurora Replica to generate the reports.

- C. Set up a new Amazon RDS for PostgreSQL Multi-AZ DB instance. Configure the reporting module to query the secondary RDS node so that the reporting module does not affect the primary node.

- D. Set up a new Amazon DynamoDB table to store the documents. Use a xed write capacity to support new document entries. Automatically scale the read capacity to support the reports.

**Correct:** B
**Why:** Aurora PostgreSQL with an Aurora Replica lets reports run against the replica without blocking writes; minimal code change (same engine).

**Incorrect:**
- A: DocumentDB or RDS Multi‑AZ do not solve read scaling as effectively.
- C: DocumentDB or RDS Multi‑AZ do not solve read scaling as effectively.
- D: DynamoDB would require a redesign.


---

---

### Question #390

A company hosts a three-tier ecommerce application on a eet of Amazon EC2 instances. The instances run in an Auto Scaling group behind an Application Load Balancer (ALB). All ecommerce data is stored in an Amazon RDS for MariaDB Multi-AZ DB instance. The company wants to optimize customer session management during transactions. The application must store session data durably. Which solutions will meet these requirements? (Choose two.)

- A. Turn on the sticky sessions feature (session anity) on the ALB.

- B. Use an Amazon DynamoDB table to store customer session information.

- C. Deploy an Amazon Cognito user pool to manage user session information.

- D. Deploy an Amazon ElastiCache for Redis cluster to store customer session information.

E. Use AWS Systems Manager Application Manager in the application to manage user session information.

**Correct:** A, B
**Why:** Sticky sessions reduce cross‑instance churn during transactions; store session data durably in DynamoDB to survive instance failure.

**Incorrect:**
- C: Cognito manages auth, not app transaction sessions.
- D: ElastiCache Redis alone is not durable.
- E: Systems Manager Application Manager is not for session storage.


---

---

### Question #400

A meteorological startup company has a custom web application to sell weather data to its users online. The company uses Amazon DynamoDB to store its data and wants to build a new service that sends an alert to the managers of four internal teams every time a new weather event is recorded. The company does not want this new service to affect the performance of the current application. What should a solutions architect do to meet these requirements with the LEAST amount of operational overhead?

- A. Use DynamoDB transactions to write new event data to the table. Configure the transactions to notify internal teams.

- B. Have the current application publish a message to four Amazon Simple Notification Service (Amazon SNS) topics. Have each team subscribe to one topic.

- C. Enable Amazon DynamoDB Streams on the table. Use triggers to write to a single Amazon Simple Notification Service (Amazon SNS) topic to which the teams can subscribe.

- D. Add a custom attribute to each record to ag new items. Write a cron job that scans the table every minute for items that are new and noties an Amazon Simple Queue Service (Amazon SQS) queue to which the teams can subscribe.

**Correct:** C
**Why:** DynamoDB Streams with a trigger can publish to a single SNS topic for fanout to the four teams without impacting the main app.

**Incorrect:**
- A: Transactions don’t notify; they ensure atomic writes.
- B: Having the app publish to four topics adds coupling.
- D: Cron scans are inefficient and add lag.


---

---

### Question #411

A company has a web application with sporadic usage patterns. There is heavy usage at the beginning of each month, moderate usage at the start of each week, and unpredictable usage during the week. The application consists of a web server and a MySQL database server running inside the data center. The company would like to move the application to the AWS Cloud, and needs to select a cost-effective database platform that will not require database modications. Which solution will meet these requirements?

- A. Amazon DynamoDB

- B. Amazon RDS for MySQL

- C. MySQL-compatible Amazon Aurora Serverless

- D. MySQL deployed on Amazon EC2 in an Auto Scaling group

**Correct:** B
**Why:** Amazon RDS for MySQL is a managed, cost‑effective, compatible drop‑in with no application or schema changes.

**Incorrect:**
- A: DynamoDB would require a rewrite.
- C: Aurora Serverless can be cost‑effective but may require migration/compatibility testing; RDS MySQL is simpler.
- D: Self-managed MySQL on EC2 increases ops.


---

---

### Question #428

A serverless application uses Amazon API Gateway, AWS Lambda, and Amazon DynamoDB. The Lambda function needs permissions to read and write to the DynamoDB table. Which solution will give the Lambda function access to the DynamoDB table MOST securely?

- A. Create an IAM user with programmatic access to the Lambda function. Attach a policy to the user that allows read and write access to the DynamoDB table. Store the access_key_id and secret_access_key parameters as part of the Lambda environment variables. Ensure that other AWS users do not have read and write access to the Lambda function configuration.

- B. Create an IAM role that includes Lambda as a trusted service. Attach a policy to the role that allows read and write access to the DynamoDB table. Update the configuration of the Lambda function to use the new role as the execution role.

- C. Create an IAM user with programmatic access to the Lambda function. Attach a policy to the user that allows read and write access to the DynamoDB table. Store the access_key_id and secret_access_key parameters in AWS Systems Manager Parameter Store as secure string parameters. Update the Lambda function code to retrieve the secure string parameters before connecting to the DynamoDB table.

- D. Create an IAM role that includes DynamoDB as a trusted service. Attach a policy to the role that allows read and write access from the Lambda function. Update the code of the Lambda function to attach to the new role as an execution role.

**Correct:** B
**Why:** Use an IAM role trusted by Lambda with a policy granting DynamoDB access; attach as the function’s execution role.

**Incorrect:**
- A: Do not use static IAM user creds in Lambda.
- C: Do not use static IAM user creds in Lambda.
- D: DynamoDB is not a trusted principal for the role.


---

---

### Question #434

A company hosts its application in the AWS Cloud. The application runs on Amazon EC2 instances behind an Elastic Load Balancer in an Auto Scaling group and with an Amazon DynamoDB table. The company wants to ensure the application can be made available in anotherAWS Region with minimal downtime. What should a solutions architect do to meet these requirements with the LEAST amount of downtime?

- A. Create an Auto Scaling group and a load balancer in the disaster recovery Region. Configure the DynamoDB table as a global table. Configure DNS failover to point to the new disaster recovery Region's load balancer.

- B. Create an AWS CloudFormation template to create EC2 instances, load balancers, and DynamoDB tables to be launched when needed Configure DNS failover to point to the new disaster recovery Region's load balancer.

- C. Create an AWS CloudFormation template to create EC2 instances and a load balancer to be launched when needed. Configure the DynamoDB table as a global table. Configure DNS failover to point to the new disaster recovery Region's load balancer.

- D. Create an Auto Scaling group and load balancer in the disaster recovery Region. Configure the DynamoDB table as a global table. Create an Amazon CloudWatch alarm to trigger an AWS Lambda function that updates Amazon Route 53 pointing to the disaster recovery load balancer.

**Correct:** A
**Why:** Pre‑provision ALB/ASG in DR Region and use DynamoDB global tables; DNS failover minimizes downtime.

**Incorrect:**
- B: Delayed provisioning increases downtime.
- C: Delayed provisioning increases downtime.
- D: Extra Lambda automation is unnecessary.


---

---

### Question #458

A solutions architect is designing a RESTAPI in Amazon API Gateway for a cash payback service. The application requires 1 GB of memory and 2 GB of storage for its computation resources. The application will require that the data is in a relational format. Which additional combination ofAWS services will meet these requirements with the LEAST administrative effort? (Choose two.)

- A. Amazon EC2

- B. AWS Lambda

- C. Amazon RDS

- D. Amazon DynamoDB

E. Amazon Elastic Kubernetes Services (Amazon EKS)

**Correct:** B, C
**Why:** Lambda can be sized to 1 GB memory and sufficient ephemeral storage; use Amazon RDS for relational storage—minimal admin.

**Incorrect:**
- A: EC2/EKS add operational overhead.
- D: DynamoDB is non‑relational.
- E: EC2/EKS add operational overhead.


---

---

### Question #461

A company is developing a mobile gaming app in a single AWS Region. The app runs on multiple Amazon EC2 instances in an Auto Scaling group. The company stores the app data in Amazon DynamoDB. The app communicates by using TCP trac and UDP trac between the users and the servers. The application will be used globally. The company wants to ensure the lowest possible latency for all users. Which solution will meet these requirements?

- A. Use AWS Global Accelerator to create an accelerator. Create an Application Load Balancer (ALB) behind an accelerator endpoint that uses Global Accelerator integration and listening on the TCP and UDP ports. Update the Auto Scaling group to register instances on the ALB.

- B. Use AWS Global Accelerator to create an accelerator. Create a Network Load Balancer (NLB) behind an accelerator endpoint that uses Global Accelerator integration and listening on the TCP and UDP ports. Update the Auto Scaling group to register instances on the NLB.

- C. Create an Amazon CloudFront content delivery network (CDN) endpoint. Create a Network Load Balancer (NLB) behind the endpoint and listening on the TCP and UDP ports. Update the Auto Scaling group to register instances on the NLB. Update CloudFront to use the NLB as the origin.

- D. Create an Amazon CloudFront content delivery network (CDN) endpoint. Create an Application Load Balancer (ALB) behind the endpoint and listening on the TCP and UDP ports. Update the Auto Scaling group to register instances on the ALB. Update CloudFront to use the ALB as the origin.

**Correct:** B
**Why:** Global Accelerator with an NLB supports both TCP and UDP and provides the lowest latency globally via the AWS edge.

**Incorrect:**
- A: ALB doesn’t support UDP; CloudFront is not for arbitrary TCP/UDP.
- C: ALB doesn’t support UDP; CloudFront is not for arbitrary TCP/UDP.
- D: ALB doesn’t support UDP; CloudFront is not for arbitrary TCP/UDP.


---

---

### Question #472

A company has a mobile chat application with a data store based in Amazon DynamoDB. Users would like new messages to be read with as little latency as possible. A solutions architect needs to design an optimal solution that requires minimal application changes. Which method should the solutions architect select?

- A. Configure Amazon DynamoDB Accelerator (DAX) for the new messages table. Update the code to use the DAX endpoint.

- B. Add DynamoDB read replicas to handle the increased read load. Update the application to point to the read endpoint for the read replicas.

- C. Double the number of read capacity units for the new messages table in DynamoDB. Continue to use the existing DynamoDB endpoint.

- D. Add an Amazon ElastiCache for Redis cache to the application stack. Update the application to point to the Redis cache endpoint instead of DynamoDB.

**Correct:** A
**Why:** DAX provides microsecond latency reads for DynamoDB with minimal app changes (use DAX endpoint).

**Incorrect:**
- B: DynamoDB “replicas” are global tables; not for latency of new messages.
- C: Simply doubling RCUs doesn’t address latency.
- D: Adding Redis increases ops and adds cache coherence issues.


---

---

### Question #489

An ecommerce company runs an application in the AWS Cloud that is integrated with an on-premises warehouse solution. The company uses Amazon Simple Notification Service (Amazon SNS) to send order messages to an on-premises HTTPS endpoint so the warehouse application can process the orders. The local data center team has detected that some of the order messages were not received. A solutions architect needs to retain messages that are not delivered and analyze the messages for up to 14 days. Which solution will meet these requirements with the LEAST development effort?

- A. Configure an Amazon SNS dead letter queue that has an Amazon Kinesis Data Stream target with a retention period of 14 days.

- B. Add an Amazon Simple Queue Service (Amazon SQS) queue with a retention period of 14 days between the application and Amazon SNS.

- C. Configure an Amazon SNS dead letter queue that has an Amazon Simple Queue Service (Amazon SQS) target with a retention period of 14 days.

- D. Configure an Amazon SNS dead letter queue that has an Amazon DynamoDB target with a TTL attribute set for a retention period of 14 days.

**Correct:** C
**Why:** Configure an SNS dead‑letter queue with SQS target and 14‑day retention to retain and analyze undelivered messages with minimal dev.

**Incorrect:**
- A: Kinesis/DynamoDB targets add complexity.
- B: Adding SQS in front of SNS changes the design.
- D: Kinesis/DynamoDB targets add complexity.


---

---

### Question #490

A gaming company uses Amazon DynamoDB to store user information such as geographic location, player data, and leaderboards. The company needs to configure continuous backups to an Amazon S3 bucket with a minimal amount of coding. The backups must not affect availability of the application and must not affect the read capacity units (RCUs) that are dened for the table. Which solution meets these requirements?

- A. Use an Amazon EMR cluster. Create an Apache Hive job to back up the data to Amazon S3.

- B. Export the data directly from DynamoDB to Amazon S3 with continuous backups. Turn on point-in-time recovery for the table.

- C. Configure Amazon DynamoDB Streams. Create an AWS Lambda function to consume the stream and export the data to an Amazon S3 bucket.

- D. Create an AWS Lambda function to export the data from the database tables to Amazon S3 on a regular basis. Turn on point-in-time recovery for the table.

**Correct:** B
**Why:** DynamoDB table export to S3 provides continuous, server‑side exports without consuming RCUs and with no downtime; enable PITR.

**Incorrect:**
- A: EMR/Lambda/Streams add code/ops and may affect capacity.
- C: EMR/Lambda/Streams add code/ops and may affect capacity.
- D: EMR/Lambda/Streams add code/ops and may affect capacity.


---

## Amazon EBS

### Question #252

A solutions architect needs to design a system to store client case files. The files are core company assets and are important. The number of files will grow over time. The files must be simultaneously accessible from multiple application servers that run on Amazon EC2 instances. The solution must have built-in redundancy. Which solution meets these requirements?

- A. Amazon Elastic File System (Amazon EFS)

- B. Amazon Elastic Block Store (Amazon EBS)

- C. Amazon S3 Glacier Deep Archive

- D. AWS Backup

**Correct:** A
**Why:** Amazon EFS provides shared, redundant NFS storage accessible concurrently by multiple EC2 instances.

**Incorrect:**
- B: EBS is block storage attached to a single AZ/instance at a time.
- C: Glacier Deep Archive is archival, not shared storage.
- D: AWS Backup is for backups, not primary shared storage.


---

---

### Question #277

A company provides an online service for posting video content and transcoding it for use by any mobile platform. The application architecture uses Amazon Elastic File System (Amazon EFS) Standard to collect and store the videos so that multiple Amazon EC2 Linux instances can access the video content for processing. As the popularity of the service has grown over time, the storage costs have become too expensive. Which storage solution is MOST cost-effective?

- A. Use AWS Storage Gateway for files to store and process the video content.

- B. Use AWS Storage Gateway for volumes to store and process the video content.

- C. Use Amazon EFS for storing the video content. Once processing is complete, transfer the files to Amazon Elastic Block Store (Amazon EBS).

- D. Use Amazon S3 for storing the video content. Move the files temporarily over to an Amazon Elastic Block Store (Amazon EBS) volume attached to the server for processing.

**Correct:** D
**Why:** Store content durably/cost‑effectively in S3 and move to EBS temporarily for processing on EC2.

**Incorrect:**
- A: Storage Gateway is not optimal for large‑scale processing cost.
- B: Storage Gateway is not optimal for large‑scale processing cost.
- C: Keeping in EFS Standard is costly for infrequently accessed media.


---

---

### Question #287

A company wants to migrate a Windows-based application from on premises to the AWS Cloud. The application has three tiers: an application tier, a business tier, and a database tier with Microsoft SQL Server. The company wants to use specic features of SQL Server such as native backups and Data Quality Services. The company also needs to share files for processing between the tiers. How should a solutions architect design the architecture to meet these requirements?

- A. Host all three tiers on Amazon EC2 instances. Use Amazon FSx File Gateway for file sharing between the tiers.

- B. Host all three tiers on Amazon EC2 instances. Use Amazon FSx for Windows File Server for file sharing between the tiers.

- C. Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon RDS. Use Amazon Elastic File System (Amazon EFS) for file sharing between the tiers.

- D. Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon RDS. Use a Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) volume for file sharing between the tiers.

**Correct:** B
**Why:** Keep SQL Server features by hosting DB on EC2; use FSx for Windows File Server for shared files; EC2 for app tiers.

**Incorrect:**
- C: RDS may not support required SQL Server features; EBS for file sharing is inappropriate.
- D: RDS may not support required SQL Server features; EBS for file sharing is inappropriate.


---

---

### Question #288

A company is migrating a Linux-based web server group to AWS. The web servers must access files in a shared file store for some content. The company must not make any changes to the application. What should a solutions architect do to meet these requirements?

- A. Create an Amazon S3 Standard bucket with access to the web servers.

- B. Configure an Amazon CloudFront distribution with an Amazon S3 bucket as the origin.

- C. Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on all web servers.

- D. Configure a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume to all web servers.

**Correct:** C
**Why:** EFS provides a POSIX shared file system for Linux without app changes.

**Incorrect:**
- A: S3/CloudFront are object/CDN; EBS is single‑attach.
- B: S3/CloudFront are object/CDN; EBS is single‑attach.
- D: S3/CloudFront are object/CDN; EBS is single‑attach.


---

---

### Question #312

A company has an application that runs on several Amazon EC2 instances. Each EC2 instance has multiple Amazon Elastic Block Store (Amazon EBS) data volumes attached to it. The application’s EC2 instance configuration and data need to be backed up nightly. The application also needs to be recoverable in a different AWS Region. Which solution will meet these requirements in the MOST operationally ecient way?

- A. Write an AWS Lambda function that schedules nightly snapshots of the application’s EBS volumes and copies the snapshots to a different Region.

- B. Create a backup plan by using AWS Backup to perform nightly backups. Copy the backups to another Region. Add the application’s EC2 instances as resources.

- C. Create a backup plan by using AWS Backup to perform nightly backups. Copy the backups to another Region. Add the application’s EBS volumes as resources.

- D. Write an AWS Lambda function that schedules nightly snapshots of the application's EBS volumes and copies the snapshots to a different Availability Zone.

**Correct:** B
**Why:** AWS Backup can back up EC2 instances (including EBS) nightly and copy to another Region with minimal ops.

**Incorrect:**
- A: DIY Lambdas or backing up only volumes miss instance config or add ops.
- C: DIY Lambdas or backing up only volumes miss instance config or add ops.
- D: DIY Lambdas or backing up only volumes miss instance config or add ops.


---

---

### Question #320

A company is using a eet of Amazon EC2 instances to ingest data from on-premises data sources. The data is in JSON format and ingestion rates can be as high as 1 MB/s. When an EC2 instance is rebooted, the data in-ight is lost. The company’s data science team wants to query ingested data in near-real time. Which solution provides near-real-time data querying that is scalable with minimal data loss?

- A. Publish data to Amazon Kinesis Data Streams, Use Kinesis Data Analytics to query the data.

- B. Publish data to Amazon Kinesis Data Firehose with Amazon Redshift as the destination. Use Amazon Redshift to query the data.

- C. Store ingested data in an EC2 instance store. Publish data to Amazon Kinesis Data Firehose with Amazon S3 as the destination. Use Amazon Athena to query the data.

- D. Store ingested data in an Amazon Elastic Block Store (Amazon EBS) volume. Publish data to Amazon ElastiCache for Redis. Subscribe to the Redis channel to query the data.

**Correct:** A
**Why:** Kinesis Data Streams with Kinesis Data Analytics provides near‑real‑time querying with minimal loss and scalability.

**Incorrect:**
- B: Firehose→Redshift has more latency; instance store/EBS risk data loss; Redis pub/sub isn’t durable.
- C: Firehose→Redshift has more latency; instance store/EBS risk data loss; Redis pub/sub isn’t durable.
- D: Firehose→Redshift has more latency; instance store/EBS risk data loss; Redis pub/sub isn’t durable.


---

---

### Question #324

A company wants to implement a disaster recovery plan for its primary on-premises file storage volume. The file storage volume is mounted from an Internet Small Computer Systems Interface (iSCSI) device on a local storage server. The file storage volume holds hundreds of terabytes (TB) of data. The company wants to ensure that end users retain immediate access to all file types from the on-premises systems without experiencing latency. Which solution will meet these requirements with the LEAST amount of change to the company's existing infrastructure?

- A. Provision an Amazon S3 File Gateway as a virtual machine (VM) that is hosted on premises. Set the local cache to 10 TB. Modify existing applications to access the files through the NFS protocol. To recover from a disaster, provision an Amazon EC2 instance and mount the S3 bucket that contains the files.

- B. Provision an AWS Storage Gateway tape gateway. Use a data backup solution to back up all existing data to a virtual tape library. Configure the data backup solution to run nightly after the initial backup is complete. To recover from a disaster, provision an Amazon EC2 instance and restore the data to an Amazon Elastic Block Store (Amazon EBS) volume from the volumes in the virtual tape library.

- C. Provision an AWS Storage Gateway Volume Gateway cached volume. Set the local cache to 10 TB. Mount the Volume Gateway cached volume to the existing file server by using iSCSI, and copy all files to the storage volume. Configure scheduled snapshots of the storage volume. To recover from a disaster, restore a snapshot to an Amazon Elastic Block Store (Amazon EBS) volume and attach the EBS volume to an Amazon EC2 instance.

- D. Provision an AWS Storage Gateway Volume Gateway stored volume with the same amount of disk space as the existing file storage volume. Mount the Volume Gateway stored volume to the existing file server by using iSCSI, and copy all files to the storage volume. Configure scheduled snapshots of the storage volume. To recover from a disaster, restore a snapshot to an Amazon Elastic Block Store (Amazon EBS) volume and attach the EBS volume to an Amazon EC2 instance.

**Correct:** D
**Why:** Stored volumes keep the full dataset on‑prem for immediate access while asynchronously replicating to AWS for DR.

**Incorrect:**
- A: S3 File Gateway changes protocols and keeps only cached data.
- B: Tape Gateway is for backups only.
- C: Cached volumes keep only recent data local, not the full dataset.


---

---

### Question #335

A company is experiencing sudden increases in demand. The company needs to provision large Amazon EC2 instances from an Amazon Machine Image (AMI). The instances will run in an Auto Scaling group. The company needs a solution that provides minimum initialization latency to meet the demand. Which solution meets these requirements?

- A. Use the aws ec2 register-image command to create an AMI from a snapshot. Use AWS Step Functions to replace the AMI in the Auto Scaling group.

- B. Enable Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot. Provision an AMI by using the snapshot. Replace the AMI in the Auto Scaling group with the new AMI.

- C. Enable AMI creation and dene lifecycle rules in Amazon Data Lifecycle Manager (Amazon DLM). Create an AWS Lambda function that modies the AMI in the Auto Scaling group.

- D. Use Amazon EventBridge to invoke AWS Backup lifecycle policies that provision AMIs. Configure Auto Scaling group capacity limits as an event source in EventBridge.

**Correct:** B
**Why:** Enable EBS Fast Snapshot Restore on the AMI’s snapshot to minimize initialization latency during scale‑out.

**Incorrect:**
- A: Register‑image/DLM/Backup events don’t directly reduce restore latency.
- C: Register‑image/DLM/Backup events don’t directly reduce restore latency.
- D: Register‑image/DLM/Backup events don’t directly reduce restore latency.


---

---

### Question #353

A company hosts a three-tier web application on Amazon EC2 instances in a single Availability Zone. The web application uses a self-managed MySQL database that is hosted on an EC2 instance to store data in an Amazon Elastic Block Store (Amazon EBS) volume. The MySQL database currently uses a 1 TB Provisioned IOPS SSD (io2) EBS volume. The company expects trac of 1,000 IOPS for both reads and writes at peak trac. The company wants to minimize any disruptions, stabilize performance, and reduce costs while retaining the capacity for double the IOPS. The company wants to move the database tier to a fully managed solution that is highly available and fault tolerant. Which solution will meet these requirements MOST cost-effectively?

- A. Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with an io2 Block Express EBS volume.

- B. Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with a General Purpose SSD (gp2) EBS volume.

- C. Use Amazon S3 Intelligent-Tiering access tiers.

- D. Use two large EC2 instances to host the database in active-passive mode.

**Correct:** B
**Why:** Multi-AZ Amazon RDS for MySQL with General Purpose SSD (gp2) provides stable, managed, highly available performance and is cost‑effective for ~1–2K IOPS needs.

**Incorrect:**
- A: io2 Block Express is not a typical/necessary choice for RDS here and is more expensive.
- C: S3 Intelligent‑Tiering is unrelated to database storage.
- D: Self-managed EC2 databases increase ops and reduce availability compared to RDS.


---

---

### Question #357

A gaming company is moving its public scoreboard from a data center to the AWS Cloud. The company uses Amazon EC2 Windows Server instances behind an Application Load Balancer to host its dynamic application. The company needs a highly available storage solution for the application. The application consists of static files and dynamic server-side code. Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)

- A. Store the static files on Amazon S3. Use Amazon CloudFront to cache objects at the edge.

- B. Store the static files on Amazon S3. Use Amazon ElastiCache to cache objects at the edge.

- C. Store the server-side code on Amazon Elastic File System (Amazon EFS). Mount the EFS volume on each EC2 instance to share the files.

- D. Store the server-side code on Amazon FSx for Windows File Server. Mount the FSx for Windows File Server volume on each EC2 instance to share the files.

E. Store the server-side code on a General Purpose SSD (gp2) Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume on each EC2 instance to share the files.

**Correct:** A, D
**Why:** Store static files in S3 and cache with CloudFront; use FSx for Windows File Server to share server‑side code across Windows EC2 instances.

**Incorrect:**
- B: ElastiCache is not an edge CDN for static files.
- C: EFS is POSIX/NFS (Linux), not ideal for Windows.
- E: EBS volumes are single‑instance; not shareable across instances.


---

---

### Question #371

A company needs to create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster to host a digital media streaming application. The EKS cluster will use a managed node group that is backed by Amazon Elastic Block Store (Amazon EBS) volumes for storage. The company must encrypt all data at rest by using a customer managed key that is stored in AWS Key Management Service (AWS KMS). Which combination of actions will meet this requirement with the LEAST operational overhead? (Choose two.)

- A. Use a Kubernetes plugin that uses the customer managed key to perform data encryption.

- B. After creation of the EKS cluster, locate the EBS volumes. Enable encryption by using the customer managed key.

- C. Enable EBS encryption by default in the AWS Region where the EKS cluster will be created. Select the customer managed key as the default key.

- D. Create the EKS cluster. Create an IAM role that has a policy that grants permission to the customer managed key. Associate the role with the EKS cluster.

E. Store the customer managed key as a Kubernetes secret in the EKS cluster. Use the customer managed key to encrypt the EBS volumes.

**Correct:** C, D
**Why:** Turn on EBS encryption by default in the Region and select the customer‑managed KMS key; ensure the cluster has permissions to use the CMK.

**Incorrect:**
- A: A plugin is unnecessary for EBS encryption at rest.
- B: You cannot enable encryption on existing EBS volumes post‑creation.
- E: Do not store CMKs as Kubernetes secrets; use KMS.


---

---

### Question #382

A company has a three-tier application on AWS that ingests sensor data from its users’ devices. The trac ows through a Network Load Balancer (NLB), then to Amazon EC2 instances for the web tier, and nally to EC2 instances for the application tier. The application tier makes calls to a database. What should a solutions architect do to improve the security of the data in transit?

- A. Configure a TLS listener. Deploy the server certicate on the NLB.

- B. Configure AWS Shield Advanced. Enable AWS WAF on the NLB.

- C. Change the load balancer to an Application Load Balancer (ALB). Enable AWS WAF on the ALB.

- D. Encrypt the Amazon Elastic Block Store (Amazon EBS) volume on the EC2 instances by using AWS Key Management Service (AWS KMS).

**Correct:** A
**Why:** Add a TLS listener and deploy certs on the NLB to encrypt data in transit from clients to the load balancer and to targets as needed.

**Incorrect:**
- B: Shield/WAF protect against attacks but don’t encrypt traffic.
- C: ALB is fine for HTTP/HTTPS but switching is unnecessary; TLS on NLB meets the need.
- D: EBS encryption is at rest, not in transit.


---

---

### Question #391

A company needs a backup strategy for its three-tier stateless web application. The web application runs on Amazon EC2 instances in an Auto Scaling group with a dynamic scaling policy that is congured to respond to scaling events. The database tier runs on Amazon RDS for PostgreSQL. The web application does not require temporary local storage on the EC2 instances. The company’s recovery point objective (RPO) is 2 hours. The backup strategy must maximize scalability and optimize resource utilization for this environment. Which solution will meet these requirements?

- A. Take snapshots of Amazon Elastic Block Store (Amazon EBS) volumes of the EC2 instances and database every 2 hours to meet the RPO.

- B. Configure a snapshot lifecycle policy to take Amazon Elastic Block Store (Amazon EBS) snapshots. Enable automated backups in Amazon RDS to meet the RPO.

- C. Retain the latest Amazon Machine Images (AMIs) of the web and application tiers. Enable automated backups in Amazon RDS and use point-in-time recovery to meet the RPO.

- D. Take snapshots of Amazon Elastic Block Store (Amazon EBS) volumes of the EC2 instances every 2 hours. Enable automated backups in Amazon RDS and use point-in-time recovery to meet the RPO.

**Correct:** C
**Why:** Keep latest AMIs for stateless tiers; enable RDS automated backups with PITR to meet a 2‑hour RPO without per‑instance EBS snapshots.

**Incorrect:**
- A: EC2 EBS snapshots are unnecessary for stateless instances.
- B: AMIs are more appropriate than frequent EBS snapshots for autoscaled stateless tiers.
- D: EC2 EBS snapshots are unnecessary for stateless instances.


---

---

### Question #394

A company is running a multi-tier ecommerce web application in the AWS Cloud. The application runs on Amazon EC2 instances with an Amazon RDS for MySQL Multi-AZ DB instance. Amazon RDS is congured with the latest generation DB instance with 2,000 GB of storage in a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume. The database performance affects the application during periods of high demand. A database administrator analyzes the logs in Amazon CloudWatch Logs and discovers that the application performance always degrades when the number of read and write IOPS is higher than 20,000. What should a solutions architect do to improve the application performance?

- A. Replace the volume with a magnetic volume.

- B. Increase the number of IOPS on the gp3 volume.

- C. Replace the volume with a Provisioned IOPS SSD (io2) volume.

- D. Replace the 2,000 GB gp3 volume with two 1,000 GB gp3 volumes.

**Correct:** C
**Why:** Move to Provisioned IOPS SSD (io2) to exceed 20K IOPS limits of gp3 and ensure consistent high IOPS for peak periods.

**Incorrect:**
- A: Magnetic volumes are low performance.
- B: gp3 max IOPS is 16K—insufficient for >20K IOPS.
- D: Splitting gp3 volumes does not increase single‑volume IOPS for RDS.


---

---

### Question #401

A company wants to use the AWS Cloud to make an existing application highly available and resilient. The current version of the application resides in the company's data center. The application recently experienced data loss after a database server crashed because of an unexpected power outage. The company needs a solution that avoids any single points of failure. The solution must give the application the ability to scale to meet user demand. Which solution will meet these requirements?

- A. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon RDS DB instance in a Multi-AZ configuration.

- B. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group in a single Availability Zone. Deploy the database on an EC2 instance. Enable EC2 Auto Recovery.

- C. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon RDS DB instance with a read replica in a single Availability Zone. Promote the read replica to replace the primary DB instance if the primary DB instance fails.

- D. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Deploy the primary and secondary database servers on EC2 instances across multiple Availability Zones. Use Amazon Elastic Block Store (Amazon EBS) Multi-Attach to create shared storage between the instances.

**Correct:** A
**Why:** Multi-AZ Auto Scaling for EC2 app tier plus RDS Multi-AZ removes single points of failure and provides scalability.

**Incorrect:**
- B: Single AZ and self-managed DB are single points of failure.
- C: A single-AZ primary with read replica is not the same as Multi-AZ failover.
- D: EBS Multi-Attach is not for databases and adds complexity.


---

---

### Question #410

A company is deploying a new application on Amazon EC2 instances. The application writes data to Amazon Elastic Block Store (Amazon EBS) volumes. The company needs to ensure that all data that is written to the EBS volumes is encrypted at rest. Which solution will meet this requirement?

- A. Create an IAM role that species EBS encryption. Attach the role to the EC2 instances.

- B. Create the EBS volumes as encrypted volumes. Attach the EBS volumes to the EC2 instances.

- C. Create an EC2 instance tag that has a key of Encrypt and a value of True. Tag all instances that require encryption at the EBS level.

- D. Create an AWS Key Management Service (AWS KMS) key policy that enforces EBS encryption in the account. Ensure that the key policy is active.

**Correct:** B
**Why:** Create and attach encrypted EBS volumes (or enable EBS encryption by default) to ensure at-rest encryption.

**Incorrect:**
- A: Tags/roles/key policies do not enforce encryption by themselves.
- C: Tags/roles/key policies do not enforce encryption by themselves.
- D: Tags/roles/key policies do not enforce encryption by themselves.


---

---

### Question #419

A company uses AWS Organizations with all features enabled and runs multiple Amazon EC2 workloads in the ap-southeast-2 Region. The company has a service control policy (SCP) that prevents any resources from being created in any other Region. A security policy requires the company to encrypt all data at rest. An audit discovers that employees have created Amazon Elastic Block Store (Amazon EBS) volumes for EC2 instances without encrypting the volumes. The company wants any new EC2 instances that any IAM user or root user launches in ap-southeast-2 to use encrypted EBS volumes. The company wants a solution that will have minimal effect on employees who create EBS volumes. Which combination of steps will meet these requirements? (Choose two.)

- A. In the Amazon EC2 console, select the EBS encryption account attribute and dene a default encryption key.

- B. Create an IAM permission boundary. Attach the permission boundary to the root organizational unit (OU). Dene the boundary to deny the ec2:CreateVolume action when the ec2:Encrypted condition equals false.

- C. Create an SCP. Attach the SCP to the root organizational unit (OU). Dene the SCP to deny the ec2:CreateVolume action whenthe ec2:Encrypted condition equals false.

- D. Update the IAM policies for each account to deny the ec2:CreateVolume action when the ec2:Encrypted condition equals false.

E. In the Organizations management account, specify the Default EBS volume encryption setting.

**Correct:** A, C
**Why:** Enable EBS default encryption in the Region/account and enforce with an SCP that denies unencrypted volume creation.

**Incorrect:**
- B: Permission boundaries or per‑account policy updates are heavier and easier to bypass than an SCP.
- D: Permission boundaries or per‑account policy updates are heavier and easier to bypass than an SCP.
- E: No organization‑level default EBS encryption setting to apply globally.


---

---

### Question #421

A company runs a highly available SFTP service. The SFTP service uses two Amazon EC2 Linux instances that run with elastic IP addresses to accept trac from trusted IP sources on the internet. The SFTP service is backed by shared storage that is attached to the instances. User accounts are created and managed as Linux users in the SFTP servers. The company wants a serverless option that provides high IOPS performance and highly congurable security. The company also wants to maintain control over user permissions. Which solution will meet these requirements?

- A. Create an encrypted Amazon Elastic Block Store (Amazon EBS) volume. Create an AWS Transfer Family SFTP service with a public endpoint that allows only trusted IP addresses. Attach the EBS volume to the SFTP service endpoint. Grant users access to the SFTP service.

- B. Create an encrypted Amazon Elastic File System (Amazon EFS) volume. Create an AWS Transfer Family SFTP service with elastic IP addresses and a VPC endpoint that has internet-facing access. Attach a security group to the endpoint that allows only trusted IP addresses. Attach the EFS volume to the SFTP service endpoint. Grant users access to the SFTP service.

- C. Create an Amazon S3 bucket with default encryption enabled. Create an AWS Transfer Family SFTP service with a public endpoint that allows only trusted IP addresses. Attach the S3 bucket to the SFTP service endpoint. Grant users access to the SFTP service.

- D. Create an Amazon S3 bucket with default encryption enabled. Create an AWS Transfer Family SFTP service with a VPC endpoint that has internal access in a private subnet. Attach a security group that allows only trusted IP addresses. Attach the S3 bucket to the SFTP service endpoint. Grant users access to the SFTP service.

**Correct:** C
**Why:** Transfer Family with an S3 backend is serverless, scalable, and supports IP allow lists; S3 delivers high throughput/IOPS.

**Incorrect:**
- A: You cannot attach EBS/EFS directly to Transfer Family.
- B: You cannot attach EBS/EFS directly to Transfer Family.
- D: A private VPC endpoint is internal; trusted public IPs could not reach it.


---

---

### Question #425

A company uses high block storage capacity to runs its workloads on premises. The company's daily peak input and output transactions per second are not more than 15,000 IOPS. The company wants to migrate the workloads to Amazon EC2 and to provision disk performance independent of storage capacity. Which Amazon Elastic Block Store (Amazon EBS) volume type will meet these requirements MOST cost-effectively?

- A. GP2 volume type

- B. io2 volume type

- C. GP3 volume type

- D. io1 volume type

**Correct:** C
**Why:** gp3 lets you provision IOPS independent of capacity up to required 15K IOPS, cost‑effectively.

**Incorrect:**
- A: gp2 IOPS scale with size, not independently.
- B: io classes are pricier than needed.
- D: io classes are pricier than needed.


---

---

### Question #456

A company runs applications on Amazon EC2 instances in one AWS Region. The company wants to back up the EC2 instances to a second Region. The company also wants to provision EC2 resources in the second Region and manage the EC2 instances centrally from one AWS account. Which solution will meet these requirements MOST cost-effectively?

- A. Create a disaster recovery (DR) plan that has a similar number of EC2 instances in the second Region. Configure data replication.

- B. Create point-in-time Amazon Elastic Block Store (Amazon EBS) snapshots of the EC2 instances. Copy the snapshots to the second Region periodically.

- C. Create a backup plan by using AWS Backup. Configure cross-Region backup to the second Region for the EC2 instances.

- D. Deploy a similar number of EC2 instances in the second Region. Use AWS DataSync to transfer the data from the source Region to the second Region.

**Correct:** C
**Why:** AWS Backup with cross‑Region backup centrally handles EC2/EBS backups to a second Region cost‑effectively.

**Incorrect:**
- A: Maintaining warm DR fleets or DataSync replication increases cost/ops.
- B: Manual EBS snapshot copying lacks centralized policy/automation.
- D: Maintaining warm DR fleets or DataSync replication increases cost/ops.


---

---

### Question #465

A company is developing an application to support customer demands. The company wants to deploy the application on multiple Amazon EC2 Nitro-based instances within the same Availability Zone. The company also wants to give the application the ability to write to multiple block storage volumes in multiple EC2 Nitro-based instances simultaneously to achieve higher application availability. Which solution will meet these requirements?

- A. Use General Purpose SSD (gp3) EBS volumes with Amazon Elastic Block Store (Amazon EBS) Multi-Attach

- B. Use Throughput Optimized HDD (st1) EBS volumes with Amazon Elastic Block Store (Amazon EBS) Multi-Attach

- C. Use Provisioned IOPS SSD (io2) EBS volumes with Amazon Elastic Block Store (Amazon EBS) Multi-Attach

- D. Use General Purpose SSD (gp2) EBS volumes with Amazon Elastic Block Store (Amazon EBS) Multi-Attach

**Correct:** C
**Why:** io2 supports EBS Multi‑Attach on Nitro instances, allowing simultaneous block access by multiple instances.

**Incorrect:**
- A: gp2/gp3/st1 do not support Multi‑Attach.
- B: gp2/gp3/st1 do not support Multi‑Attach.
- D: gp2/gp3/st1 do not support Multi‑Attach.


---

---

### Question #487

A company seeks a storage solution for its application. The solution must be highly available and scalable. The solution also must function as a file system be mountable by multiple Linux instances in AWS and on premises through native protocols, and have no minimum size requirements. The company has set up a Site-to-Site VPN for access from its on-premises network to its VPC. Which storage solution meets these requirements?

- A. Amazon FSx Multi-AZ deployments

- B. Amazon Elastic Block Store (Amazon EBS) Multi-Attach volumes

- C. Amazon Elastic File System (Amazon EFS) with multiple mount targets

- D. Amazon Elastic File System (Amazon EFS) with a single mount target and multiple access points

**Correct:** C
**Why:** EFS is a scalable, highly available NFS file system with multiple mount targets; accessible from on‑prem via VPN.

**Incorrect:**
- A: FSx options/EBS don’t match NFS multi‑attach across many clients.
- B: FSx options/EBS don’t match NFS multi‑attach across many clients.
- D: FSx options/EBS don’t match NFS multi‑attach across many clients.


---

## Amazon EC2 / Auto Scaling

### Question #251

An Amazon EC2 instance is located in a private subnet in a new VPC. This subnet does not have outbound internet access, but the EC2 instance needs the ability to download monthly security updates from an outside vendor. What should a solutions architect do to meet these requirements?

- A. Create an internet gateway, and attach it to the VPC. Configure the private subnet route table to use the internet gateway as the default route.

- B. Create a NAT gateway, and place it in a public subnet. Configure the private subnet route table to use the NAT gateway as the default route.

- C. Create a NAT instance, and place it in the same subnet where the EC2 instance is located. Configure the private subnet route table to use the NAT instance as the default route.

- D. Create an internet gateway, and attach it to the VPC. Create a NAT instance, and place it in the same subnet where the EC2 instance is located. Configure the private subnet route table to use the internet gateway as the default route.

**Correct:** B
**Why:** Place a NAT gateway in a public subnet and route the private subnet’s default route to it for secure outbound internet access.

**Incorrect:**
- A: An internet gateway alone does not provide egress for private subnets.
- C: NAT instance in same private subnet will not work and adds ops burden.
- D: An internet gateway alone does not provide egress for private subnets.


---

---

### Question #252

A solutions architect needs to design a system to store client case files. The files are core company assets and are important. The number of files will grow over time. The files must be simultaneously accessible from multiple application servers that run on Amazon EC2 instances. The solution must have built-in redundancy. Which solution meets these requirements?

- A. Amazon Elastic File System (Amazon EFS)

- B. Amazon Elastic Block Store (Amazon EBS)

- C. Amazon S3 Glacier Deep Archive

- D. AWS Backup

**Correct:** A
**Why:** Amazon EFS provides shared, redundant NFS storage accessible concurrently by multiple EC2 instances.

**Incorrect:**
- B: EBS is block storage attached to a single AZ/instance at a time.
- C: Glacier Deep Archive is archival, not shared storage.
- D: AWS Backup is for backups, not primary shared storage.


---

---

### Question #253

A solutions architect has created two IAM policies: Policy1 and Policy2. Both policies are attached to an IAM group. A cloud engineer is added as an IAM user to the IAM group. The policy documents are:

Policy 1
{
	"Version": "2012-10-17",
	"Statement": [
		{
			"Effect": "Allow",
			"Action": [
				"iam:Get*",
				"iam:List*",
				"kms:List*",
				"ec2:*",
				"ds:*",
				"logs:Get*",
				"logs:Describe*"
			],
			"Resource": "*"
		}
	]
}

Policy 2
{
	"Version": "2012-10-17",
	"Statement": [
		{
			"Effect": "Deny",
			"Action": "ds:Delete*",
			"Resource": "*"
		}
	]
}

Which action will the cloud engineer be able to perform?

- A. Deleting IAM users

- B. Deleting directories

- C. Deleting Amazon EC2 instances

- D. Deleting logs from Amazon CloudWatch Logs

**Correct:** C
**Why:** Policy 1 explicitly allows ec2:* so terminating/deleting EC2 instances is allowed. Policy 2 adds an explicit Deny only for ds:Delete*, which overrides ds:* but does not affect EC2. Deleting IAM users and CloudWatch Logs are not covered by the allowed actions (only iam:Get*/List* and logs:Get*/Describe* are allowed).

**Incorrect:**
- A: Not allowed. Only read-only IAM permissions (iam:Get*/List*); DeleteUser requires iam:DeleteUser.
- B: Explicitly denied by Policy 2 (ds:Delete*), which overrides any Allow.
- D: Not allowed. Only logs:Get*/Describe* are allowed; deletions require logs:Delete* permissions.


---

---

### Question #254

A company is reviewing a recent migration of a three-tier application to a VPC. The security team discovers that the principle of least privilege is not being applied to Amazon EC2 security group ingress and egress rules between the application tiers. What should a solutions architect do to correct this issue?

- A. Create security group rules using the instance ID as the source or destination.

- B. Create security group rules using the security group ID as the source or destination.

- C. Create security group rules using the VPC CIDR blocks as the source or destination.

- D. Create security group rules using the subnet CIDR blocks as the source or destination.

**Correct:** B
**Why:** Reference security group IDs in rules to tightly allow only the specific tier’s traffic (least privilege).

**Incorrect:**
- A: Instance IDs, VPC CIDRs, or subnet CIDRs are broader and less maintainable.
- C: Instance IDs, VPC CIDRs, or subnet CIDRs are broader and less maintainable.
- D: Instance IDs, VPC CIDRs, or subnet CIDRs are broader and less maintainable.


---

---

### Question #257

A company is building a solution that will report Amazon EC2 Auto Scaling events across all the applications in an AWS account. The company needs to use a serverless solution to store the EC2 Auto Scaling status data in Amazon S3. The company then will use the data in Amazon S3 to provide near-real-time updates in a dashboard. The solution must not affect the speed of EC2 instance launches. How should the company move the data to Amazon S3 to meet these requirements?

- A. Use an Amazon CloudWatch metric stream to send the EC2 Auto Scaling status data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.

- B. Launch an Amazon EMR cluster to collect the EC2 Auto Scaling status data and send the data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.

- C. Create an Amazon EventBridge rule to invoke an AWS Lambda function on a schedule. Configure the Lambda function to send the EC2 Auto Scaling status data directly to Amazon S3.

- D. Use a bootstrap script during the launch of an EC2 instance to install Amazon Kinesis Agent. Configure Kinesis Agent to collect the EC2 Auto Scaling status data and send the data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.

**Correct:** A
**Why:** CloudWatch metric streams → Kinesis Data Firehose → S3 is serverless and near real time without impacting launches.

**Incorrect:**
- B: EMR/Kinesis Agent add unnecessary infrastructure.
- C: Scheduled polling adds latency and complexity.
- D: EMR/Kinesis Agent add unnecessary infrastructure.


---

---

### Question #261

A company recently announced the deployment of its retail website to a global audience. The website runs on multiple Amazon EC2 instances behind an Elastic Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The company wants to provide its customers with different versions of content based on the devices that the customers use to access the website. Which combination of actions should a solutions architect take to meet these requirements? (Choose two.)

- A. Configure Amazon CloudFront to cache multiple versions of the content.

- B. Configure a host header in a Network Load Balancer to forward trac to different instances.

- C. Configure a Lambda@Edge function to send specic objects to users based on the User-Agent header.

- D. Configure AWS Global Accelerator. Forward requests to a Network Load Balancer (NLB). Configure the NLB to set up host-based routing to different EC2 instances.

E. Configure AWS Global Accelerator. Forward requests to a Network Load Balancer (NLB). Configure the NLB to set up path-based routing to different EC2 instances.

**Correct:** A, C
**Why:** Use CloudFront for global caching and Lambda@Edge to vary content based on User‑Agent header.

**Incorrect:**
- B: NLB host/path routing or Global Accelerator aren’t needed for device‑specific content at the edge.
- D: NLB host/path routing or Global Accelerator aren’t needed for device‑specific content at the edge.
- E: NLB host/path routing or Global Accelerator aren’t needed for device‑specific content at the edge.


---

---

### Question #262

A company plans to use Amazon ElastiCache for its multi-tier web application. A solutions architect creates a Cache VPC for the ElastiCache cluster and an App VPC for the application’s Amazon EC2 instances. Both VPCs are in the us-east-1 Region. The solutions architect must implement a solution to provide the application’s EC2 instances with access to the ElastiCache cluster. Which solution will meet these requirements MOST cost-effectively?

- A. Create a peering connection between the VPCs. Add a route table entry for the peering connection in both VPCs. Configure an inbound rule for the ElastiCache cluster’s security group to allow inbound connection from the application’s security group.

- B. Create a Transit VPC. Update the VPC route tables in the Cache VPC and the App VPC to route trac through the Transit VPC. Configure an inbound rule for the ElastiCache cluster's security group to allow inbound connection from the application’s security group.

- C. Create a peering connection between the VPCs. Add a route table entry for the peering connection in both VPCs. Configure an inbound rule for the peering connection’s security group to allow inbound connection from the application’s security group.

- D. Create a Transit VPC. Update the VPC route tables in the Cache VPC and the App VPC to route trac through the Transit VPC. Configure an inbound rule for the Transit VPC’s security group to allow inbound connection from the application’s security group.

**Correct:** A
**Why:** VPC peering with route entries and SG inbound from the app’s SG is the most cost‑effective cross‑VPC access.

**Incorrect:**
- B: Transit VPC adds cost/complexity without need.
- C: Peering connections don’t have security groups.
- D: Transit VPC adds cost/complexity without need.


---

---

### Question #263

A company is building an application that consists of several microservices. The company has decided to use container technologies to deploy its software on AWS. The company needs a solution that minimizes the amount of ongoing effort for maintenance and scaling. The company cannot manage additional infrastructure. Which combination of actions should a solutions architect take to meet these requirements? (Choose two.)

- A. Deploy an Amazon Elastic Container Service (Amazon ECS) cluster.

- B. Deploy the Kubernetes control plane on Amazon EC2 instances that span multiple Availability Zones.

- C. Deploy an Amazon Elastic Container Service (Amazon ECS) service with an Amazon EC2 launch type. Specify a desired task number level of greater than or equal to 2.

- D. Deploy an Amazon Elastic Container Service (Amazon ECS) service with a Fargate launch type. Specify a desired task number level of greater than or equal to 2.

E. Deploy Kubernetes worker nodes on Amazon EC2 instances that span multiple Availability Zones. Create a deployment that species two or more replicas for each microservice.

**Correct:** A, D
**Why:** Use ECS (control plane managed by AWS) and run services on Fargate with at least two tasks for HA and minimal ops.

**Incorrect:**
- B: Managing EKS control plane or EC2 workers adds ops.
- C: EC2 launch type manages instances directly.
- E: Managing EKS control plane or EC2 workers adds ops.


---

---

### Question #264

A company has a web application hosted over 10 Amazon EC2 instances with trac directed by Amazon Route 53. The company occasionally experiences a timeout error when attempting to browse the application. The networking team nds that some DNS queries return IP addresses of unhealthy instances, resulting in the timeout error. What should a solutions architect implement to overcome these timeout errors?

- A. Create a Route 53 simple routing policy record for each EC2 instance. Associate a health check with each record.

- B. Create a Route 53 failover routing policy record for each EC2 instance. Associate a health check with each record.

- C. Create an Amazon CloudFront distribution with EC2 instances as its origin. Associate a health check with the EC2 instances.

- D. Create an Application Load Balancer (ALB) with a health check in front of the EC2 instances. Route to the ALB from Route 53.

**Correct:** D
**Why:** Put an ALB with health checks in front of instances and route DNS to the ALB to avoid returning unhealthy instance IPs.

**Incorrect:**
- A: Route 53 alone still returns unhealthy A records without ALB health integration.
- B: Route 53 alone still returns unhealthy A records without ALB health integration.
- C: CloudFront is for CDN, not origin health for EC2 fleet.


---

---

### Question #265

A solutions architect needs to design a highly available application consisting of web, application, and database tiers. HTTPS content delivery should be as close to the edge as possible, with the least delivery time. Which solution meets these requirements and is MOST secure?

- A. Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in public subnets. Configure Amazon CloudFront to deliver HTTPS content using the public ALB as the origin.

- B. Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in private subnets. Configure Amazon CloudFront to deliver HTTPS content using the EC2 instances as the origin.

- C. Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in private subnets. Configure Amazon CloudFront to deliver HTTPS content using the public ALB as the origin.

- D. Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in public subnets. Configure Amazon CloudFront to deliver HTTPS content using the EC2 instances as the origin.

**Correct:** C
**Why:** Place EC2 in private subnets behind a public ALB; put CloudFront in front for HTTPS at the edge.

**Incorrect:**
- A: Either expose EC2 publicly or skip CloudFront edge TLS offload.
- B: Either expose EC2 publicly or skip CloudFront edge TLS offload.
- D: Either expose EC2 publicly or skip CloudFront edge TLS offload.


---

---

### Question #266

A company has a popular gaming platform running on AWS. The application is sensitive to latency because latency can impact the user experience and introduce unfair advantages to some players. The application is deployed in every AWS Region. It runs on Amazon EC2 instances that are part of Auto Scaling groups congured behind Application Load Balancers (ALBs). A solutions architect needs to implement a mechanism to monitor the health of the application and redirect trac to healthy endpoints. Which solution meets these requirements?

- A. Configure an accelerator in AWS Global Accelerator. Add a listener for the port that the application listens on, and attach it to a Regional endpoint in each Region. Add the ALB as the endpoint.

- B. Create an Amazon CloudFront distribution and specify the ALB as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the trac.

- C. Create an Amazon CloudFront distribution and specify Amazon S3 as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the trac.

- D. Configure an Amazon DynamoDB database to serve as the data store for the application. Create a DynamoDB Accelerator (DAX) cluster to act as the in-memory cache for DynamoDB hosting the application data.

**Correct:** A
**Why:** Global Accelerator provides health checks and intelligent routing to the nearest healthy Regional endpoint (ALB).

**Incorrect:**
- B: CloudFront/S3 or DynamoDB/DAX don’t meet UDP/TCP app health redirect needs across Regions.
- C: CloudFront/S3 or DynamoDB/DAX don’t meet UDP/TCP app health redirect needs across Regions.
- D: CloudFront/S3 or DynamoDB/DAX don’t meet UDP/TCP app health redirect needs across Regions.


---

---

### Question #268

A gaming company has a web application that displays scores. The application runs on Amazon EC2 instances behind an Application Load Balancer. The application stores data in an Amazon RDS for MySQL database. Users are starting to experience long delays and interruptions that are caused by database read performance. The company wants to improve the user experience while minimizing changes to the application’s architecture. What should a solutions architect do to meet these requirements?

- A. Use Amazon ElastiCache in front of the database.

- B. Use RDS Proxy between the application and the database.

- C. Migrate the application from EC2 instances to AWS Lambda.

- D. Migrate the database from Amazon RDS for MySQL to Amazon DynamoDB.

**Correct:** A
**Why:** Add ElastiCache in front of RDS to reduce read pressure without major architecture change.

**Incorrect:**
- B: RDS Proxy assists connections, not read latency.
- C: Lambda/DynamoDB are re‑architectures.
- D: Lambda/DynamoDB are re‑architectures.


---

---

### Question #271

A solutions architect observes that a nightly batch processing job is automatically scaled up for 1 hour before the desired Amazon EC2 capacity is reached. The peak capacity is the ‘same every night and the batch jobs always start at 1 AM. The solutions architect needs to nd a cost-effective solution that will allow for the desired EC2 capacity to be reached quickly and allow the Auto Scaling group to scale down after the batch jobs are complete. What should the solutions architect do to meet these requirements?

- A. Increase the minimum capacity for the Auto Scaling group.

- B. Increase the maximum capacity for the Auto Scaling group.

- C. Configure scheduled scaling to scale up to the desired compute level.

- D. Change the scaling policy to add more EC2 instances during each scaling operation.

**Correct:** C
**Why:** Scheduled scaling to pre‑scale 30 minutes before jobs avoids slow warm‑up while allowing scale down after.

**Incorrect:**
- A: Adjusting min/max or step policy won’t guarantee pre‑scale in time.
- B: Adjusting min/max or step policy won’t guarantee pre‑scale in time.
- D: Adjusting min/max or step policy won’t guarantee pre‑scale in time.


---

---

### Question #272

A company serves a dynamic website from a eet of Amazon EC2 instances behind an Application Load Balancer (ALB). The website needs to support multiple languages to serve customers around the world. The website’s architecture is running in the us-west-1 Region and is exhibiting high request latency for users that are located in other parts of the world. The website needs to serve requests quickly and eciently regardless of a user’s location. However, the company does not want to recreate the existing architecture across multiple Regions. What should a solutions architect do to meet these requirements?

- A. Replace the existing architecture with a website that is served from an Amazon S3 bucket. Configure an Amazon CloudFront distribution with the S3 bucket as the origin. Set the cache behavior settings to cache based on the Accept-Language request header.

- B. Configure an Amazon CloudFront distribution with the ALB as the origin. Set the cache behavior settings to cache based on the Accept- Language request header.

- C. Create an Amazon API Gateway API that is integrated with the ALB. Configure the API to use the HTTP integration type. Set up an API Gateway stage to enable the API cache based on the Accept-Language request header.

- D. Launch an EC2 instance in each additional Region and configure NGINX to act as a cache server for that Region. Put all the EC2 instances and the ALB behind an Amazon Route 53 record set with a geolocation routing policy.

**Correct:** B
**Why:** CloudFront with ALB origin and cache based on Accept‑Language provides low latency globally without multi‑Region.

**Incorrect:**
- A: S3 static site or regional NGINX cache adds complexity or doesn’t fit dynamic origin.
- C: S3 static site or regional NGINX cache adds complexity or doesn’t fit dynamic origin.
- D: S3 static site or regional NGINX cache adds complexity or doesn’t fit dynamic origin.


---

---

### Question #274

A company runs an application on Amazon EC2 instances. The company needs to implement a disaster recovery (DR) solution for the application. The DR solution needs to have a recovery time objective (RTO) of less than 4 hours. The DR solution also needs to use the fewest possible AWS resources during normal operations. Which solution will meet these requirements in the MOST operationally ecient way?

- A. Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a secondary AWS Region. Automate infrastructure deployment in the secondary Region by using AWS Lambda and custom scripts.

- B. Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a secondary AWS Region. Automate infrastructure deployment in the secondary Region by using AWS CloudFormation.

- C. Launch EC2 instances in a secondary AWS Region. Keep the EC2 instances in the secondary Region active at all times.

- D. Launch EC2 instances in a secondary Availability Zone. Keep the EC2 instances in the secondary Availability Zone active at all times.

**Correct:** B
**Why:** AMIs copied cross‑Region and CloudFormation to provision infra offers <4h RTO with minimal steady‑state resources.

**Incorrect:**
- A: Custom scripts add ops.
- C: Keeping instances running increases steady‑state cost.
- D: Keeping instances running increases steady‑state cost.


---

---

### Question #275

A company runs an internal browser-based application. The application runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. The Auto Scaling group scales up to 20 instances during work hours, but scales down to 2 instances overnight. Staff are complaining that the application is very slow when the day begins, although it runs well by mid-morning. How should the scaling be changed to address the staff complaints and keep costs to a minimum?

- A. Implement a scheduled action that sets the desired capacity to 20 shortly before the oce opens.

- B. Implement a step scaling action triggered at a lower CPU threshold, and decrease the cooldown period.

- C. Implement a target tracking action triggered at a lower CPU threshold, and decrease the cooldown period.

- D. Implement a scheduled action that sets the minimum and maximum capacity to 20 shortly before the oce opens.

**Correct:** A
**Why:** Pre‑scale via scheduled action to desired capacity before office hours to avoid cold starts while minimizing costs.

**Incorrect:**
- B: Threshold policies or setting min/max to 20 waste cost or react too late.
- C: Threshold policies or setting min/max to 20 waste cost or react too late.
- D: Threshold policies or setting min/max to 20 waste cost or react too late.


---

---

### Question #276

A company has a multi-tier application deployed on several Amazon EC2 instances in an Auto Scaling group. An Amazon RDS for Oracle instance is the application’ s data layer that uses Oracle-specic PL/SQL functions. Trac to the application has been steadily increasing. This is causing the EC2 instances to become overloaded and the RDS instance to run out of storage. The Auto Scaling group does not have any scaling metrics and denes the minimum healthy instance count only. The company predicts that trac will continue to increase at a steady but unpredictable rate before leveling off. What should a solutions architect do to ensure the system can automatically scale for the increased trac? (Choose two.)

- A. Configure storage Auto Scaling on the RDS for Oracle instance.

- B. Migrate the database to Amazon Aurora to use Auto Scaling storage.

- C. Configure an alarm on the RDS for Oracle instance for low free storage space.

- D. Configure the Auto Scaling group to use the average CPU as the scaling metric.

E. Configure the Auto Scaling group to use the average free memory as the scaling metric.

**Correct:** A, D
**Why:** Enable RDS storage autoscaling and scale EC2 fleet on average CPU to handle increasing traffic automatically.

**Incorrect:**
- B: Aurora migration is heavier change.
- C: Alarm only or memory metric alone is insufficient as primary scaler.
- E: Alarm only or memory metric alone is insufficient as primary scaler.


---

---

### Question #277

A company provides an online service for posting video content and transcoding it for use by any mobile platform. The application architecture uses Amazon Elastic File System (Amazon EFS) Standard to collect and store the videos so that multiple Amazon EC2 Linux instances can access the video content for processing. As the popularity of the service has grown over time, the storage costs have become too expensive. Which storage solution is MOST cost-effective?

- A. Use AWS Storage Gateway for files to store and process the video content.

- B. Use AWS Storage Gateway for volumes to store and process the video content.

- C. Use Amazon EFS for storing the video content. Once processing is complete, transfer the files to Amazon Elastic Block Store (Amazon EBS).

- D. Use Amazon S3 for storing the video content. Move the files temporarily over to an Amazon Elastic Block Store (Amazon EBS) volume attached to the server for processing.

**Correct:** D
**Why:** Store content durably/cost‑effectively in S3 and move to EBS temporarily for processing on EC2.

**Incorrect:**
- A: Storage Gateway is not optimal for large‑scale processing cost.
- B: Storage Gateway is not optimal for large‑scale processing cost.
- C: Keeping in EFS Standard is costly for infrequently accessed media.


---

---

### Question #281

A company runs a eet of web servers using an Amazon RDS for PostgreSQL DB instance. After a routine compliance check, the company sets a standard that requires a recovery point objective (RPO) of less than 1 second for all its production databases. Which solution meets these requirements?

- A. Enable a Multi-AZ deployment for the DB instance.

- B. Enable auto scaling for the DB instance in one Availability Zone.

- C. Configure the DB instance in one Availability Zone, and create multiple read replicas in a separate Availability Zone.

- D. Configure the DB instance in one Availability Zone, and configure AWS Database Migration Service (AWS DMS) change data capture (CDC) tasks.

**Correct:** A
**Why:** RDS Multi‑AZ uses synchronous replication to a standby, providing near‑zero RPO (<1s) and automatic failover.

**Incorrect:**
- B: Do not meet the <1‑second RPO requirement.
- C: Do not meet the <1‑second RPO requirement.
- D: Do not meet the <1‑second RPO requirement.


---

---

### Question #282

A company runs a web application that is deployed on Amazon EC2 instances in the private subnet of a VPC. An Application Load Balancer (ALB) that extends across the public subnets directs web trac to the EC2 instances. The company wants to implement new security measures to restrict inbound trac from the ALB to the EC2 instances while preventing access from any other source inside or outside the private subnet of the EC2 instances. Which solution will meet these requirements?

- A. Configure a route in a route table to direct trac from the internet to the private IP addresses of the EC2 instances.

- B. Configure the security group for the EC2 instances to only allow trac that comes from the security group for the ALB.

- C. Move the EC2 instances into the public subnet. Give the EC2 instances a set of Elastic IP addresses.

- D. Configure the security group for the ALB to allow any TCP trac on any port.

**Correct:** B
**Why:** Allow only the ALB security group as a source to the EC2 instances’ security group.

**Incorrect:**
- A: Route table or broad ALB SG rules do not enforce least privilege inbound to EC2.
- C: Route table or broad ALB SG rules do not enforce least privilege inbound to EC2.
- D: Route table or broad ALB SG rules do not enforce least privilege inbound to EC2.


---

---

### Question #283

A research company runs experiments that are powered by a simulation application and a visualization application. The simulation application runs on Linux and outputs intermediate data to an NFS share every 5 minutes. The visualization application is a Windows desktop application that displays the simulation output and requires an SMB file system. The company maintains two synchronized file systems. This strategy is causing data duplication and inecient resource usage. The company needs to migrate the applications to AWS without making code changes to either application. Which solution will meet these requirements?

- A. Migrate both applications to AWS Lambda. Create an Amazon S3 bucket to exchange data between the applications.

- B. Migrate both applications to Amazon Elastic Container Service (Amazon ECS). Configure Amazon FSx File Gateway for storage.

- C. Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization application to Windows EC2 instances. Configure Amazon Simple Queue Service (Amazon SQS) to exchange data between the applications.

- D. Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization application to Windows EC2 instances. Configure Amazon FSx for NetApp ONTAP for storage.

**Correct:** D
**Why:** FSx for NetApp ONTAP supports both NFS and SMB on the same data, removing duplicate storage and maintaining app compatibility.

**Incorrect:**
- A: Lambda/ECS + File Gateway or SQS don’t solve multi‑protocol file sharing.
- B: Lambda/ECS + File Gateway or SQS don’t solve multi‑protocol file sharing.
- C: Lambda/ECS + File Gateway or SQS don’t solve multi‑protocol file sharing.


---

---

### Question #285

A company hosts its static website by using Amazon S3. The company wants to add a contact form to its webpage. The contact form will have dynamic server-side components for users to input their name, email address, phone number, and user message. The company anticipates that there will be fewer than 100 site visits each month. Which solution will meet these requirements MOST cost-effectively?

- A. Host a dynamic contact form page in Amazon Elastic Container Service (Amazon ECS). Set up Amazon Simple Email Service (Amazon SES) to connect to any third-party email provider.

- B. Create an Amazon API Gateway endpoint with an AWS Lambda backend that makes a call to Amazon Simple Email Service (Amazon SES).

- C. Convert the static webpage to dynamic by deploying Amazon Lightsail. Use client-side scripting to build the contact form. Integrate the form with Amazon WorkMail.

- D. Create a t2.micro Amazon EC2 instance. Deploy a LAMP (Linux, Apache, MySQL, PHP/Perl/Python) stack to host the webpage. Use client- side scripting to build the contact form. Integrate the form with Amazon WorkMail.

**Correct:** B
**Why:** API Gateway + Lambda → SES adds minimal serverless backend to a static S3 site.

**Incorrect:**
- A: ECS/Lightsail/EC2 add cost and ops for very low traffic.
- C: ECS/Lightsail/EC2 add cost and ops for very low traffic.
- D: ECS/Lightsail/EC2 add cost and ops for very low traffic.


---

---

### Question #287

A company wants to migrate a Windows-based application from on premises to the AWS Cloud. The application has three tiers: an application tier, a business tier, and a database tier with Microsoft SQL Server. The company wants to use specic features of SQL Server such as native backups and Data Quality Services. The company also needs to share files for processing between the tiers. How should a solutions architect design the architecture to meet these requirements?

- A. Host all three tiers on Amazon EC2 instances. Use Amazon FSx File Gateway for file sharing between the tiers.

- B. Host all three tiers on Amazon EC2 instances. Use Amazon FSx for Windows File Server for file sharing between the tiers.

- C. Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon RDS. Use Amazon Elastic File System (Amazon EFS) for file sharing between the tiers.

- D. Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon RDS. Use a Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) volume for file sharing between the tiers.

**Correct:** B
**Why:** Keep SQL Server features by hosting DB on EC2; use FSx for Windows File Server for shared files; EC2 for app tiers.

**Incorrect:**
- C: RDS may not support required SQL Server features; EBS for file sharing is inappropriate.
- D: RDS may not support required SQL Server features; EBS for file sharing is inappropriate.


---

---

### Question #290

A company hosts a web application on multiple Amazon EC2 instances. The EC2 instances are in an Auto Scaling group that scales in response to user demand. The company wants to optimize cost savings without making a long-term commitment. Which EC2 instance purchasing option should a solutions architect recommend to meet these requirements?

- A. Dedicated Instances only

- B. On-Demand Instances only

- C. A mix of On-Demand Instances and Spot Instances

- D. A mix of On-Demand Instances and Reserved Instances

**Correct:** C
**Why:** Use On‑Demand for baseline with Spot Instances for bursts to reduce cost without long commitments.

**Incorrect:**
- A: Dedicated/only On‑Demand or RIs reduce flexibility/cost optimization.
- B: Dedicated/only On‑Demand or RIs reduce flexibility/cost optimization.
- D: Dedicated/only On‑Demand or RIs reduce flexibility/cost optimization.


---

---

### Question #294

An application that is hosted on Amazon EC2 instances needs to access an Amazon S3 bucket. Trac must not traverse the internet. How should a solutions architect configure access to meet these requirements?

- A. Create a private hosted zone by using Amazon Route 53.

- B. Set up a gateway VPC endpoint for Amazon S3 in the VPC.

- C. Configure the EC2 instances to use a NAT gateway to access the S3 bucket.

- D. Establish an AWS Site-to-Site VPN connection between the VPC and the S3 bucket.

**Correct:** B
**Why:** S3 gateway VPC endpoint keeps traffic private between EC2 and S3.

**Incorrect:**
- A: Private hosted zones/NAT/VPN are unnecessary for S3 private access.
- C: Private hosted zones/NAT/VPN are unnecessary for S3 private access.
- D: Private hosted zones/NAT/VPN are unnecessary for S3 private access.


---

---

### Question #296

A development team has launched a new application that is hosted on Amazon EC2 instances inside a development VPC. A solutions architect needs to create a new VPC in the same account. The new VPC will be peered with the development VPC. The VPC CIDR block for the development VPC is 192.168.0.0/24. The solutions architect needs to create a CIDR block for the new VPC. The CIDR block must be valid for a VPC peering connection to the development VPC. What is the SMALLEST CIDR block that meets these requirements?

- A. 10.0.1.0/32

- B. 192.168.0.0/24

- C. 192.168.1.0/32

- D. 10.0.1.0/24

**Correct:** D
**Why:** VPC peering requires non‑overlapping CIDRs; /24 is the smallest VPC CIDR; 10.0.1.0/24 does not overlap.

**Incorrect:**
- A: /32 is invalid for VPC.
- B: Overlaps with 192.168.0.0/24.
- C: /32 is invalid for VPC.


---

---

### Question #297

A company deploys an application on ve Amazon EC2 instances. An Application Load Balancer (ALB) distributes trac to the instances by using a target group. The average CPU usage on each of the instances is below 10% most of the time, with occasional surges to 65%. A solutions architect needs to implement a solution to automate the scalability of the application. The solution must optimize the cost of the architecture and must ensure that the application has enough CPU resources when surges occur. Which solution will meet these requirements?

- A. Create an Amazon CloudWatch alarm that enters the ALARM state when the CPUUtilization metric is less than 20%. Create an AWS Lambda function that the CloudWatch alarm invokes to terminate one of the EC2 instances in the ALB target group.

- B. Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing target group as the target group. Set a target tracking scaling policy that is based on the ASGAverageCPUUtilization metric. Set the minimum instances to 2, the desired capacity to 3, the maximum instances to 6, and the target value to 50%. Add the EC2 instances to the Auto Scaling group.

- C. Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing target group as the target group. Set the minimum instances to 2, the desired capacity to 3, and the maximum instances to 6. Add the EC2 instances to the Auto Scaling group.

- D. Create two Amazon CloudWatch alarms. Configure the first CloudWatch alarm to enter the ALARM state when the average CPUUtilization metric is below 20%. Configure the second CloudWatch alarm to enter the ALARM state when the average CPUUtilization matric is above 50%. Configure the alarms to publish to an Amazon Simple Notification Service (Amazon SNS) topic to send an email message. After receiving the message, log in to decrease or increase the number of EC2 instances that are running.

**Correct:** B
**Why:** Create an Auto Scaling group with target tracking on CPU and attach to existing ALB with appropriate min/desired/max.

**Incorrect:**
- A: Manual alarms/emails are not automation.
- C: Missing scaling policy.
- D: Manual alarms/emails are not automation.


---

---

### Question #298

A company is running a critical business application on Amazon EC2 instances behind an Application Load Balancer. The EC2 instances run in an Auto Scaling group and access an Amazon RDS DB instance. The design did not pass an operational review because the EC2 instances and the DB instance are all located in a single Availability Zone. A solutions architect must update the design to use a second Availability Zone. Which solution will make the application highly available?

- A. Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance with connections to each network.

- B. Provision two subnets that extend across both Availability Zones. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance with connections to each network.

- C. Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance for Multi-AZ deployment.

- D. Provision a subnet that extends across both Availability Zones. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance for Multi-AZ deployment.

**Correct:** C
**Why:** Distribute EC2 across AZs via ASG and enable RDS Multi‑AZ for database HA.

**Incorrect:**
- A: Incorrect subnet constructs or missing Multi‑AZ DB.
- B: Incorrect subnet constructs or missing Multi‑AZ DB.
- D: Incorrect subnet constructs or missing Multi‑AZ DB.


---

---

### Question #299

A research laboratory needs to process approximately 8 TB of data. The laboratory requires sub-millisecond latencies and a minimum throughput of 6 GBps for the storage subsystem. Hundreds of Amazon EC2 instances that run Amazon Linux will distribute and process the data. Which solution will meet the performance requirements?

- A. Create an Amazon FSx for NetApp ONTAP file system. Sat each volume’ tiering policy to ALL. Import the raw data into the file system. Mount the la system on the EC2 instances.

- B. Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system that uses persistent SSD storage. Select the option to import data from and export data to Amazon S3. Mount the file system on the EC2 instances.

- C. Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system that uses persistent HDD storage. Select the option to import data from and export data to Amazon S3. Mount the file system on the EC2 instances.

- D. Create an Amazon FSx for NetApp ONTAP file system. Set each volume’s tiering policy to NONE. Import the raw data into the file system. Mount the file system on the EC2 instances.

**Correct:** B
**Why:** FSx for Lustre persistent SSD with S3 integration meets sub‑ms latency and high throughput across many EC2 instances.

**Incorrect:**
- A: NetApp ONTAP tiering/none policies don’t match performance requirements.
- C: HDD tier insufficient for performance.
- D: NetApp ONTAP tiering/none policies don’t match performance requirements.


---

---

### Question #300

A company needs to migrate a legacy application from an on-premises data center to the AWS Cloud because of hardware capacity constraints. The application runs 24 hours a day, 7 days a week. The application’s database storage continues to grow over time. What should a solutions architect do to meet these requirements MOST cost-effectively?

- A. Migrate the application layer to Amazon EC2 Spot Instances. Migrate the data storage layer to Amazon S3.

- B. Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer to Amazon RDS On-Demand Instances.

- C. Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer to Amazon Aurora Reserved Instances.

- D. Migrate the application layer to Amazon EC2 On-Demand Instances. Migrate the data storage layer to Amazon RDS Reserved Instances.

**Correct:** C
**Why:** 24/7 app → EC2 RIs; Aurora RIs for growing database with storage autoscaling; cost‑effective for continuous use.

**Incorrect:**
- A: Spot unsuitable for 24/7.
- B: On‑Demand for DB or app increases cost.
- D: On‑Demand for DB or app increases cost.


---

---

### Question #302

A company wants to create a mobile app that allows users to stream slow-motion video clips on their mobile devices. Currently, the app captures video clips and uploads the video clips in raw format into an Amazon S3 bucket. The app retrieves these video clips directly from the S3 bucket. However, the videos are large in their raw format. Users are experiencing issues with buffering and playback on mobile devices. The company wants to implement solutions to maximize the performance and scalability of the app while minimizing operational overhead. Which combination of solutions will meet these requirements? (Choose two.)

- A. Deploy Amazon CloudFront for content delivery and caching.

- B. Use AWS DataSync to replicate the video files across AW'S Regions in other S3 buckets.

- C. Use Amazon Elastic Transcoder to convert the video files to more appropriate formats.

- D. Deploy an Auto Sealing group of Amazon EC2 instances in Local Zones for content delivery and caching.

E. Deploy an Auto Scaling group of Amazon EC2 instances to convert the video files to more appropriate formats.

**Correct:** A, C
**Why:** CloudFront accelerates delivery and caching; Elastic Transcoder (or AWS Elemental MediaConvert) converts to mobile‑friendly formats.

**Incorrect:**
- B: Cross‑Region replication or EC2 fleets add ops without solving core playback/format issues.
- D: Cross‑Region replication or EC2 fleets add ops without solving core playback/format issues.
- E: Cross‑Region replication or EC2 fleets add ops without solving core playback/format issues.


---

---

### Question #303

A company is launching a new application deployed on an Amazon Elastic Container Service (Amazon ECS) cluster and is using the Fargate launch type for ECS tasks. The company is monitoring CPU and memory usage because it is expecting high trac to the application upon its launch. However, the company wants to reduce costs when utilization decreases. What should a solutions architect recommend?

- A. Use Amazon EC2 Auto Scaling to scale at certain periods based on previous trac patterns.

- B. Use an AWS Lambda function to scale Amazon ECS based on metric breaches that trigger an Amazon CloudWatch alarm.

- C. Use Amazon EC2 Auto Scaling with simple scaling policies to scale when ECS metric breaches trigger an Amazon CloudWatch alarm.

- D. Use AWS Application Auto Scaling with target tracking policies to scale when ECS metric breaches trigger an Amazon CloudWatch alarm.

**Correct:** D
**Why:** Use Application Auto Scaling target tracking for ECS on Fargate based on CPU/memory metrics.

**Incorrect:**
- A: EC2 Auto Scaling not applicable to Fargate tasks.
- B: Lambda intermediates are unnecessary.
- C: EC2 Auto Scaling not applicable to Fargate tasks.


---

---

### Question #304

A company recently created a disaster recovery site in a different AWS Region. The company needs to transfer large amounts of data back and forth between NFS file systems in the two Regions on a periodic basis. Which solution will meet these requirements with the LEAST operational overhead?

- A. Use AWS DataSync.

- B. Use AWS Snowball devices.

- C. Set up an SFTP server on Amazon EC2.

- D. Use AWS Database Migration Service (AWS DMS).

**Correct:** A
**Why:** DataSync moves data between NFS file systems across Regions with minimal ops and scheduling.

**Incorrect:**
- B: Snowball/SFTP/DMS don’t fit ongoing NFS replication.
- C: Snowball/SFTP/DMS don’t fit ongoing NFS replication.
- D: Snowball/SFTP/DMS don’t fit ongoing NFS replication.


---

---

### Question #305

A company is designing a shared storage solution for a gaming application that is hosted in the AWS Cloud. The company needs the ability to use SMB clients to access data. The solution must be fully managed. Which AWS solution meets these requirements?

- A. Create an AWS DataSync task that shares the data as a mountable file system. Mount the file system to the application server.

- B. Create an Amazon EC2 Windows instance. Install and configure a Windows file share role on the instance. Connect the application server to the file share.

- C. Create an Amazon FSx for Windows File Server file system. Attach the file system to the origin server. Connect the application server to the file system.

- D. Create an Amazon S3 bucket. Assign an IAM role to the application to grant access to the S3 bucket. Mount the S3 bucket to the application server.

**Correct:** C
**Why:** FSx for Windows File Server provides managed SMB file shares.

**Incorrect:**
- A: DataSync/Tape/Direct S3 are not SMB file servers.
- B: DataSync/Tape/Direct S3 are not SMB file servers.
- D: DataSync/Tape/Direct S3 are not SMB file servers.


---

---

### Question #306

A company wants to run an in-memory database for a latency-sensitive application that runs on Amazon EC2 instances. The application processes more than 100,000 transactions each minute and requires high network throughput. A solutions architect needs to provide a cost- effective network design that minimizes data transfer charges. Which solution meets these requirements?

- A. Launch all EC2 instances in the same Availability Zone within the same AWS Region. Specify a placement group with cluster strategy when launching EC2 instances.

- B. Launch all EC2 instances in different Availability Zones within the same AWS Region. Specify a placement group with partition strategy when launching EC2 instances.

- C. Deploy an Auto Scaling group to launch EC2 instances in different Availability Zones based on a network utilization target.

- D. Deploy an Auto Scaling group with a step scaling policy to launch EC2 instances in different Availability Zones.

**Correct:** A
**Why:** Cluster placement group in one AZ maximizes east‑west network throughput/low latency, minimizing data transfer charges.

**Incorrect:**
- B: Spreading across AZs increases latency and inter‑AZ data costs.
- C: Spreading across AZs increases latency and inter‑AZ data costs.
- D: Spreading across AZs increases latency and inter‑AZ data costs.


---

---

### Question #310

A company sells datasets to customers who do research in articial intelligence and machine learning (AI/ML). The datasets are large, formatted files that are stored in an Amazon S3 bucket in the us-east-1 Region. The company hosts a web application that the customers use to purchase access to a given dataset. The web application is deployed on multiple Amazon EC2 instances behind an Application Load Balancer. After a purchase is made, customers receive an S3 signed URL that allows access to the files. The customers are distributed across North America and Europe. The company wants to reduce the cost that is associated with data transfers and wants to maintain or improve performance. What should a solutions architect do to meet these requirements?

- A. Configure S3 Transfer Acceleration on the existing S3 bucket. Direct customer requests to the S3 Transfer Acceleration endpoint. Continue to use S3 signed URLs for access control.

- B. Deploy an Amazon CloudFront distribution with the existing S3 bucket as the origin. Direct customer requests to the CloudFront URL. Switch to CloudFront signed URLs for access control.

- C. Set up a second S3 bucket in the eu-central-1 Region with S3 Cross-Region Replication between the buckets. Direct customer requests to the closest Region. Continue to use S3 signed URLs for access control.

- D. Modify the web application to enable streaming of the datasets to end users. Configure the web application to read the data from the existing S3 bucket. Implement access control directly in the application.

**Correct:** B
**Why:** CloudFront in front of S3 reduces egress cost (regional edge caches/edge), improves performance; use signed URLs for access control.

**Incorrect:**
- A: Transfer Acceleration/CRR/streaming app changes are less optimal or higher ops.
- C: Transfer Acceleration/CRR/streaming app changes are less optimal or higher ops.
- D: Transfer Acceleration/CRR/streaming app changes are less optimal or higher ops.


---

---

### Question #312

A company has an application that runs on several Amazon EC2 instances. Each EC2 instance has multiple Amazon Elastic Block Store (Amazon EBS) data volumes attached to it. The application’s EC2 instance configuration and data need to be backed up nightly. The application also needs to be recoverable in a different AWS Region. Which solution will meet these requirements in the MOST operationally ecient way?

- A. Write an AWS Lambda function that schedules nightly snapshots of the application’s EBS volumes and copies the snapshots to a different Region.

- B. Create a backup plan by using AWS Backup to perform nightly backups. Copy the backups to another Region. Add the application’s EC2 instances as resources.

- C. Create a backup plan by using AWS Backup to perform nightly backups. Copy the backups to another Region. Add the application’s EBS volumes as resources.

- D. Write an AWS Lambda function that schedules nightly snapshots of the application's EBS volumes and copies the snapshots to a different Availability Zone.

**Correct:** B
**Why:** AWS Backup can back up EC2 instances (including EBS) nightly and copy to another Region with minimal ops.

**Incorrect:**
- A: DIY Lambdas or backing up only volumes miss instance config or add ops.
- C: DIY Lambdas or backing up only volumes miss instance config or add ops.
- D: DIY Lambdas or backing up only volumes miss instance config or add ops.


---

---

### Question #315

A company experienced a breach that affected several applications in its on-premises data center. The attacker took advantage of vulnerabilities in the custom applications that were running on the servers. The company is now migrating its applications to run on Amazon EC2 instances. The company wants to implement a solution that actively scans for vulnerabilities on the EC2 instances and sends a report that details the ndings. Which solution will meet these requirements?

- A. Deploy AWS Shield to scan the EC2 instances for vulnerabilities. Create an AWS Lambda function to log any ndings to AWS CloudTrail.

- B. Deploy Amazon Macie and AWS Lambda functions to scan the EC2 instances for vulnerabilities. Log any ndings to AWS CloudTrail.

- C. Turn on Amazon GuardDuty. Deploy the GuardDuty agents to the EC2 instances. Configure an AWS Lambda function to automate the generation and distribution of reports that detail the ndings.

- D. Turn on Amazon Inspector. Deploy the Amazon Inspector agent to the EC2 instances. Configure an AWS Lambda function to automate the generation and distribution of reports that detail the ndings.

**Correct:** D
**Why:** Amazon Inspector scans EC2 for vulnerabilities; use Lambda/automation to generate and distribute reports.

**Incorrect:**
- A: Shield/Macie/GuardDuty are not host vulnerability scanners.
- B: Shield/Macie/GuardDuty are not host vulnerability scanners.
- C: Shield/Macie/GuardDuty are not host vulnerability scanners.


---

---

### Question #316

A company uses an Amazon EC2 instance to run a script to poll for and process messages in an Amazon Simple Queue Service (Amazon SQS) queue. The company wants to reduce operational costs while maintaining its ability to process a growing number of messages that are added to the queue. What should a solutions architect recommend to meet these requirements?

- A. Increase the size of the EC2 instance to process messages faster.

- B. Use Amazon EventBridge to turn off the EC2 instance when the instance is underutilized.

- C. Migrate the script on the EC2 instance to an AWS Lambda function with the appropriate runtime.

- D. Use AWS Systems Manager Run Command to run the script on demand.

**Correct:** C
**Why:** Replace the EC2 poller with a Lambda function subscribed to SQS; scales automatically, lower cost.

**Incorrect:**
- A: Larger instance, turning off, or Run Command do not reduce ops or scale on demand.
- B: Larger instance, turning off, or Run Command do not reduce ops or scale on demand.
- D: Larger instance, turning off, or Run Command do not reduce ops or scale on demand.


---

---

### Question #317

A company uses a legacy application to produce data in CSV format. The legacy application stores the output data in Amazon S3. The company is deploying a new commercial off-the-shelf (COTS) application that can perform complex SQL queries to analyze data that is stored in Amazon Redshift and Amazon S3 only. However, the COTS application cannot process the .csv files that the legacy application produces. The company cannot update the legacy application to produce data in another format. The company needs to implement a solution so that the COTS application can use the data that the legacy application produces. Which solution will meet these requirements with the LEAST operational overhead?

- A. Create an AWS Glue extract, transform, and load (ETL) job that runs on a schedule. Configure the ETL job to process the .csv files and store the processed data in Amazon Redshift.

- B. Develop a Python script that runs on Amazon EC2 instances to convert the .csv files to .sql files. Invoke the Python script on a cron schedule to store the output files in Amazon S3.

- C. Create an AWS Lambda function and an Amazon DynamoDB table. Use an S3 event to invoke the Lambda function. Configure the Lambda function to perform an extract, transform, and load (ETL) job to process the .csv files and store the processed data in the DynamoDB table.

- D. Use Amazon EventBridge to launch an Amazon EMR cluster on a weekly schedule. Configure the EMR cluster to perform an extract, transform, and load (ETL) job to process the .csv files and store the processed data in an Amazon Redshift table.

**Correct:** A
**Why:** Glue ETL job on a schedule to process CSVs and load into Redshift with least ops.

**Incorrect:**
- B: EC2 scripts, DynamoDB, or EMR add ops for the use case.
- C: EC2 scripts, DynamoDB, or EMR add ops for the use case.
- D: EC2 scripts, DynamoDB, or EMR add ops for the use case.


---

---

### Question #318

A company recently migrated its entire IT environment to the AWS Cloud. The company discovers that users are provisioning oversized Amazon EC2 instances and modifying security group rules without using the appropriate change control process. A solutions architect must devise a strategy to track and audit these inventory and configuration changes. Which actions should the solutions architect take to meet these requirements? (Choose two.)

- A. Enable AWS CloudTrail and use it for auditing.

- B. Use data lifecycle policies for the Amazon EC2 instances.

- C. Enable AWS Trusted Advisor and reference the security dashboard.

- D. Enable AWS Cong and create rules for auditing and compliance purposes.

E. Restore previous resource congurations with an AWS CloudFormation template.

**Correct:** A, D
**Why:** CloudTrail audits API changes; AWS Config tracks resource configurations and can enforce rules.

**Incorrect:**
- B: DLM is for EBS; Trusted Advisor is advisory; CloudFormation restore doesn’t audit.
- C: DLM is for EBS; Trusted Advisor is advisory; CloudFormation restore doesn’t audit.
- E: DLM is for EBS; Trusted Advisor is advisory; CloudFormation restore doesn’t audit.


---

---

### Question #319

A company has hundreds of Amazon EC2 Linux-based instances in the AWS Cloud. Systems administrators have used shared SSH keys to manage the instances. After a recent audit, the company’s security team is mandating the removal of all shared keys. A solutions architect must design a solution that provides secure access to the EC2 instances. Which solution will meet this requirement with the LEAST amount of administrative overhead?

- A. Use AWS Systems Manager Session Manager to connect to the EC2 instances.

- B. Use AWS Security Token Service (AWS STS) to generate one-time SSH keys on demand.

- C. Allow shared SSH access to a set of bastion instances. Configure all other instances to allow only SSH access from the bastion instances.

- D. Use an Amazon Cognito custom authorizer to authenticate users. Invoke an AWS Lambda function to generate a temporary SSH key.

**Correct:** A
**Why:** Session Manager provides secure, auditable access without SSH keys or open ports.

**Incorrect:**
- B: STS one‑time keys, bastions, or custom auth add overhead.
- C: STS one‑time keys, bastions, or custom auth add overhead.
- D: STS one‑time keys, bastions, or custom auth add overhead.


---

---

### Question #320

A company is using a eet of Amazon EC2 instances to ingest data from on-premises data sources. The data is in JSON format and ingestion rates can be as high as 1 MB/s. When an EC2 instance is rebooted, the data in-ight is lost. The company’s data science team wants to query ingested data in near-real time. Which solution provides near-real-time data querying that is scalable with minimal data loss?

- A. Publish data to Amazon Kinesis Data Streams, Use Kinesis Data Analytics to query the data.

- B. Publish data to Amazon Kinesis Data Firehose with Amazon Redshift as the destination. Use Amazon Redshift to query the data.

- C. Store ingested data in an EC2 instance store. Publish data to Amazon Kinesis Data Firehose with Amazon S3 as the destination. Use Amazon Athena to query the data.

- D. Store ingested data in an Amazon Elastic Block Store (Amazon EBS) volume. Publish data to Amazon ElastiCache for Redis. Subscribe to the Redis channel to query the data.

**Correct:** A
**Why:** Kinesis Data Streams with Kinesis Data Analytics provides near‑real‑time querying with minimal loss and scalability.

**Incorrect:**
- B: Firehose→Redshift has more latency; instance store/EBS risk data loss; Redis pub/sub isn’t durable.
- C: Firehose→Redshift has more latency; instance store/EBS risk data loss; Redis pub/sub isn’t durable.
- D: Firehose→Redshift has more latency; instance store/EBS risk data loss; Redis pub/sub isn’t durable.


---

---

### Question #323

A company’s facility has badge readers at every entrance throughout the building. When badges are scanned, the readers send a message over HTTPS to indicate who attempted to access that particular entrance. A solutions architect must design a system to process these messages from the sensors. The solution must be highly available, and the results must be made available for the company’s security team to analyze. Which system architecture should the solutions architect recommend?

- A. Launch an Amazon EC2 instance to serve as the HTTPS endpoint and to process the messages. Configure the EC2 instance to save the results to an Amazon S3 bucket.

- B. Create an HTTPS endpoint in Amazon API Gateway. Configure the API Gateway endpoint to invoke an AWS Lambda function to process the messages and save the results to an Amazon DynamoDB table.

- C. Use Amazon Route 53 to direct incoming sensor messages to an AWS Lambda function. Configure the Lambda function to process the messages and save the results to an Amazon DynamoDB table.

- D. Create a gateway VPC endpoint for Amazon S3. Configure a Site-to-Site VPN connection from the facility network to the VPC so that sensor data can be written directly to an S3 bucket by way of the VPC endpoint.

**Correct:** B
**Why:** API Gateway HTTPS endpoint invoking Lambda, storing to DynamoDB, is highly available and serverless.

**Incorrect:**
- A: EC2 endpoint or direct S3 via VPN increases ops/complexity.
- C: Route 53 cannot directly invoke code.
- D: EC2 endpoint or direct S3 via VPN increases ops/complexity.


---

---

### Question #324

A company wants to implement a disaster recovery plan for its primary on-premises file storage volume. The file storage volume is mounted from an Internet Small Computer Systems Interface (iSCSI) device on a local storage server. The file storage volume holds hundreds of terabytes (TB) of data. The company wants to ensure that end users retain immediate access to all file types from the on-premises systems without experiencing latency. Which solution will meet these requirements with the LEAST amount of change to the company's existing infrastructure?

- A. Provision an Amazon S3 File Gateway as a virtual machine (VM) that is hosted on premises. Set the local cache to 10 TB. Modify existing applications to access the files through the NFS protocol. To recover from a disaster, provision an Amazon EC2 instance and mount the S3 bucket that contains the files.

- B. Provision an AWS Storage Gateway tape gateway. Use a data backup solution to back up all existing data to a virtual tape library. Configure the data backup solution to run nightly after the initial backup is complete. To recover from a disaster, provision an Amazon EC2 instance and restore the data to an Amazon Elastic Block Store (Amazon EBS) volume from the volumes in the virtual tape library.

- C. Provision an AWS Storage Gateway Volume Gateway cached volume. Set the local cache to 10 TB. Mount the Volume Gateway cached volume to the existing file server by using iSCSI, and copy all files to the storage volume. Configure scheduled snapshots of the storage volume. To recover from a disaster, restore a snapshot to an Amazon Elastic Block Store (Amazon EBS) volume and attach the EBS volume to an Amazon EC2 instance.

- D. Provision an AWS Storage Gateway Volume Gateway stored volume with the same amount of disk space as the existing file storage volume. Mount the Volume Gateway stored volume to the existing file server by using iSCSI, and copy all files to the storage volume. Configure scheduled snapshots of the storage volume. To recover from a disaster, restore a snapshot to an Amazon Elastic Block Store (Amazon EBS) volume and attach the EBS volume to an Amazon EC2 instance.

**Correct:** D
**Why:** Stored volumes keep the full dataset on‑prem for immediate access while asynchronously replicating to AWS for DR.

**Incorrect:**
- A: S3 File Gateway changes protocols and keeps only cached data.
- B: Tape Gateway is for backups only.
- C: Cached volumes keep only recent data local, not the full dataset.


---

---

### Question #327

A solutions architect must secure a VPC network that hosts Amazon EC2 instances. The EC2 instances contain highly sensitive data and run in a private subnet. According to company policy, the EC2 instances that run in the VPC can access only approved third-party software repositories on the internet for software product updates that use the third party’s URL. Other internet trac must be blocked. Which solution meets these requirements?

- A. Update the route table for the private subnet to route the outbound trac to an AWS Network Firewall firewall. Configure domain list rule groups.

- B. Set up an AWS WAF web ACL. Create a custom set of rules that filter trac requests based on source and destination IP address range sets.

- C. Implement strict inbound security group rules. Configure an outbound rule that allows trac only to the authorized software repositories on the internet by specifying the URLs.

- D. Configure an Application Load Balancer (ALB) in front of the EC2 instances. Direct all outbound trac to the ALB. Use a URL-based rule listener in the ALB’s target group for outbound access to the internet.

**Correct:** A
**Why:** AWS Network Firewall with domain list rules provides egress filtering to approved repositories from private subnets.

**Incorrect:**
- B: WAF is for HTTP(S) to apps; SGs cannot filter by domain/URL; ALB is inbound.
- C: WAF is for HTTP(S) to apps; SGs cannot filter by domain/URL; ALB is inbound.
- D: WAF is for HTTP(S) to apps; SGs cannot filter by domain/URL; ALB is inbound.


---

---

### Question #328

A company is hosting a three-tier ecommerce application in the AWS Cloud. The company hosts the website on Amazon S3 and integrates the website with an API that handles sales requests. The company hosts the API on three Amazon EC2 instances behind an Application Load Balancer (ALB). The API consists of static and dynamic front-end content along with backend workers that process sales requests asynchronously. The company is expecting a signicant and sudden increase in the number of sales requests during events for the launch of new products. What should a solutions architect recommend to ensure that all the requests are processed successfully?

- A. Add an Amazon CloudFront distribution for the dynamic content. Increase the number of EC2 instances to handle the increase in trac.

- B. Add an Amazon CloudFront distribution for the static content. Place the EC2 instances in an Auto Scaling group to launch new instances based on network trac.

- C. Add an Amazon CloudFront distribution for the dynamic content. Add an Amazon ElastiCache instance in front of the ALB to reduce trac for the API to handle.

- D. Add an Amazon CloudFront distribution for the static content. Add an Amazon Simple Queue Service (Amazon SQS) queue to receive requests from the website for later processing by the EC2 instances.

**Correct:** D
**Why:** Cache static content with CloudFront and use SQS to buffer backend requests to ensure all are processed.

**Incorrect:**
- A: Dynamic CloudFront or ElastiCache alone won’t handle sudden async workload spikes.
- B: Without queueing, backend can still be overwhelmed.
- C: Dynamic CloudFront or ElastiCache alone won’t handle sudden async workload spikes.


---

---

### Question #329

A security audit reveals that Amazon EC2 instances are not being patched regularly. A solutions architect needs to provide a solution that will run regular security scans across a large eet of EC2 instances. The solution should also patch the EC2 instances on a regular schedule and provide a report of each instance’s patch status. Which solution will meet these requirements?

- A. Set up Amazon Macie to scan the EC2 instances for software vulnerabilities. Set up a cron job on each EC2 instance to patch the instance on a regular schedule.

- B. Turn on Amazon GuardDuty in the account. Configure GuardDuty to scan the EC2 instances for software vulnerabilities. Set up AWS Systems Manager Session Manager to patch the EC2 instances on a regular schedule.

- C. Set up Amazon Detective to scan the EC2 instances for software vulnerabilities. Set up an Amazon EventBridge scheduled rule to patch the EC2 instances on a regular schedule.

- D. Turn on Amazon Inspector in the account. Configure Amazon Inspector to scan the EC2 instances for software vulnerabilities. Set up AWS Systems Manager Patch Manager to patch the EC2 instances on a regular schedule.

**Correct:** D
**Why:** Inspector for vulnerability scanning and Systems Manager Patch Manager for scheduled patching with compliance reports.

**Incorrect:**
- A: Macie/GuardDuty/Detective do not patch or scan OS packages.
- B: Macie/GuardDuty/Detective do not patch or scan OS packages.
- C: Macie/GuardDuty/Detective do not patch or scan OS packages.


---

---

### Question #332

A company needs to provide its employees with secure access to condential and sensitive files. The company wants to ensure that the files can be accessed only by authorized users. The files must be downloaded securely to the employees’ devices. The files are stored in an on-premises Windows file server. However, due to an increase in remote usage, the file server is running out of capacity. . Which solution will meet these requirements?

- A. Migrate the file server to an Amazon EC2 instance in a public subnet. Configure the security group to limit inbound trac to the employees’ IP addresses.

- B. Migrate the files to an Amazon FSx for Windows File Server file system. Integrate the Amazon FSx file system with the on-premises Active Directory. Configure AWS Client VPN.

- C. Migrate the files to Amazon S3, and create a private VPC endpoint. Create a signed URL to allow download.

- D. Migrate the files to Amazon S3, and create a public VPC endpoint. Allow employees to sign on with AWS IAM Identity Center (AWS Single Sign-On).

**Correct:** B
**Why:** FSx for Windows with on‑prem AD integration preserves permissions; Client VPN provides secure remote access and downloads.

**Incorrect:**
- A: Public EC2, S3 signed URLs, or public endpoints don’t integrate with AD or meet security requirements.
- C: Public EC2, S3 signed URLs, or public endpoints don’t integrate with AD or meet security requirements.
- D: Public EC2, S3 signed URLs, or public endpoints don’t integrate with AD or meet security requirements.


---

---

### Question #333

A company’s application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. On the first day of every month at midnight, the application becomes much slower when the month-end nancial calculation batch runs. This causes the CPU utilization of the EC2 instances to immediately peak to 100%, which disrupts the application. What should a solutions architect recommend to ensure the application is able to handle the workload and avoid downtime?

- A. Configure an Amazon CloudFront distribution in front of the ALB.

- B. Configure an EC2 Auto Scaling simple scaling policy based on CPU utilization.

- C. Configure an EC2 Auto Scaling scheduled scaling policy based on the monthly schedule.

- D. Configure Amazon ElastiCache to remove some of the workload from the EC2 instances.

**Correct:** C
**Why:** Scheduled scaling handles known monthly spikes at midnight without downtime.

**Incorrect:**
- A: CDN or reactive scaling alone may be too late; caching doesn’t fix CPU‑bound computation.
- B: CDN or reactive scaling alone may be too late; caching doesn’t fix CPU‑bound computation.
- D: CDN or reactive scaling alone may be too late; caching doesn’t fix CPU‑bound computation.


---

---

### Question #334

A company wants to give a customer the ability to use on-premises Microsoft Active Directory to download files that are stored in Amazon S3. The customer’s application uses an SFTP client to download the files. Which solution will meet these requirements with the LEAST operational overhead and no changes to the customer’s application?

- A. Set up AWS Transfer Family with SFTP for Amazon S3. Configure integrated Active Directory authentication.

- B. Set up AWS Database Migration Service (AWS DMS) to synchronize the on-premises client with Amazon S3. Configure integrated Active Directory authentication.

- C. Set up AWS DataSync to synchronize between the on-premises location and the S3 location by using AWS IAM Identity Center (AWS Single Sign-On).

- D. Set up a Windows Amazon EC2 instance with SFTP to connect the on-premises client with Amazon S3. Integrate AWS Identity and Access Management (IAM).

**Correct:** A
**Why:** AWS Transfer Family SFTP for S3 with AD auth meets two‑way SFTP needs without app changes.

**Incorrect:**
- B: DMS/DataSync/IAM‑backed EC2 SFTP add ops or don’t integrate AD SFTP properly.
- C: DMS/DataSync/IAM‑backed EC2 SFTP add ops or don’t integrate AD SFTP properly.
- D: DMS/DataSync/IAM‑backed EC2 SFTP add ops or don’t integrate AD SFTP properly.


---

---

### Question #335

A company is experiencing sudden increases in demand. The company needs to provision large Amazon EC2 instances from an Amazon Machine Image (AMI). The instances will run in an Auto Scaling group. The company needs a solution that provides minimum initialization latency to meet the demand. Which solution meets these requirements?

- A. Use the aws ec2 register-image command to create an AMI from a snapshot. Use AWS Step Functions to replace the AMI in the Auto Scaling group.

- B. Enable Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot. Provision an AMI by using the snapshot. Replace the AMI in the Auto Scaling group with the new AMI.

- C. Enable AMI creation and dene lifecycle rules in Amazon Data Lifecycle Manager (Amazon DLM). Create an AWS Lambda function that modies the AMI in the Auto Scaling group.

- D. Use Amazon EventBridge to invoke AWS Backup lifecycle policies that provision AMIs. Configure Auto Scaling group capacity limits as an event source in EventBridge.

**Correct:** B
**Why:** Enable EBS Fast Snapshot Restore on the AMI’s snapshot to minimize initialization latency during scale‑out.

**Incorrect:**
- A: Register‑image/DLM/Backup events don’t directly reduce restore latency.
- C: Register‑image/DLM/Backup events don’t directly reduce restore latency.
- D: Register‑image/DLM/Backup events don’t directly reduce restore latency.


---

---

### Question #336

A company hosts a multi-tier web application that uses an Amazon Aurora MySQL DB cluster for storage. The application tier is hosted on Amazon EC2 instances. The company’s IT security guidelines mandate that the database credentials be encrypted and rotated every 14 days. What should a solutions architect do to meet this requirement with the LEAST operational effort?

- A. Create a new AWS Key Management Service (AWS KMS) encryption key. Use AWS Secrets Manager to create a new secret that uses the KMS key with the appropriate credentials. Associate the secret with the Aurora DB cluster. Configure a custom rotation period of 14 days.

- B. Create two parameters in AWS Systems Manager Parameter Store: one for the user name as a string parameter and one that uses the SecureString type for the password. Select AWS Key Management Service (AWS KMS) encryption for the password parameter, and load these parameters in the application tier. Implement an AWS Lambda function that rotates the password every 14 days.

- C. Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system in all EC2 instances of the application tier. Restrict the access to the file on the file system so that the application can read the file and that only super users can modify the file. Implement an AWS Lambda function that rotates the key in Aurora every 14 days and writes new credentials into the file.

- D. Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon S3 bucket that the application uses to load the credentials. Download the file to the application regularly to ensure that the correct credentials are used. Implement an AWS Lambda function that rotates the Aurora credentials every 14 days and uploads these credentials to the file in the S3 bucket.

**Correct:** A
**Why:** Secrets Manager with KMS‑encrypted secret and built‑in rotation integrates with Aurora; set 14‑day rotation.

**Incorrect:**
- B: Parameter Store/EFS/S3 solutions require more glue code and ops.
- C: Parameter Store/EFS/S3 solutions require more glue code and ops.
- D: Parameter Store/EFS/S3 solutions require more glue code and ops.


---

---

### Question #337

A company has deployed a web application on AWS. The company hosts the backend database on Amazon RDS for MySQL with a primary DB instance and ve read replicas to support scaling needs. The read replicas must lag no more than 1 second behind the primary DB instance. The database routinely runs scheduled stored procedures. As trac on the website increases, the replicas experience additional lag during periods of peak load. A solutions architect must reduce the replication lag as much as possible. The solutions architect must minimize changes to the application code and must minimize ongoing operational overhead. Which solution will meet these requirements?

- A. Migrate the database to Amazon Aurora MySQL. Replace the read replicas with Aurora Replicas, and configure Aurora Auto Scaling. Replace the stored procedures with Aurora MySQL native functions.

- B. Deploy an Amazon ElastiCache for Redis cluster in front of the database. Modify the application to check the cache before the application queries the database. Replace the stored procedures with AWS Lambda functions.

- C. Migrate the database to a MySQL database that runs on Amazon EC2 instances. Choose large, compute optimized EC2 instances for all replica nodes. Maintain the stored procedures on the EC2 instances.

- D. Migrate the database to Amazon DynamoDB. Provision a large number of read capacity units (RCUs) to support the required throughput, and configure on-demand capacity scaling. Replace the stored procedures with DynamoDB streams.

**Correct:** A
**Why:** Aurora MySQL with Aurora Replicas and Auto Scaling reduces replica lag and operational overhead.

**Incorrect:**
- B: Cache/EC2 self‑managed or DynamoDB require more changes.
- C: Cache/EC2 self‑managed or DynamoDB require more changes.
- D: Cache/EC2 self‑managed or DynamoDB require more changes.


---

---

### Question #340

A media company hosts its website on AWS. The website application’s architecture includes a eet of Amazon EC2 instances behind an Application Load Balancer (ALB) and a database that is hosted on Amazon Aurora. The company’s cybersecurity team reports that the application is vulnerable to SQL injection. How should the company resolve this issue?

- A. Use AWS WAF in front of the ALB. Associate the appropriate web ACLs with AWS WAF.

- B. Create an ALB listener rule to reply to SQL injections with a xed response.

- C. Subscribe to AWS Shield Advanced to block all SQL injection attempts automatically.

- D. Set up Amazon Inspector to block all SQL injection attempts automatically.

**Correct:** A
**Why:** Put WAF in front of ALB and enable managed rules for SQLi to mitigate vulnerabilities quickly.

**Incorrect:**
- B: ALB listener responses or Shield/Inspector don’t address SQLi at the app layer.
- C: ALB listener responses or Shield/Inspector don’t address SQLi at the app layer.
- D: ALB listener responses or Shield/Inspector don’t address SQLi at the app layer.


---

---

### Question #342

A transaction processing company has weekly scripted batch jobs that run on Amazon EC2 instances. The EC2 instances are in an Auto Scaling group. The number of transactions can vary, but the baseline CPU utilization that is noted on each run is at least 60%. The company needs to provision the capacity 30 minutes before the jobs run. Currently, engineers complete this task by manually modifying the Auto Scaling group parameters. The company does not have the resources to analyze the required capacity trends for the Auto Scaling group counts. The company needs an automated way to modify the Auto Scaling group’s desired capacity. Which solution will meet these requirements with the LEAST operational overhead?

- A. Create a dynamic scaling policy for the Auto Scaling group. Configure the policy to scale based on the CPU utilization metric. Set the target value for the metric to 60%.

- B. Create a scheduled scaling policy for the Auto Scaling group. Set the appropriate desired capacity, minimum capacity, and maximum capacity. Set the recurrence to weekly. Set the start time to 30 minutes before the batch jobs run.

- C. Create a predictive scaling policy for the Auto Scaling group. Configure the policy to scale based on forecast. Set the scaling metric to CPU utilization. Set the target value for the metric to 60%. In the policy, set the instances to pre-launch 30 minutes before the jobs run.

- D. Create an Amazon EventBridge event to invoke an AWS Lambda function when the CPU utilization metric value for the Auto Scaling group reaches 60%. Configure the Lambda function to increase the Auto Scaling group’s desired capacity and maximum capacity by 20%.

**Correct:** C
**Why:** Predictive scaling can forecast and pre‑launch capacity 30 minutes before the weekly batch run.

**Incorrect:**
- A: Dynamic/scheduled without forecasting or EventBridge on metric threshold won’t proactively pre‑launch as needed.
- B: Dynamic/scheduled without forecasting or EventBridge on metric threshold won’t proactively pre‑launch as needed.
- D: Dynamic/scheduled without forecasting or EventBridge on metric threshold won’t proactively pre‑launch as needed.


---

---

### Question #343

A solutions architect is designing a company’s disaster recovery (DR) architecture. The company has a MySQL database that runs on an Amazon EC2 instance in a private subnet with scheduled backup. The DR design needs to include multiple AWS Regions. Which solution will meet these requirements with the LEAST operational overhead?

- A. Migrate the MySQL database to multiple EC2 instances. Configure a standby EC2 instance in the DR Region. Turn on replication.

- B. Migrate the MySQL database to Amazon RDS. Use a Multi-AZ deployment. Turn on read replication for the primary DB instance in the different Availability Zones.

- C. Migrate the MySQL database to an Amazon Aurora global database. Host the primary DB cluster in the primary Region. Host the secondary DB cluster in the DR Region.

- D. Store the scheduled backup of the MySQL database in an Amazon S3 bucket that is congured for S3 Cross-Region Replication (CRR). Use the data backup to restore the database in the DR Region.

**Correct:** C
**Why:** Aurora Global Database provides multi‑Region design with low ops and built‑in replication; best for DR across Regions.

**Incorrect:**
- A: EC2 self‑managed, single‑Region RDS Multi‑AZ, or CRR of backups offer higher RTO/RPO or more ops.
- B: EC2 self‑managed, single‑Region RDS Multi‑AZ, or CRR of backups offer higher RTO/RPO or more ops.
- D: EC2 self‑managed, single‑Region RDS Multi‑AZ, or CRR of backups offer higher RTO/RPO or more ops.


---

---

### Question #347

A company has an application that is running on Amazon EC2 instances. A solutions architect has standardized the company on a particular instance family and various instance sizes based on the current needs of the company. The company wants to maximize cost savings for the application over the next 3 years. The company needs to be able to change the instance family and sizes in the next 6 months based on application popularity and usage. Which solution will meet these requirements MOST cost-effectively?

- A. Compute Savings Plan

- B. EC2 Instance Savings Plan

- C. Zonal Reserved Instances

- D. Standard Reserved Instances

**Correct:** A
**Why:** Compute Savings Plans provide 1–3 year savings with flexibility to change instance families/sizes/Regions.

**Incorrect:**
- B: Instance RIs or Zonal RIs are less flexible.
- C: Instance RIs or Zonal RIs are less flexible.
- D: Instance RIs or Zonal RIs are less flexible.


---

---

### Question #351

A company is moving its data management application to AWS. The company wants to transition to an event-driven architecture. The architecture needs to be more distributed and to use serverless concepts while performing the different aspects of the workow. The company also wants to minimize operational overhead. Which solution will meet these requirements?

- A. Build out the workow in AWS Glue. Use AWS Glue to invoke AWS Lambda functions to process the workow steps.

- B. Build out the workow in AWS Step Functions. Deploy the application on Amazon EC2 instances. Use Step Functions to invoke the workow steps on the EC2 instances.

- C. Build out the workow in Amazon EventBridge. Use EventBridge to invoke AWS Lambda functions on a schedule to process the workow steps.

- D. Build out the workow in AWS Step Functions. Use Step Functions to create a state machine. Use the state machine to invoke AWS Lambda functions to process the workow steps.

**Correct:** D
**Why:** Step Functions orchestrates serverless, event-driven workflows with a state machine that invokes Lambda for steps, minimizing ops and providing retries and branching.

**Incorrect:**
- A: AWS Glue is for ETL, not general workflow orchestration; invoking Lambda from Glue adds complexity.
- B: Orchestrating EC2 increases operational overhead and is not serverless.
- C: EventBridge schedules are time-based, not a workflow engine; you still need orchestration and step control.


---

---

### Question #353

A company hosts a three-tier web application on Amazon EC2 instances in a single Availability Zone. The web application uses a self-managed MySQL database that is hosted on an EC2 instance to store data in an Amazon Elastic Block Store (Amazon EBS) volume. The MySQL database currently uses a 1 TB Provisioned IOPS SSD (io2) EBS volume. The company expects trac of 1,000 IOPS for both reads and writes at peak trac. The company wants to minimize any disruptions, stabilize performance, and reduce costs while retaining the capacity for double the IOPS. The company wants to move the database tier to a fully managed solution that is highly available and fault tolerant. Which solution will meet these requirements MOST cost-effectively?

- A. Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with an io2 Block Express EBS volume.

- B. Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with a General Purpose SSD (gp2) EBS volume.

- C. Use Amazon S3 Intelligent-Tiering access tiers.

- D. Use two large EC2 instances to host the database in active-passive mode.

**Correct:** B
**Why:** Multi-AZ Amazon RDS for MySQL with General Purpose SSD (gp2) provides stable, managed, highly available performance and is cost‑effective for ~1–2K IOPS needs.

**Incorrect:**
- A: io2 Block Express is not a typical/necessary choice for RDS here and is more expensive.
- C: S3 Intelligent‑Tiering is unrelated to database storage.
- D: Self-managed EC2 databases increase ops and reduce availability compared to RDS.


---

---

### Question #355

A company is migrating an old application to AWS. The application runs a batch job every hour and is CPU intensive. The batch job takes 15 minutes on average with an on-premises server. The server has 64 virtual CPU (vCPU) and 512 GiB of memory. Which solution will run the batch job within 15 minutes with the LEAST operational overhead?

- A. Use AWS Lambda with functional scaling.

- B. Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate.

- C. Use Amazon Lightsail with AWS Auto Scaling.

- D. Use AWS Batch on Amazon EC2.

**Correct:** D
**Why:** AWS Batch on EC2 efficiently runs scheduled, CPU‑intensive batch jobs with needed large vCPU/memory shapes and minimal ops.

**Incorrect:**
- A: Lambda is not suitable for long, heavy jobs and has runtime/size limits.
- B: Fargate max vCPU/memory may not meet 64 vCPU/512 GiB needs and costs more.
- C: Lightsail is not appropriate for scalable batch.


---

---

### Question #357

A gaming company is moving its public scoreboard from a data center to the AWS Cloud. The company uses Amazon EC2 Windows Server instances behind an Application Load Balancer to host its dynamic application. The company needs a highly available storage solution for the application. The application consists of static files and dynamic server-side code. Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)

- A. Store the static files on Amazon S3. Use Amazon CloudFront to cache objects at the edge.

- B. Store the static files on Amazon S3. Use Amazon ElastiCache to cache objects at the edge.

- C. Store the server-side code on Amazon Elastic File System (Amazon EFS). Mount the EFS volume on each EC2 instance to share the files.

- D. Store the server-side code on Amazon FSx for Windows File Server. Mount the FSx for Windows File Server volume on each EC2 instance to share the files.

E. Store the server-side code on a General Purpose SSD (gp2) Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume on each EC2 instance to share the files.

**Correct:** A, D
**Why:** Store static files in S3 and cache with CloudFront; use FSx for Windows File Server to share server‑side code across Windows EC2 instances.

**Incorrect:**
- B: ElastiCache is not an edge CDN for static files.
- C: EFS is POSIX/NFS (Linux), not ideal for Windows.
- E: EBS volumes are single‑instance; not shareable across instances.


---

---

### Question #358

A social media company runs its application on Amazon EC2 instances behind an Application Load Balancer (ALB). The ALB is the origin for an Amazon CloudFront distribution. The application has more than a billion images stored in an Amazon S3 bucket and processes thousands of images each second. The company wants to resize the images dynamically and serve appropriate formats to clients. Which solution will meet these requirements with the LEAST operational overhead?

- A. Install an external image management library on an EC2 instance. Use the image management library to process the images.

- B. Create a CloudFront origin request policy. Use the policy to automatically resize images and to serve the appropriate format based on the User-Agent HTTP header in the request.

- C. Use a Lambda@Edge function with an external image management library. Associate the Lambda@Edge function with the CloudFront behaviors that serve the images.

- D. Create a CloudFront response headers policy. Use the policy to automatically resize images and to serve the appropriate format based on the User-Agent HTTP header in the request.

**Correct:** C
**Why:** Lambda@Edge can dynamically transform/resize images at the edge with minimal ops and latency.

**Incorrect:**
- A: Managing EC2 image service increases ops and scale burden.
- B: CloudFront policies/headers cannot perform image processing by themselves.
- D: CloudFront policies/headers cannot perform image processing by themselves.


---

---

### Question #369

A company has migrated an application to Amazon EC2 Linux instances. One of these EC2 instances runs several 1-hour tasks on a schedule. These tasks were written by different teams and have no common programming language. The company is concerned about performance and scalability while these tasks run on a single instance. A solutions architect needs to implement a solution to resolve these concerns. Which solution will meet these requirements with the LEAST operational overhead?

- A. Use AWS Batch to run the tasks as jobs. Schedule the jobs by using Amazon EventBridge (Amazon CloudWatch Events).

- B. Convert the EC2 instance to a container. Use AWS App Runner to create the container on demand to run the tasks as jobs.

- C. Copy the tasks into AWS Lambda functions. Schedule the Lambda functions by using Amazon EventBridge (Amazon CloudWatch Events).

- D. Create an Amazon Machine Image (AMI) of the EC2 instance that runs the tasks. Create an Auto Scaling group with the AMI to run multiple copies of the instance.

**Correct:** A
**Why:** AWS Batch runs heterogeneous jobs (any language) as managed jobs and can be scheduled with EventBridge, improving scalability with minimal ops.

**Incorrect:**
- B: App Runner targets web services/containers, not scheduled arbitrary jobs as simply.
- C: Rewriting tasks into Lambda functions is heavy and may hit limits.
- D: Cloning instances increases cost/ops and doesn’t address scheduling/queuing.


---

---

### Question #370

A company runs a public three-tier web application in a VPC. The application runs on Amazon EC2 instances across multiple Availability Zones. The EC2 instances that run in private subnets need to communicate with a license server over the internet. The company needs a managed solution that minimizes operational maintenance. Which solution meets these requirements?

- A. Provision a NAT instance in a public subnet. Modify each private subnet's route table with a default route that points to the NAT instance.

- B. Provision a NAT instance in a private subnet. Modify each private subnet's route table with a default route that points to the NAT instance.

- C. Provision a NAT gateway in a public subnet. Modify each private subnet's route table with a default route that points to the NAT gateway.

- D. Provision a NAT gateway in a private subnet. Modify each private subnet's route table with a default route that points to the NAT gateway.

**Correct:** C
**Why:** NAT gateway in a public subnet is the managed solution for outbound internet from private subnets with minimal ops.

**Incorrect:**
- A: NAT instance adds ops; NAT gateway must be in a public subnet.
- B: NAT instance adds ops; NAT gateway must be in a public subnet.
- D: NAT instance adds ops; NAT gateway must be in a public subnet.


---

---

### Question #375

An ecommerce company is building a distributed application that involves several serverless functions and AWS services to complete order- processing tasks. These tasks require manual approvals as part of the workow. A solutions architect needs to design an architecture for the order-processing application. The solution must be able to combine multiple AWS Lambda functions into responsive serverless applications. The solution also must orchestrate data and services that run on Amazon EC2 instances, containers, or on-premises servers. Which solution will meet these requirements with the LEAST operational overhead?

- A. Use AWS Step Functions to build the application.

- B. Integrate all the application components in an AWS Glue job.

- C. Use Amazon Simple Queue Service (Amazon SQS) to build the application.

- D. Use AWS Lambda functions and Amazon EventBridge events to build the application.

**Correct:** A
**Why:** Step Functions orchestrates serverless and non‑serverless tasks, supports human approval steps, and simplifies complex workflows.

**Incorrect:**
- B: AWS Glue is for data ETL, not general orchestration with approvals.
- C: SQS/EventBridge alone do not provide orchestration/state management.
- D: SQS/EventBridge alone do not provide orchestration/state management.


---

---

### Question #377

A company recently deployed a new auditing system to centralize information about operating system versions, patching, and installed software for Amazon EC2 instances. A solutions architect must ensure all instances provisioned through EC2 Auto Scaling groups successfully send reports to the auditing system as soon as they are launched and terminated. Which solution achieves these goals MOST eciently?

- A. Use a scheduled AWS Lambda function and run a script remotely on all EC2 instances to send data to the audit system.

- B. Use EC2 Auto Scaling lifecycle hooks to run a custom script to send data to the audit system when instances are launched and terminated.

- C. Use an EC2 Auto Scaling launch configuration to run a custom script through user data to send data to the audit system when instances are launched and terminated.

- D. Run a custom script on the instance operating system to send data to the audit system. Configure the script to be invoked by the EC2 Auto Scaling group when the instance starts and is terminated.

**Correct:** B
**Why:** EC2 Auto Scaling lifecycle hooks trigger custom scripts on launch/terminate so instances report to the audit system reliably.

**Incorrect:**
- A: Running remote scripts on all instances is brittle and inefficient.
- C: Launch configuration user data runs only on launch, not termination.
- D: Auto Scaling cannot invoke scripts inside the instance without lifecycle hooks.


---

---

### Question #378

A company is developing a real-time multiplayer game that uses UDP for communications between the client and servers in an Auto Scaling group. Spikes in demand are anticipated during the day, so the game server platform must adapt accordingly. Developers want to store gamer scores and other non-relational data in a database solution that will scale without intervention. Which solution should a solutions architect recommend?

- A. Use Amazon Route 53 for trac distribution and Amazon Aurora Serverless for data storage.

- B. Use a Network Load Balancer for trac distribution and Amazon DynamoDB on-demand for data storage.

- C. Use a Network Load Balancer for trac distribution and Amazon Aurora Global Database for data storage.

- D. Use an Application Load Balancer for trac distribution and Amazon DynamoDB global tables for data storage.

**Correct:** B
**Why:** NLB supports UDP for game traffic; DynamoDB on‑demand scales automatically for storing scores/non‑relational data.

**Incorrect:**
- A: Aurora is relational and requires capacity planning.
- C: Aurora is relational and requires capacity planning.
- D: ALB does not support UDP.


---

---

### Question #380

A company is migrating its on-premises workload to the AWS Cloud. The company already uses several Amazon EC2 instances and Amazon RDS DB instances. The company wants a solution that automatically starts and stops the EC2 instances and DB instances outside of business hours. The solution must minimize cost and infrastructure maintenance. Which solution will meet these requirements?

- A. Scale the EC2 instances by using elastic resize. Scale the DB instances to zero outside of business hours.

- B. Explore AWS Marketplace for partner solutions that will automatically start and stop the EC2 instances and DB instances on a schedule.

- C. Launch another EC2 instance. Configure a crontab schedule to run shell scripts that will start and stop the existing EC2 instances and DB instances on a schedule.

- D. Create an AWS Lambda function that will start and stop the EC2 instances and DB instances. Configure Amazon EventBridge to invoke the Lambda function on a schedule.

**Correct:** D
**Why:** An EventBridge‑scheduled Lambda can start/stop EC2 and RDS on a schedule with minimal cost and ops.

**Incorrect:**
- A: EC2 “elastic resize” and RDS scale‑to‑zero do not apply.
- B: Marketplace adds cost/complexity.
- C: Managing a cron EC2 instance adds ops.


---

---

### Question #382

A company has a three-tier application on AWS that ingests sensor data from its users’ devices. The trac ows through a Network Load Balancer (NLB), then to Amazon EC2 instances for the web tier, and nally to EC2 instances for the application tier. The application tier makes calls to a database. What should a solutions architect do to improve the security of the data in transit?

- A. Configure a TLS listener. Deploy the server certicate on the NLB.

- B. Configure AWS Shield Advanced. Enable AWS WAF on the NLB.

- C. Change the load balancer to an Application Load Balancer (ALB). Enable AWS WAF on the ALB.

- D. Encrypt the Amazon Elastic Block Store (Amazon EBS) volume on the EC2 instances by using AWS Key Management Service (AWS KMS).

**Correct:** A
**Why:** Add a TLS listener and deploy certs on the NLB to encrypt data in transit from clients to the load balancer and to targets as needed.

**Incorrect:**
- B: Shield/WAF protect against attacks but don’t encrypt traffic.
- C: ALB is fine for HTTP/HTTPS but switching is unnecessary; TLS on NLB meets the need.
- D: EBS encryption is at rest, not in transit.


---

---

### Question #383

A company is planning to migrate a commercial off-the-shelf application from its on-premises data center to AWS. The software has a software licensing model using sockets and cores with predictable capacity and uptime requirements. The company wants to use its existing licenses, which were purchased earlier this year. Which Amazon EC2 pricing option is the MOST cost-effective?

- A. Dedicated Reserved Hosts

- B. Dedicated On-Demand Hosts

- C. Dedicated Reserved Instances

- D. Dedicated On-Demand Instances

**Correct:** A
**Why:** Dedicated Reserved Hosts allow BYOL tied to sockets/cores and provide the most cost‑effective pricing for predictable capacity.

**Incorrect:**
- B: On‑Demand is more expensive for steady usage.
- C: Reserved Instances don’t satisfy host‑level licensing requirements.
- D: On‑Demand is more expensive for steady usage.


---

---

### Question #384

A company runs an application on Amazon EC2 Linux instances across multiple Availability Zones. The application needs a storage layer that is highly available and Portable Operating System Interface (POSIX)-compliant. The storage layer must provide maximum data durability and must be shareable across the EC2 instances. The data in the storage layer will be accessed frequently for the first 30 days and will be accessed infrequently after that time. Which solution will meet these requirements MOST cost-effectively?

- A. Use the Amazon S3 Standard storage class. Create an S3 Lifecycle policy to move infrequently accessed data to S3 Glacier.

- B. Use the Amazon S3 Standard storage class. Create an S3 Lifecycle policy to move infrequently accessed data to S3 Standard-Infrequent Access (S3 Standard-IA).

- C. Use the Amazon Elastic File System (Amazon EFS) Standard storage class. Create a lifecycle management policy to move infrequently accessed data to EFS Standard-Infrequent Access (EFS Standard-IA).

- D. Use the Amazon Elastic File System (Amazon EFS) One Zone storage class. Create a lifecycle management policy to move infrequently accessed data to EFS One Zone-Infrequent Access (EFS One Zone-IA).

**Correct:** C
**Why:** EFS provides shared, POSIX‑compliant, highly available storage; lifecycle moves infrequently accessed data to EFS Standard‑IA to save cost.

**Incorrect:**
- A: S3 is not POSIX‑compliant and not a shared filesystem for EC2.
- B: S3 is not POSIX‑compliant and not a shared filesystem for EC2.
- D: One Zone is not as highly available as required.


---

---

### Question #386

An ecommerce company is running a multi-tier application on AWS. The front-end and backend tiers both run on Amazon EC2, and the database runs on Amazon RDS for MySQL. The backend tier communicates with the RDS instance. There are frequent calls to return identical datasets from the database that are causing performance slowdowns. Which action should be taken to improve the performance of the backend?

- A. Implement Amazon SNS to store the database calls.

- B. Implement Amazon ElastiCache to cache the large datasets.

- C. Implement an RDS for MySQL read replica to cache database calls.

- D. Implement Amazon Kinesis Data Firehose to stream the calls to the database.

**Correct:** B
**Why:** ElastiCache (Redis/Memcached) caches repeated reads to reduce database load and improve performance.

**Incorrect:**
- A: SNS is a pub/sub service, not a cache.
- C: Read replicas help, but cache is better for repetitive identical datasets and lower latency.
- D: Kinesis Firehose is for data streaming, not query caching.


---

---

### Question #388

A company is deploying a two-tier web application in a VPC. The web tier is using an Amazon EC2 Auto Scaling group with public subnets that span multiple Availability Zones. The database tier consists of an Amazon RDS for MySQL DB instance in separate private subnets. The web tier requires access to the database to retrieve product information. The web application is not working as intended. The web application reports that it cannot connect to the database. The database is conrmed to be up and running. All congurations for the network ACLs, security groups, and route tables are still in their default states. What should a solutions architect recommend to x the application?

- A. Add an explicit rule to the private subnet’s network ACL to allow trac from the web tier’s EC2 instances.

- B. Add a route in the VPC route table to allow trac between the web tier’s EC2 instances and the database tier.

- C. Deploy the web tier's EC2 instances and the database tier’s RDS instance into two separate VPCs, and configure VPC peering.

- D. Add an inbound rule to the security group of the database tier’s RDS instance to allow trac from the web tiers security group.

**Correct:** D
**Why:** By default, RDS SG needs an inbound rule from the web tier SG; this enables connectivity while keeping least privilege.

**Incorrect:**
- A: Default NACLs/route tables already allow VPC internal traffic appropriately.
- B: Default NACLs/route tables already allow VPC internal traffic appropriately.
- C: VPC peering is unnecessary for a single VPC.


---

---

### Question #390

A company hosts a three-tier ecommerce application on a eet of Amazon EC2 instances. The instances run in an Auto Scaling group behind an Application Load Balancer (ALB). All ecommerce data is stored in an Amazon RDS for MariaDB Multi-AZ DB instance. The company wants to optimize customer session management during transactions. The application must store session data durably. Which solutions will meet these requirements? (Choose two.)

- A. Turn on the sticky sessions feature (session anity) on the ALB.

- B. Use an Amazon DynamoDB table to store customer session information.

- C. Deploy an Amazon Cognito user pool to manage user session information.

- D. Deploy an Amazon ElastiCache for Redis cluster to store customer session information.

E. Use AWS Systems Manager Application Manager in the application to manage user session information.

**Correct:** A, B
**Why:** Sticky sessions reduce cross‑instance churn during transactions; store session data durably in DynamoDB to survive instance failure.

**Incorrect:**
- C: Cognito manages auth, not app transaction sessions.
- D: ElastiCache Redis alone is not durable.
- E: Systems Manager Application Manager is not for session storage.


---

---

### Question #391

A company needs a backup strategy for its three-tier stateless web application. The web application runs on Amazon EC2 instances in an Auto Scaling group with a dynamic scaling policy that is congured to respond to scaling events. The database tier runs on Amazon RDS for PostgreSQL. The web application does not require temporary local storage on the EC2 instances. The company’s recovery point objective (RPO) is 2 hours. The backup strategy must maximize scalability and optimize resource utilization for this environment. Which solution will meet these requirements?

- A. Take snapshots of Amazon Elastic Block Store (Amazon EBS) volumes of the EC2 instances and database every 2 hours to meet the RPO.

- B. Configure a snapshot lifecycle policy to take Amazon Elastic Block Store (Amazon EBS) snapshots. Enable automated backups in Amazon RDS to meet the RPO.

- C. Retain the latest Amazon Machine Images (AMIs) of the web and application tiers. Enable automated backups in Amazon RDS and use point-in-time recovery to meet the RPO.

- D. Take snapshots of Amazon Elastic Block Store (Amazon EBS) volumes of the EC2 instances every 2 hours. Enable automated backups in Amazon RDS and use point-in-time recovery to meet the RPO.

**Correct:** C
**Why:** Keep latest AMIs for stateless tiers; enable RDS automated backups with PITR to meet a 2‑hour RPO without per‑instance EBS snapshots.

**Incorrect:**
- A: EC2 EBS snapshots are unnecessary for stateless instances.
- B: AMIs are more appropriate than frequent EBS snapshots for autoscaled stateless tiers.
- D: EC2 EBS snapshots are unnecessary for stateless instances.


---

---

### Question #392

A company wants to deploy a new public web application on AWS. The application includes a web server tier that uses Amazon EC2 instances. The application also includes a database tier that uses an Amazon RDS for MySQL DB instance. The application must be secure and accessible for global customers that have dynamic IP addresses. How should a solutions architect configure the security groups to meet these requirements?

- A. Configure the security group for the web servers to allow inbound trac on port 443 from 0.0.0.0/0. Configure the security group for the DB instance to allow inbound trac on port 3306 from the security group of the web servers.

- B. Configure the security group for the web servers to allow inbound trac on port 443 from the IP addresses of the customers. Configure the security group for the DB instance to allow inbound trac on port 3306 from the security group of the web servers.

- C. Configure the security group for the web servers to allow inbound trac on port 443 from the IP addresses of the customers. Configure the security group for the DB instance to allow inbound trac on port 3306 from the IP addresses of the customers.

- D. Configure the security group for the web servers to allow inbound trac on port 443 from 0.0.0.0/0. Configure the security group for the DB instance to allow inbound trac on port 3306 from 0.0.0.0/0.

**Correct:** A
**Why:** Allow HTTPS from anywhere for global access; restrict DB access to only the web tier SG on 3306.

**Incorrect:**
- B: Customer IPs are dynamic; not maintainable.
- C: Customer IPs are dynamic; not maintainable.
- D: Opening DB to the world is insecure.


---

---

### Question #394

A company is running a multi-tier ecommerce web application in the AWS Cloud. The application runs on Amazon EC2 instances with an Amazon RDS for MySQL Multi-AZ DB instance. Amazon RDS is congured with the latest generation DB instance with 2,000 GB of storage in a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume. The database performance affects the application during periods of high demand. A database administrator analyzes the logs in Amazon CloudWatch Logs and discovers that the application performance always degrades when the number of read and write IOPS is higher than 20,000. What should a solutions architect do to improve the application performance?

- A. Replace the volume with a magnetic volume.

- B. Increase the number of IOPS on the gp3 volume.

- C. Replace the volume with a Provisioned IOPS SSD (io2) volume.

- D. Replace the 2,000 GB gp3 volume with two 1,000 GB gp3 volumes.

**Correct:** C
**Why:** Move to Provisioned IOPS SSD (io2) to exceed 20K IOPS limits of gp3 and ensure consistent high IOPS for peak periods.

**Incorrect:**
- A: Magnetic volumes are low performance.
- B: gp3 max IOPS is 16K—insufficient for >20K IOPS.
- D: Splitting gp3 volumes does not increase single‑volume IOPS for RDS.


---

---

### Question #396

A company has implemented a self-managed DNS service on AWS. The solution consists of the following: • Amazon EC2 instances in different AWS Regions • Endpoints of a standard accelerator in AWS Global Accelerator The company wants to protect the solution against DDoS attacks. What should a solutions architect do to meet this requirement?

- A. Subscribe to AWS Shield Advanced. Add the accelerator as a resource to protect.

- B. Subscribe to AWS Shield Advanced. Add the EC2 instances as resources to protect.

- C. Create an AWS WAF web ACL that includes a rate-based rule. Associate the web ACL with the accelerator.

- D. Create an AWS WAF web ACL that includes a rate-based rule. Associate the web ACL with the EC2 instances.

**Correct:** A
**Why:** Shield Advanced protects Global Accelerator resources against DDoS; add the accelerator as a protected resource.

**Incorrect:**
- B: Protecting individual EC2 instances misses the accelerator edge.
- C: AWS WAF rate rules do not mitigate volumetric DDoS at the network layer.
- D: AWS WAF rate rules do not mitigate volumetric DDoS at the network layer.


---

---

### Question #397

An ecommerce company needs to run a scheduled daily job to aggregate and filter sales records for analytics. The company stores the sales records in an Amazon S3 bucket. Each object can be up to 10 GB in size. Based on the number of sales events, the job can take up to an hour to complete. The CPU and memory usage of the job are constant and are known in advance. A solutions architect needs to minimize the amount of operational effort that is needed for the job to run. Which solution meets these requirements?

- A. Create an AWS Lambda function that has an Amazon EventBridge notification. Schedule the EventBridge event to run once a day.

- B. Create an AWS Lambda function. Create an Amazon API Gateway HTTP API, and integrate the API with the function. Create an Amazon EventBridge scheduled event that calls the API and invokes the function.

- C. Create an Amazon Elastic Container Service (Amazon ECS) cluster with an AWS Fargate launch type. Create an Amazon EventBridge scheduled event that launches an ECS task on the cluster to run the job.

- D. Create an Amazon Elastic Container Service (Amazon ECS) cluster with an Amazon EC2 launch type and an Auto Scaling group with at least one EC2 instance. Create an Amazon EventBridge scheduled event that launches an ECS task on the cluster to run the job.

**Correct:** C
**Why:** ECS on Fargate with an EventBridge schedule runs the containerized job with known CPU/memory, minimizing ops (no servers to manage).

**Incorrect:**
- A: Lambda is not suitable for up to 10 GB objects and 1‑hour jobs.
- B: Lambda is not suitable for up to 10 GB objects and 1‑hour jobs.
- D: EC2 launch type requires managing instances/ASG.


---

---

### Question #401

A company wants to use the AWS Cloud to make an existing application highly available and resilient. The current version of the application resides in the company's data center. The application recently experienced data loss after a database server crashed because of an unexpected power outage. The company needs a solution that avoids any single points of failure. The solution must give the application the ability to scale to meet user demand. Which solution will meet these requirements?

- A. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon RDS DB instance in a Multi-AZ configuration.

- B. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group in a single Availability Zone. Deploy the database on an EC2 instance. Enable EC2 Auto Recovery.

- C. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon RDS DB instance with a read replica in a single Availability Zone. Promote the read replica to replace the primary DB instance if the primary DB instance fails.

- D. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Deploy the primary and secondary database servers on EC2 instances across multiple Availability Zones. Use Amazon Elastic Block Store (Amazon EBS) Multi-Attach to create shared storage between the instances.

**Correct:** A
**Why:** Multi-AZ Auto Scaling for EC2 app tier plus RDS Multi-AZ removes single points of failure and provides scalability.

**Incorrect:**
- B: Single AZ and self-managed DB are single points of failure.
- C: A single-AZ primary with read replica is not the same as Multi-AZ failover.
- D: EBS Multi-Attach is not for databases and adds complexity.


---

---

### Question #402

A company needs to ingest and handle large amounts of streaming data that its application generates. The application runs on Amazon EC2 instances and sends data to Amazon Kinesis Data Streams, which is congured with default settings. Every other day, the application consumes the data and writes the data to an Amazon S3 bucket for business intelligence (BI) processing. The company observes that Amazon S3 is not receiving all the data that the application sends to Kinesis Data Streams. What should a solutions architect do to resolve this issue?

- A. Update the Kinesis Data Streams default settings by modifying the data retention period.

- B. Update the application to use the Kinesis Producer Library (KPL) to send the data to Kinesis Data Streams.

- C. Update the number of Kinesis shards to handle the throughput of the data that is sent to Kinesis Data Streams.

- D. Turn on S3 Versioning within the S3 bucket to preserve every version of every object that is ingested in the S3 bucket.

**Correct:** A
**Why:** Default Kinesis Data Streams retention is 24 hours; consuming every other day requires a longer retention period.

**Incorrect:**
- B: KPL helps with throughput, not expired records.
- C: More shards won’t recover expired data.
- D: S3 Versioning does not affect Kinesis delivery.


---

---

### Question #405

A solutions architect is designing the architecture for a software demonstration environment. The environment will run on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). The system will experience signicant increases in trac during working hours but is not required to operate on weekends. Which combination of actions should the solutions architect take to ensure that the system can scale to meet demand? (Choose two.)

- A. Use AWS Auto Scaling to adjust the ALB capacity based on request rate.

- B. Use AWS Auto Scaling to scale the capacity of the VPC internet gateway.

- C. Launch the EC2 instances in multiple AWS Regions to distribute the load across Regions.

- D. Use a target tracking scaling policy to scale the Auto Scaling group based on instance CPU utilization.

E. Use scheduled scaling to change the Auto Scaling group minimum, maximum, and desired capacity to zero for weekends. Revert to the default values at the start of the week.

**Correct:** D, E
**Why:** Use target tracking for dynamic scaling and scheduled scaling to zero on weekends to save cost.

**Incorrect:**
- A: ALB and internet gateways are managed; you don’t scale them.
- B: ALB and internet gateways are managed; you don’t scale them.
- C: Multi-Region is unnecessary for this need.


---

---

### Question #409

A solutions architect must migrate a Windows Internet Information Services (IIS) web application to AWS. The application currently relies on a file share hosted in the user's on-premises network-attached storage (NAS). The solutions architect has proposed migrating the IIS web servers to Amazon EC2 instances in multiple Availability Zones that are connected to the storage solution, and conguring an Elastic Load Balancer attached to the instances. Which replacement to the on-premises file share is MOST resilient and durable?

- A. Migrate the file share to Amazon RDS.

- B. Migrate the file share to AWS Storage Gateway.

- C. Migrate the file share to Amazon FSx for Windows File Server.

- D. Migrate the file share to Amazon Elastic File System (Amazon EFS).

**Correct:** C
**Why:** FSx for Windows File Server provides fully managed SMB shares for Windows IIS with HA/durability.

**Incorrect:**
- A: RDS is a database, not a file share.
- B: Storage Gateway is for hybrid access, not the most resilient/durable for primary shared storage.
- D: EFS is NFS (Linux), not SMB.


---

---

### Question #410

A company is deploying a new application on Amazon EC2 instances. The application writes data to Amazon Elastic Block Store (Amazon EBS) volumes. The company needs to ensure that all data that is written to the EBS volumes is encrypted at rest. Which solution will meet this requirement?

- A. Create an IAM role that species EBS encryption. Attach the role to the EC2 instances.

- B. Create the EBS volumes as encrypted volumes. Attach the EBS volumes to the EC2 instances.

- C. Create an EC2 instance tag that has a key of Encrypt and a value of True. Tag all instances that require encryption at the EBS level.

- D. Create an AWS Key Management Service (AWS KMS) key policy that enforces EBS encryption in the account. Ensure that the key policy is active.

**Correct:** B
**Why:** Create and attach encrypted EBS volumes (or enable EBS encryption by default) to ensure at-rest encryption.

**Incorrect:**
- A: Tags/roles/key policies do not enforce encryption by themselves.
- C: Tags/roles/key policies do not enforce encryption by themselves.
- D: Tags/roles/key policies do not enforce encryption by themselves.


---

---

### Question #411

A company has a web application with sporadic usage patterns. There is heavy usage at the beginning of each month, moderate usage at the start of each week, and unpredictable usage during the week. The application consists of a web server and a MySQL database server running inside the data center. The company would like to move the application to the AWS Cloud, and needs to select a cost-effective database platform that will not require database modications. Which solution will meet these requirements?

- A. Amazon DynamoDB

- B. Amazon RDS for MySQL

- C. MySQL-compatible Amazon Aurora Serverless

- D. MySQL deployed on Amazon EC2 in an Auto Scaling group

**Correct:** B
**Why:** Amazon RDS for MySQL is a managed, cost‑effective, compatible drop‑in with no application or schema changes.

**Incorrect:**
- A: DynamoDB would require a rewrite.
- C: Aurora Serverless can be cost‑effective but may require migration/compatibility testing; RDS MySQL is simpler.
- D: Self-managed MySQL on EC2 increases ops.


---

---

### Question #413

An ecommerce company is experiencing an increase in user trac. The company’s store is deployed on Amazon EC2 instances as a two-tier web application consisting of a web tier and a separate database tier. As trac increases, the company notices that the architecture is causing signicant delays in sending timely marketing and order conrmation email to users. The company wants to reduce the time it spends resolving complex email delivery issues and minimize operational overhead. What should a solutions architect do to meet these requirements?

- A. Create a separate application tier using EC2 instances dedicated to email processing.

- B. Configure the web instance to send email through Amazon Simple Email Service (Amazon SES).

- C. Configure the web instance to send email through Amazon Simple Notification Service (Amazon SNS).

- D. Create a separate application tier using EC2 instances dedicated to email processing. Place the instances in an Auto Scaling group.

**Correct:** B
**Why:** Use Amazon SES to handle outbound email at scale with minimal ops.

**Incorrect:**
- A: Additional EC2 tiers increase ops and still require mail handling.
- C: SNS is not an SMTP service.
- D: Additional EC2 tiers increase ops and still require mail handling.


---

---

### Question #417

A company uses Amazon EC2 instances and AWS Lambda functions to run its application. The company has VPCs with public subnets and private subnets in its AWS account. The EC2 instances run in a private subnet in one of the VPCs. The Lambda functions need direct network access to the EC2 instances for the application to work. The application will run for at least 1 year. The company expects the number of Lambda functions that the application uses to increase during that time. The company wants to maximize its savings on all application resources and to keep network latency between the services low. Which solution will meet these requirements?

- A. Purchase an EC2 Instance Savings Plan Optimize the Lambda functions’ duration and memory usage and the number of invocations. Connect the Lambda functions to the private subnet that contains the EC2 instances.

- B. Purchase an EC2 Instance Savings Plan Optimize the Lambda functions' duration and memory usage, the number of invocations, and the amount of data that is transferred. Connect the Lambda functions to a public subnet in the same VPC where the EC2 instances run.

- C. Purchase a Compute Savings Plan. Optimize the Lambda functions’ duration and memory usage, the number of invocations, and the amount of data that is transferred. Connect the Lambda functions to the private subnet that contains the EC2 instances.

- D. Purchase a Compute Savings Plan. Optimize the Lambda functions’ duration and memory usage, the number of invocations, and the amount of data that is transferred. Keep the Lambda functions in the Lambda service VPC.

**Correct:** C
**Why:** Compute Savings Plan covers EC2 and Lambda; attach Lambda to the private subnet for low‑latency access to EC2.

**Incorrect:**
- A: EC2 Instance Savings Plan does not apply to Lambda.
- B: EC2 Instance Savings Plan does not apply to Lambda.
- D: Keeping Lambda in the service VPC prevents direct access to private EC2.


---

---

### Question #419

A company uses AWS Organizations with all features enabled and runs multiple Amazon EC2 workloads in the ap-southeast-2 Region. The company has a service control policy (SCP) that prevents any resources from being created in any other Region. A security policy requires the company to encrypt all data at rest. An audit discovers that employees have created Amazon Elastic Block Store (Amazon EBS) volumes for EC2 instances without encrypting the volumes. The company wants any new EC2 instances that any IAM user or root user launches in ap-southeast-2 to use encrypted EBS volumes. The company wants a solution that will have minimal effect on employees who create EBS volumes. Which combination of steps will meet these requirements? (Choose two.)

- A. In the Amazon EC2 console, select the EBS encryption account attribute and dene a default encryption key.

- B. Create an IAM permission boundary. Attach the permission boundary to the root organizational unit (OU). Dene the boundary to deny the ec2:CreateVolume action when the ec2:Encrypted condition equals false.

- C. Create an SCP. Attach the SCP to the root organizational unit (OU). Dene the SCP to deny the ec2:CreateVolume action whenthe ec2:Encrypted condition equals false.

- D. Update the IAM policies for each account to deny the ec2:CreateVolume action when the ec2:Encrypted condition equals false.

E. In the Organizations management account, specify the Default EBS volume encryption setting.

**Correct:** A, C
**Why:** Enable EBS default encryption in the Region/account and enforce with an SCP that denies unencrypted volume creation.

**Incorrect:**
- B: Permission boundaries or per‑account policy updates are heavier and easier to bypass than an SCP.
- D: Permission boundaries or per‑account policy updates are heavier and easier to bypass than an SCP.
- E: No organization‑level default EBS encryption setting to apply globally.


---

---

### Question #421

A company runs a highly available SFTP service. The SFTP service uses two Amazon EC2 Linux instances that run with elastic IP addresses to accept trac from trusted IP sources on the internet. The SFTP service is backed by shared storage that is attached to the instances. User accounts are created and managed as Linux users in the SFTP servers. The company wants a serverless option that provides high IOPS performance and highly congurable security. The company also wants to maintain control over user permissions. Which solution will meet these requirements?

- A. Create an encrypted Amazon Elastic Block Store (Amazon EBS) volume. Create an AWS Transfer Family SFTP service with a public endpoint that allows only trusted IP addresses. Attach the EBS volume to the SFTP service endpoint. Grant users access to the SFTP service.

- B. Create an encrypted Amazon Elastic File System (Amazon EFS) volume. Create an AWS Transfer Family SFTP service with elastic IP addresses and a VPC endpoint that has internet-facing access. Attach a security group to the endpoint that allows only trusted IP addresses. Attach the EFS volume to the SFTP service endpoint. Grant users access to the SFTP service.

- C. Create an Amazon S3 bucket with default encryption enabled. Create an AWS Transfer Family SFTP service with a public endpoint that allows only trusted IP addresses. Attach the S3 bucket to the SFTP service endpoint. Grant users access to the SFTP service.

- D. Create an Amazon S3 bucket with default encryption enabled. Create an AWS Transfer Family SFTP service with a VPC endpoint that has internal access in a private subnet. Attach a security group that allows only trusted IP addresses. Attach the S3 bucket to the SFTP service endpoint. Grant users access to the SFTP service.

**Correct:** C
**Why:** Transfer Family with an S3 backend is serverless, scalable, and supports IP allow lists; S3 delivers high throughput/IOPS.

**Incorrect:**
- A: You cannot attach EBS/EFS directly to Transfer Family.
- B: You cannot attach EBS/EFS directly to Transfer Family.
- D: A private VPC endpoint is internal; trusted public IPs could not reach it.


---

---

### Question #422

A company is developing a new machine learning (ML) model solution on AWS. The models are developed as independent microservices that fetch approximately 1 GB of model data from Amazon S3 at startup and load the data into memory. Users access the models through an asynchronous API. Users can send a request or a batch of requests and specify where the results should be sent. The company provides models to hundreds of users. The usage patterns for the models are irregular. Some models could be unused for days or weeks. Other models could receive batches of thousands of requests at a time. Which design should a solutions architect recommend to meet these requirements?

- A. Direct the requests from the API to a Network Load Balancer (NLB). Deploy the models as AWS Lambda functions that are invoked by the NLB.

- B. Direct the requests from the API to an Application Load Balancer (ALB). Deploy the models as Amazon Elastic Container Service (Amazon ECS) services that read from an Amazon Simple Queue Service (Amazon SQS) queue. Use AWS App Mesh to scale the instances of the ECS cluster based on the SQS queue size.

- C. Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue. Deploy the models as AWS Lambda functions that are invoked by SQS events. Use AWS Auto Scaling to increase the number of vCPUs for the Lambda functions based on the SQS queue size.

- D. Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue. Deploy the models as Amazon Elastic Container Service (Amazon ECS) services that read from the queue. Enable AWS Auto Scaling on Amazon ECS for both the cluster and copies of the service based on the queue size.

**Correct:** D
**Why:** Queue requests in SQS and run models on ECS services that scale on queue depth; suitable for 1 GB model loads and bursty use.

**Incorrect:**
- A: NLB→Lambda is not a pattern; Lambda cold starts with 1 GB model pulls are costly.
- B: ALB→ECS without decoupling risks backpressure; App Mesh is unnecessary.
- C: Lambda with 1 GB model loads and vCPU scaling is not appropriate.


---

---

### Question #423

A solutions architect wants to use the following JSON text as an identity-based policy to grant specic permissions: Which IAM principals can the solutions architect attach this policy to? (Choose two.)

- A. Role

- B. Group

- C. Organization

- D. Amazon Elastic Container Service (Amazon ECS) resource

E. Amazon EC2 resource

**Correct:** A, B
**Why:** Identity‑based policies attach to IAM users, groups, or roles. Among options, roles and groups are valid.

**Incorrect:**
- C: Organizations and resource types are not IAM identities for identity policies.
- D: Organizations and resource types are not IAM identities for identity policies.
- E: Organizations and resource types are not IAM identities for identity policies.


---

---

### Question #424

A company is running a custom application on Amazon EC2 On-Demand Instances. The application has frontend nodes that need to run 24 hours a day, 7 days a week and backend nodes that need to run only for a short time based on workload. The number of backend nodes varies during the day. The company needs to scale out and scale in more instances based on workload. Which solution will meet these requirements MOST cost-effectively?

- A. Use Reserved Instances for the frontend nodes. Use AWS Fargate for the backend nodes.

- B. Use Reserved Instances for the frontend nodes. Use Spot Instances for the backend nodes.

- C. Use Spot Instances for the frontend nodes. Use Reserved Instances for the backend nodes.

- D. Use Spot Instances for the frontend nodes. Use AWS Fargate for the backend nodes.

**Correct:** B
**Why:** Reserve capacity for always‑on frontend; use Spot for variable, interruptible backend workloads to save cost.

**Incorrect:**
- A: Either misplace RIs or use costlier options.
- C: Either misplace RIs or use costlier options.
- D: Either misplace RIs or use costlier options.


---

---

### Question #425

A company uses high block storage capacity to runs its workloads on premises. The company's daily peak input and output transactions per second are not more than 15,000 IOPS. The company wants to migrate the workloads to Amazon EC2 and to provision disk performance independent of storage capacity. Which Amazon Elastic Block Store (Amazon EBS) volume type will meet these requirements MOST cost-effectively?

- A. GP2 volume type

- B. io2 volume type

- C. GP3 volume type

- D. io1 volume type

**Correct:** C
**Why:** gp3 lets you provision IOPS independent of capacity up to required 15K IOPS, cost‑effectively.

**Incorrect:**
- A: gp2 IOPS scale with size, not independently.
- B: io classes are pricier than needed.
- D: io classes are pricier than needed.


---

---

### Question #427

A solutions architect is implementing a complex Java application with a MySQL database. The Java application must be deployed on Apache Tomcat and must be highly available. What should the solutions architect do to meet these requirements?

- A. Deploy the application in AWS Lambda. Configure an Amazon API Gateway API to connect with the Lambda functions.

- B. Deploy the application by using AWS Elastic Beanstalk. Configure a load-balanced environment and a rolling deployment policy.

- C. Migrate the database to Amazon ElastiCache. Configure the ElastiCache security group to allow access from the application.

- D. Launch an Amazon EC2 instance. Install a MySQL server on the EC2 instance. Configure the application on the server. Create an AMI. Use the AMI to create a launch template with an Auto Scaling group.

**Correct:** B
**Why:** Elastic Beanstalk for Tomcat with load‑balanced, rolling deployments meets HA needs with low ops; use RDS for MySQL.

**Incorrect:**
- A: Lambda is not for Tomcat Java web apps.
- C: ElastiCache is not a database.
- D: DIY EC2/ASG raises operational burden.


---

---

### Question #429

The following IAM policy is attached to an IAM group. This is the only policy applied to the group. What are the effective IAM permissions of this policy for group members?

- A. Group members are permitted any Amazon EC2 action within the us-east-1 Region. Statements after the Allow permission are not applied.

- B. Group members are denied any Amazon EC2 permissions in the us-east-1 Region unless they are logged in with multi-factor authentication (MFA).

- C. Group members are allowed the ec2:StopInstances and ec2:TerminateInstances permissions for all Regions when logged in with multi- factor authentication (MFA). Group members are permitted any other Amazon EC2 action.

- D. Group members are allowed the ec2:StopInstances and ec2:TerminateInstances permissions for the us-east-1 Region only when logged in with multi-factor authentication (MFA). Group members are permitted any other Amazon EC2 action within the us-east-1 Region.

**Correct:** D
**Why:** The policy allows EC2 actions in us‑east‑1 but requires MFA for Stop/Terminate. Other EC2 actions in us‑east‑1 remain allowed.

**Incorrect:**
- A: Misstate the MFA condition scope or Regions.
- B: Misstate the MFA condition scope or Regions.
- C: Misstate the MFA condition scope or Regions.


---

---

### Question #430

A manufacturing company has machine sensors that upload .csv files to an Amazon S3 bucket. These .csv files must be converted into images and must be made available as soon as possible for the automatic generation of graphical reports. The images become irrelevant after 1 month, but the .csv files must be kept to train machine learning (ML) models twice a year. The ML trainings and audits are planned weeks in advance. Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)

- A. Launch an Amazon EC2 Spot Instance that downloads the .csv files every hour, generates the image files, and uploads the images to the S3 bucket.

- B. Design an AWS Lambda function that converts the .csv files into images and stores the images in the S3 bucket. Invoke the Lambda function when a .csv file is uploaded.

- C. Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files from S3 Standard to S3 Glacier 1 day after they are uploaded. Expire the image files after 30 days.

- D. Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) 1 day after they are uploaded. Expire the image files after 30 days.

E. Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 1 day after they are uploaded. Keep the image files in Reduced Redundancy Storage (RRS).

**Correct:** B, C
**Why:** Invoke Lambda on S3 put to convert CSV to images quickly; transition CSVs to Glacier and expire images after 30 days to cut cost.

**Incorrect:**
- A: Managing Spot EC2 adds ops and latency.
- D: One Zone‑IA/RRS are not optimal and RRS is deprecated.
- E: One Zone‑IA/RRS are not optimal and RRS is deprecated.


---

---

### Question #434

A company hosts its application in the AWS Cloud. The application runs on Amazon EC2 instances behind an Elastic Load Balancer in an Auto Scaling group and with an Amazon DynamoDB table. The company wants to ensure the application can be made available in anotherAWS Region with minimal downtime. What should a solutions architect do to meet these requirements with the LEAST amount of downtime?

- A. Create an Auto Scaling group and a load balancer in the disaster recovery Region. Configure the DynamoDB table as a global table. Configure DNS failover to point to the new disaster recovery Region's load balancer.

- B. Create an AWS CloudFormation template to create EC2 instances, load balancers, and DynamoDB tables to be launched when needed Configure DNS failover to point to the new disaster recovery Region's load balancer.

- C. Create an AWS CloudFormation template to create EC2 instances and a load balancer to be launched when needed. Configure the DynamoDB table as a global table. Configure DNS failover to point to the new disaster recovery Region's load balancer.

- D. Create an Auto Scaling group and load balancer in the disaster recovery Region. Configure the DynamoDB table as a global table. Create an Amazon CloudWatch alarm to trigger an AWS Lambda function that updates Amazon Route 53 pointing to the disaster recovery load balancer.

**Correct:** A
**Why:** Pre‑provision ALB/ASG in DR Region and use DynamoDB global tables; DNS failover minimizes downtime.

**Incorrect:**
- B: Delayed provisioning increases downtime.
- C: Delayed provisioning increases downtime.
- D: Extra Lambda automation is unnecessary.


---

---

### Question #437

A company operates an ecommerce website on Amazon EC2 instances behind an Application Load Balancer (ALB) in an Auto Scaling group. The site is experiencing performance issues related to a high request rate from illegitimate external systems with changing IP addresses. The security team is worried about potential DDoS attacks against the website. The company must block the illegitimate incoming requests in a way that has a minimal impact on legitimate users. What should a solutions architect recommend?

- A. Deploy Amazon Inspector and associate it with the ALB.

- B. Deploy AWS WAF, associate it with the ALB, and configure a rate-limiting rule.

- C. Deploy rules to the network ACLs associated with the ALB to block the incomingtrac.

- D. Deploy Amazon GuardDuty and enable rate-limiting protection when conguring GuardDuty.

**Correct:** B
**Why:** Attach AWS WAF to the ALB and configure rate‑based rules to throttle illegitimate request floods while allowing legitimate users.

**Incorrect:**
- A: Inspector/GuardDuty do not mitigate at the edge.
- C: NACLs are coarse and static; IPs change.
- D: Inspector/GuardDuty do not mitigate at the edge.


---

---

### Question #439

A solutions architect congured a VPC that has a small range of IP addresses. The number of Amazon EC2 instances that are in the VPC is increasing, and there is an insucient number of IP addresses for future workloads. Which solution resolves this issue with the LEAST operational overhead?

- A. Add an additional IPv4 CIDR block to increase the number of IP addresses and create additional subnets in the VPC. Create new resources in the new subnets by using the new CIDR.

- B. Create a second VPC with additional subnets. Use a peering connection to connect the second VPC with the first VPC Update the routes and create new resources in the subnets of the second VPC.

- C. Use AWS Transit Gateway to add a transit gateway and connect a second VPC with the first VPUpdate the routes of the transit gateway and VPCs. Create new resources in the subnets of the second VPC.

- D. Create a second VPC. Create a Site-to-Site VPN connection between the first VPC and the second VPC by using a VPN-hosted solution on Amazon EC2 and a virtual private gateway. Update the route between VPCs to the trac through the VPN. Create new resources in the subnets of the second VPC.

**Correct:** A
**Why:** Add an additional IPv4 CIDR to the VPC and create new subnets—simple and low‑ops.

**Incorrect:**
- B: Creating/connecting a second VPC adds complexity.
- C: Creating/connecting a second VPC adds complexity.
- D: Creating/connecting a second VPC adds complexity.


---

---

### Question #441

A company hosts a multi-tier web application on Amazon Linux Amazon EC2 instances behind an Application Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The company observes that the Auto Scaling group launches more On-Demand Instances when the application's end users access high volumes of static web content. The company wants to optimize cost. What should a solutions architect do to redesign the application MOST cost-effectively?

- A. Update the Auto Scaling group to use Reserved Instances instead of On-Demand Instances.

- B. Update the Auto Scaling group to scale by launching Spot Instances instead of On-Demand Instances.

- C. Create an Amazon CloudFront distribution to host the static web contents from an Amazon S3 bucket.

- D. Create an AWS Lambda function behind an Amazon API Gateway API to host the static website contents.

**Correct:** C
**Why:** Serve static content from S3 via CloudFront to reduce load on EC2/ALB and lower cost.

**Incorrect:**
- A: Instance pricing choices don’t remove static load driver.
- B: Instance pricing choices don’t remove static load driver.
- D: Lambda+API Gateway is unnecessary for static hosting.


---

---

### Question #443

A company wants to host a scalable web application on AWS. The application will be accessed by users from different geographic regions of the world. Application users will be able to download and upload unique data up to gigabytes in size. The development team wants a cost-effective solution to minimize upload and download latency and maximize performance. What should a solutions architect do to accomplish this?

- A. Use Amazon S3 with Transfer Acceleration to host the application.

- B. Use Amazon S3 with CacheControl headers to host the application.

- C. Use Amazon EC2 with Auto Scaling and Amazon CloudFront to host the application.

- D. Use Amazon EC2 with Auto Scaling and Amazon ElastiCache to host the application.

**Correct:** A
**Why:** S3 Transfer Acceleration provides globally distributed edge ingress/egress to minimize latency for large uploads/downloads.

**Incorrect:**
- B: CacheControl headers do not improve upload latency.
- C: EC2 adds ops and may not minimize global transfer latency.
- D: EC2 adds ops and may not minimize global transfer latency.


---

---

### Question #444

A company has hired a solutions architect to design a reliable architecture for its application. The application consists of one Amazon RDS DB instance and two manually provisioned Amazon EC2 instances that run web servers. The EC2 instances are located in a single Availability Zone. An employee recently deleted the DB instance, and the application was unavailable for 24 hours as a result. The company is concerned with the overall reliability of its environment. What should the solutions architect do to maximize reliability of the application's infrastructure?

- A. Delete one EC2 instance and enable termination protection on the other EC2 instance. Update the DB instance to be Multi-AZ, and enable deletion protection.

- B. Update the DB instance to be Multi-AZ, and enable deletion protection. Place the EC2 instances behind an Application Load Balancer, and run them in an EC2 Auto Scaling group across multiple Availability Zones.

- C. Create an additional DB instance along with an Amazon API Gateway and an AWS Lambda function. Configure the application to invoke the Lambda function through API Gateway. Have the Lambda function write the data to the two DB instances.

- D. Place the EC2 instances in an EC2 Auto Scaling group that has multiple subnets located in multiple Availability Zones. Use Spot Instances instead of On-Demand Instances. Set up Amazon CloudWatch alarms to monitor the health of the instances Update the DB instance to be Multi-AZ, and enable deletion protection.

**Correct:** B
**Why:** Protect RDS with Multi‑AZ and deletion protection; place EC2 in ALB+Auto Scaling across AZs to maximize reliability.

**Incorrect:**
- A: Either reduce availability or add unnecessary components/cost.
- C: Either reduce availability or add unnecessary components/cost.
- D: Either reduce availability or add unnecessary components/cost.


---

---

### Question #449

A company runs its application on an Oracle database. The company plans to quickly migrate to AWS because of limited resources for the database, backup administration, and data center maintenance. The application uses third-party database features that require privileged access. Which solution will help the company migrate the database to AWS MOST cost-effectively?

- A. Migrate the database to Amazon RDS for Oracle. Replace third-party features with cloud services.

- B. Migrate the database to Amazon RDS Custom for Oracle. Customize the database settings to support third-party features.

- C. Migrate the database to an Amazon EC2 Amazon Machine Image (AMI) for Oracle. Customize the database settings to support third-party features.

- D. Migrate the database to Amazon RDS for PostgreSQL by rewriting the application code to remove dependency on Oracle APEX.

**Correct:** B
**Why:** RDS Custom for Oracle allows privileged access/customization for third‑party features with managed backups/patching and lower ops than EC2.

**Incorrect:**
- A: RDS for Oracle restricts privileged access needed for third‑party features.
- C: EC2 is higher ops/cost to manage.
- D: Rewriting to PostgreSQL increases effort and risk.


---

---

### Question #450

A company has a three-tier web application that is in a single server. The company wants to migrate the application to the AWS Cloud. The company also wants the application to align with the AWS Well-Architected Framework and to be consistent with AWS recommended best practices for security, scalability, and resiliency. Which combination of solutions will meet these requirements? (Choose three.)

- A. Create a VPC across two Availability Zones with the application's existing architecture. Host the application with existing architecture on an Amazon EC2 instance in a private subnet in each Availability Zone with EC2 Auto Scaling groups. Secure the EC2 instance with security groups and network access control lists (network ACLs).

- B. Set up security groups and network access control lists (network ACLs) to control access to the database layer. Set up a single Amazon RDS database in a private subnet.

- C. Create a VPC across two Availability Zones. Refactor the application to host the web tier, application tier, and database tier. Host each tier on its own private subnet with Auto Scaling groups for the web tier and application tier.

- D. Use a single Amazon RDS database. Allow database access only from the application tier security group.

E. Use Elastic Load Balancers in front of the web tier. Control access by using security groups containing references to each layer's security groups.

F. Use an Amazon RDS database Multi-AZ cluster deployment in private subnets. Allow database access only from application tier security groups.

**Correct:** C, E, F
**Why:** Refactor into three tiers across two AZs; use ELBs for the web tier; and use RDS Multi‑AZ in private subnets with SGs restricting access from the app tier.

**Incorrect:**
- A: Single RDS or single‑AZ design and flat architectures don’t meet best practices for resiliency/security.
- B: Single RDS or single‑AZ design and flat architectures don’t meet best practices for resiliency/security.
- D: Single RDS or single‑AZ design and flat architectures don’t meet best practices for resiliency/security.


---

---

### Question #452

A company runs a Java-based job on an Amazon EC2 instance. The job runs every hour and takes 10 seconds to run. The job runs on a scheduled interval and consumes 1 GB of memory. The CPU utilization of the instance is low except for short surges during which the job uses the maximum CPU available. The company wants to optimize the costs to run the job. Which solution will meet these requirements?

- A. Use AWS App2Container (A2C) to containerize the job. Run the job as an Amazon Elastic Container Service (Amazon ECS) task on AWS Fargate with 0.5 virtual CPU (vCPU) and 1 GB of memory.

- B. Copy the code into an AWS Lambda function that has 1 GB of memory. Create an Amazon EventBridge scheduled rule to run the code each hour.

- C. Use AWS App2Container (A2C) to containerize the job. Install the container in the existing Amazon Machine Image (AMI). Ensure that the schedule stops the container when the task nishes.

- D. Configure the existing schedule to stop the EC2 instance at the completion of the job and restart the EC2 instance when the next job starts.

**Correct:** B
**Why:** Lambda with 1 GB memory and an EventBridge schedule runs for ~10 seconds/hour at the lowest cost; no servers to manage.

**Incorrect:**
- A: Containerize/EC2 approaches incur more baseline cost and management for such a short job.
- C: Containerize/EC2 approaches incur more baseline cost and management for such a short job.
- D: Containerize/EC2 approaches incur more baseline cost and management for such a short job.


---

---

### Question #453

A company wants to implement a backup strategy for Amazon EC2 data and multiple Amazon S3 buckets. Because of regulatory requirements, the company must retain backup files for a specic time period. The company must not alter the files for the duration of the retention period. Which solution will meet these requirements?

- A. Use AWS Backup to create a backup vault that has a vault lock in governance mode. Create the required backup plan.

- B. Use Amazon Data Lifecycle Manager to create the required automated snapshot policy.

- C. Use Amazon S3 File Gateway to create the backup. Configure the appropriate S3 Lifecycle management.

- D. Use AWS Backup to create a backup vault that has a vault lock in compliance mode. Create the required backup plan.

**Correct:** D
**Why:** AWS Backup Vault Lock in compliance mode enforces WORM retention for EC2 and S3 backups to meet regulatory requirements.

**Incorrect:**
- A: Governance mode can be bypassed by privileged users.
- B: DLM and File Gateway do not provide WORM‑style immutability across all backups.
- C: DLM and File Gateway do not provide WORM‑style immutability across all backups.


---

---

### Question #456

A company runs applications on Amazon EC2 instances in one AWS Region. The company wants to back up the EC2 instances to a second Region. The company also wants to provision EC2 resources in the second Region and manage the EC2 instances centrally from one AWS account. Which solution will meet these requirements MOST cost-effectively?

- A. Create a disaster recovery (DR) plan that has a similar number of EC2 instances in the second Region. Configure data replication.

- B. Create point-in-time Amazon Elastic Block Store (Amazon EBS) snapshots of the EC2 instances. Copy the snapshots to the second Region periodically.

- C. Create a backup plan by using AWS Backup. Configure cross-Region backup to the second Region for the EC2 instances.

- D. Deploy a similar number of EC2 instances in the second Region. Use AWS DataSync to transfer the data from the source Region to the second Region.

**Correct:** C
**Why:** AWS Backup with cross‑Region backup centrally handles EC2/EBS backups to a second Region cost‑effectively.

**Incorrect:**
- A: Maintaining warm DR fleets or DataSync replication increases cost/ops.
- B: Manual EBS snapshot copying lacks centralized policy/automation.
- D: Maintaining warm DR fleets or DataSync replication increases cost/ops.


---

---

### Question #458

A solutions architect is designing a RESTAPI in Amazon API Gateway for a cash payback service. The application requires 1 GB of memory and 2 GB of storage for its computation resources. The application will require that the data is in a relational format. Which additional combination ofAWS services will meet these requirements with the LEAST administrative effort? (Choose two.)

- A. Amazon EC2

- B. AWS Lambda

- C. Amazon RDS

- D. Amazon DynamoDB

E. Amazon Elastic Kubernetes Services (Amazon EKS)

**Correct:** B, C
**Why:** Lambda can be sized to 1 GB memory and sufficient ephemeral storage; use Amazon RDS for relational storage—minimal admin.

**Incorrect:**
- A: EC2/EKS add operational overhead.
- D: DynamoDB is non‑relational.
- E: EC2/EKS add operational overhead.


---

---

### Question #459

A company uses AWS Organizations to run workloads within multiple AWS accounts. A tagging policy adds department tags to AWS resources when the company creates tags. An accounting team needs to determine spending on Amazon EC2 consumption. The accounting team must determine which departments are responsible for the costs regardless ofAWS account. The accounting team has access to AWS Cost Explorer for all AWS accounts within the organization and needs to access all reports from Cost Explorer. Which solution meets these requirements in the MOST operationally ecient way?

- A. From the Organizations management account billing console, activate a user-dened cost allocation tag named department. Create one cost report in Cost Explorer grouping by tag name, and filter by EC2.

- B. From the Organizations management account billing console, activate an AWS-dened cost allocation tag named department. Create one cost report in Cost Explorer grouping by tag name, and filter by EC2.

- C. From the Organizations member account billing console, activate a user-dened cost allocation tag named department. Create one cost report in Cost Explorer grouping by the tag name, and filter by EC2.

- D. From the Organizations member account billing console, activate an AWS-dened cost allocation tag named department. Create one cost report in Cost Explorer grouping by tag name, and filter by EC2.

**Correct:** A
**Why:** Activate the user‑defined cost allocation tag in the management account and build one Cost Explorer report grouped by that tag and filtered to EC2.

**Incorrect:**
- B: “AWS‑defined” cost tag for department does not exist; use your user‑defined tag.
- C: Activations must be in the management account for org‑wide reporting.
- D: Activations must be in the management account for org‑wide reporting.


---

---

### Question #461

A company is developing a mobile gaming app in a single AWS Region. The app runs on multiple Amazon EC2 instances in an Auto Scaling group. The company stores the app data in Amazon DynamoDB. The app communicates by using TCP trac and UDP trac between the users and the servers. The application will be used globally. The company wants to ensure the lowest possible latency for all users. Which solution will meet these requirements?

- A. Use AWS Global Accelerator to create an accelerator. Create an Application Load Balancer (ALB) behind an accelerator endpoint that uses Global Accelerator integration and listening on the TCP and UDP ports. Update the Auto Scaling group to register instances on the ALB.

- B. Use AWS Global Accelerator to create an accelerator. Create a Network Load Balancer (NLB) behind an accelerator endpoint that uses Global Accelerator integration and listening on the TCP and UDP ports. Update the Auto Scaling group to register instances on the NLB.

- C. Create an Amazon CloudFront content delivery network (CDN) endpoint. Create a Network Load Balancer (NLB) behind the endpoint and listening on the TCP and UDP ports. Update the Auto Scaling group to register instances on the NLB. Update CloudFront to use the NLB as the origin.

- D. Create an Amazon CloudFront content delivery network (CDN) endpoint. Create an Application Load Balancer (ALB) behind the endpoint and listening on the TCP and UDP ports. Update the Auto Scaling group to register instances on the ALB. Update CloudFront to use the ALB as the origin.

**Correct:** B
**Why:** Global Accelerator with an NLB supports both TCP and UDP and provides the lowest latency globally via the AWS edge.

**Incorrect:**
- A: ALB doesn’t support UDP; CloudFront is not for arbitrary TCP/UDP.
- C: ALB doesn’t support UDP; CloudFront is not for arbitrary TCP/UDP.
- D: ALB doesn’t support UDP; CloudFront is not for arbitrary TCP/UDP.


---

---

### Question #462

A company has an application that processes customer orders. The company hosts the application on an Amazon EC2 instance that saves the orders to an Amazon Aurora database. Occasionally when trac is high the workload does not process orders fast enough. What should a solutions architect do to write the orders reliably to the database as quickly as possible?

- A. Increase the instance size of the EC2 instance when trac is high. Write orders to Amazon Simple Notification Service (Amazon SNS). Subscribe the database endpoint to the SNS topic.

- B. Write orders to an Amazon Simple Queue Service (Amazon SQS) queue. Use EC2 instances in an Auto Scaling group behind an Application Load Balancer to read from the SQS queue and process orders into the database.

- C. Write orders to Amazon Simple Notification Service (Amazon SNS). Subscribe the database endpoint to the SNS topic. Use EC2 instances in an Auto Scaling group behind an Application Load Balancer to read from the SNS topic.

- D. Write orders to an Amazon Simple Queue Service (Amazon SQS) queue when the EC2 instance reaches CPU threshold limits. Use scheduled scaling of EC2 instances in an Auto Scaling group behind an Application Load Balancer to read from the SQS queue and process orders into the database.

**Correct:** B
**Why:** Buffer orders to SQS and scale consumers behind an ALB/ASG to write quickly and reliably to Aurora.

**Incorrect:**
- A: SNS lacks durable queue semantics; scaling only on CPU misses spikes; D is reactive.
- C: SNS lacks durable queue semantics; scaling only on CPU misses spikes; D is reactive.
- D: SNS lacks durable queue semantics; scaling only on CPU misses spikes; D is reactive.


---

---

### Question #464

A company hosts an online shopping application that stores all orders in an Amazon RDS for PostgreSQL Single-AZ DB instance. Management wants to eliminate single points of failure and has asked a solutions architect to recommend an approach to minimize database downtime without requiring any changes to the application code. Which solution meets these requirements?

- A. Convert the existing database instance to a Multi-AZ deployment by modifying the database instance and specifying the Multi-AZ option.

- B. Create a new RDS Multi-AZ deployment. Take a snapshot of the current RDS instance and restore the new Multi-AZ deployment with the snapshot.

- C. Create a read-only replica of the PostgreSQL database in another Availability Zone. Use Amazon Route 53 weighted record sets to distribute requests across the databases.

- D. Place the RDS for PostgreSQL database in an Amazon EC2 Auto Scaling group with a minimum group size of two. Use Amazon Route 53 weighted record sets to distribute requests across instances.

**Correct:** A
**Why:** Modifying the RDS instance to Multi‑AZ provides automatic failover with minimal downtime and no app changes.

**Incorrect:**
- B: Spinning up a new deployment adds downtime.
- C: Read replicas/EC2 ASG do not provide transparent failover for writes.
- D: Read replicas/EC2 ASG do not provide transparent failover for writes.


---

---

### Question #465

A company is developing an application to support customer demands. The company wants to deploy the application on multiple Amazon EC2 Nitro-based instances within the same Availability Zone. The company also wants to give the application the ability to write to multiple block storage volumes in multiple EC2 Nitro-based instances simultaneously to achieve higher application availability. Which solution will meet these requirements?

- A. Use General Purpose SSD (gp3) EBS volumes with Amazon Elastic Block Store (Amazon EBS) Multi-Attach

- B. Use Throughput Optimized HDD (st1) EBS volumes with Amazon Elastic Block Store (Amazon EBS) Multi-Attach

- C. Use Provisioned IOPS SSD (io2) EBS volumes with Amazon Elastic Block Store (Amazon EBS) Multi-Attach

- D. Use General Purpose SSD (gp2) EBS volumes with Amazon Elastic Block Store (Amazon EBS) Multi-Attach

**Correct:** C
**Why:** io2 supports EBS Multi‑Attach on Nitro instances, allowing simultaneous block access by multiple instances.

**Incorrect:**
- A: gp2/gp3/st1 do not support Multi‑Attach.
- B: gp2/gp3/st1 do not support Multi‑Attach.
- D: gp2/gp3/st1 do not support Multi‑Attach.


---

---

### Question #466

A company designed a stateless two-tier application that uses Amazon EC2 in a single Availability Zone and an Amazon RDS Multi-AZ DB instance. New company management wants to ensure the application is highly available. What should a solutions architect do to meet this requirement?

- A. Configure the application to use Multi-AZ EC2 Auto Scaling and create an Application Load Balancer

- B. Configure the application to take snapshots of the EC2 instances and send them to a different AWS Region

- C. Configure the application to use Amazon Route 53 latency-based routing to feed requests to the application

- D. Configure Amazon Route 53 rules to handle incoming requests and create a Multi-AZ Application Load Balancer

**Correct:** A
**Why:** Make the stateless tier highly available with Multi‑AZ Auto Scaling and an ALB; DB is already Multi‑AZ.

**Incorrect:**
- B: Snapshots/Route 53 alone do not provide app‑tier HA.
- C: Snapshots/Route 53 alone do not provide app‑tier HA.
- D: Snapshots/Route 53 alone do not provide app‑tier HA.


---

---

### Question #470

A company has applications hosted on Amazon EC2 instances with IPv6 addresses. The applications must initiate communications with other external applications using the internet. However the company’s security policy states that any external service cannot initiate a connection to the EC2 instances. What should a solutions architect recommend to resolve this issue?

- A. Create a NAT gateway and make it the destination of the subnet's route table

- B. Create an internet gateway and make it the destination of the subnet's route table

- C. Create a virtual private gateway and make it the destination of the subnet's route table

- D. Create an egress-only internet gateway and make it the destination of the subnet's route table

**Correct:** D
**Why:** For IPv6, use an egress‑only internet gateway to allow outbound‑only connections from instances.

**Incorrect:**
- A: NAT/IGW/VGW do not provide outbound‑only behavior for IPv6.
- B: NAT/IGW/VGW do not provide outbound‑only behavior for IPv6.
- C: NAT/IGW/VGW do not provide outbound‑only behavior for IPv6.


---

---

### Question #473

A company hosts a website on Amazon EC2 instances behind an Application Load Balancer (ALB). The website serves static content. Website trac is increasing, and the company is concerned about a potential increase in cost.

- A. Create an Amazon CloudFront distribution to cache state files at edge locations

- B. Create an Amazon ElastiCache cluster. Connect the ALB to the ElastiCache cluster to serve cached files

- C. Create an AWS WAF web ACL and associate it with the ALB. Add a rule to the web ACL to cache static files

- D. Create a second ALB in an alternative AWS Region. Route user trac to the closest Region to minimize data transfer costs

**Correct:** A
**Why:** CloudFront caches static files at edge locations, reducing ALB/EC2 load and cost.

**Incorrect:**
- B: ALB cannot connect directly to ElastiCache to serve files.
- C: WAF is for filtering, not caching.
- D: Adding a second ALB/Region doesn’t address caching/cost.


---

---

### Question #479

A company is making a prototype of the infrastructure for its new website by manually provisioning the necessary infrastructure. This infrastructure includes an Auto Scaling group, an Application Load Balancer and an Amazon RDS database. After the configuration has been thoroughly validated, the company wants the capability to immediately deploy the infrastructure for development and production use in two Availability Zones in an automated fashion. What should a solutions architect recommend to meet these requirements?

- A. Use AWS Systems Manager to replicate and provision the prototype infrastructure in two Availability Zones

- B. Dene the infrastructure as a template by using the prototype infrastructure as a guide. Deploy the infrastructure with AWS CloudFormation.

- C. Use AWS Cong to record the inventory of resources that are used in the prototype infrastructure. Use AWS Cong to deploy the prototype infrastructure into two Availability Zones.

- D. Use AWS Elastic Beanstalk and configure it to use an automated reference to the prototype infrastructure to automatically deploy new environments in two Availability Zones.

**Correct:** B
**Why:** Define the infra in CloudFormation templates and deploy to multiple AZs automatically.

**Incorrect:**
- A: Systems Manager/Config/Elastic Beanstalk are not ideal for full infra replication here.
- C: Systems Manager/Config/Elastic Beanstalk are not ideal for full infra replication here.
- D: Systems Manager/Config/Elastic Beanstalk are not ideal for full infra replication here.


---

---

### Question #480

A business application is hosted on Amazon EC2 and uses Amazon S3 for encrypted object storage. The chief information security ocer has directed that no application trac between the two services should traverse the public internet. Which capability should the solutions architect use to meet the compliance requirements?

- A. AWS Key Management Service (AWS KMS)

- B. VPC endpoint

- C. Private subnet

- D. Virtual private gateway

**Correct:** B
**Why:** A VPC gateway endpoint for S3 keeps traffic on the AWS network, not the public internet.

**Incorrect:**
- A: KMS is for encryption, not network path.
- C: Subnet type/VPN don’t guarantee private S3 access.
- D: Subnet type/VPN don’t guarantee private S3 access.


---

---

### Question #486

A company is building a three-tier application on AWS. The presentation tier will serve a static website The logic tier is a containerized application. This application will store data in a relational database. The company wants to simplify deployment and to reduce operational costs. Which solution will meet these requirements?

- A. Use Amazon S3 to host static content. Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for compute power. Use a managed Amazon RDS cluster for the database.

- B. Use Amazon CloudFront to host static content. Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 for compute power. Use a managed Amazon RDS cluster for the database.

- C. Use Amazon S3 to host static content. Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for compute power. Use a managed Amazon RDS cluster for the database.

- D. Use Amazon EC2 Reserved Instances to host static content. Use Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 for compute power. Use a managed Amazon RDS cluster for the database.

**Correct:** A
**Why:** S3 static hosting + ECS Fargate for containers + managed RDS minimizes ops and simplifies deployment.

**Incorrect:**
- B: CloudFront alone doesn’t host; EKS/EC2 increase ops.
- C: CloudFront alone doesn’t host; EKS/EC2 increase ops.
- D: CloudFront alone doesn’t host; EKS/EC2 increase ops.


---

---

### Question #492

A company has multiple AWS accounts for development work. Some staff consistently use oversized Amazon EC2 instances, which causes the company to exceed the yearly budget for the development accounts. The company wants to centrally restrict the creation of AWS resources in these accounts. Which solution will meet these requirements with the LEAST development effort?

- A. Develop AWS Systems Manager templates that use an approved EC2 creation process. Use the approved Systems Manager templates to provision EC2 instances.

- B. Use AWS Organizations to organize the accounts into organizational units (OUs). Dene and attach a service control policy (SCP) to control the usage of EC2 instance types.

- C. Configure an Amazon EventBridge rule that invokes an AWS Lambda function when an EC2 instance is created. Stop disallowed EC2 instance types.

- D. Set up AWS Service Catalog products for the staff to create the allowed EC2 instance types. Ensure that staff can deploy EC2 instances only by using the Service Catalog products.

**Correct:** B
**Why:** Organize accounts and attach an SCP that denies disallowed EC2 instance types (least effort, central control).

**Incorrect:**
- A: Heavier build or reactive after creation.
- C: Heavier build or reactive after creation.
- D: Heavier build or reactive after creation.


---

---

### Question #494

A company uses Amazon EC2 instances to host its internal systems. As part of a deployment operation, an administrator tries to use the AWS CLI to terminate an EC2 instance. However, the administrator receives a 403 (Access Denied) error message. The administrator is using an IAM role that has the following IAM policy attached: What is the cause of the unsuccessful request?

- A. The EC2 instance has a resource-based policy with a Deny statement.

- B. The principal has not been specied in the policy statement.

- C. The "Action" eld does not grant the actions that are required to terminate the EC2 instance.

- D. The request to terminate the EC2 instance does not originate from the CIDR blocks 192.0.2.0/24 or 203.0.113.0/24.

**Correct:** D
**Why:** The IAM policy likely restricts termination by source IP; a 403 occurs if the request is not from the allowed CIDR blocks.

**Incorrect:**
- A: Less likely without a resource policy; missing actions would be AccessDenied for specific actions, not IP condition failure.
- B: Less likely without a resource policy; missing actions would be AccessDenied for specific actions, not IP condition failure.
- C: Less likely without a resource policy; missing actions would be AccessDenied for specific actions, not IP condition failure.


---

---

### Question #497

A company has a service that reads and writes large amounts of data from an Amazon S3 bucket in the same AWS Region. The service is deployed on Amazon EC2 instances within the private subnet of a VPC. The service communicates with Amazon S3 over a NAT gateway in the public subnet. However, the company wants a solution that will reduce the data output costs. Which solution will meet these requirements MOST cost-effectively?

- A. Provision a dedicated EC2 NAT instance in the public subnet. Configure the route table for the private subnet to use the elastic network interface of this instance as the destination for all S3 trac.

- B. Provision a dedicated EC2 NAT instance in the private subnet. Configure the route table for the public subnet to use the elastic network interface of this instance as the destination for all S3 trac.

- C. Provision a VPC gateway endpoint. Configure the route table for the private subnet to use the gateway endpoint as the route for all S3 trac.

- D. Provision a second NAT gateway. Configure the route table for the private subnet to use this NAT gateway as the destination for all S3 trac.

**Correct:** C
**Why:** An S3 gateway endpoint keeps S3 traffic on the AWS network and avoids NAT gateway data processing charges.

**Incorrect:**
- A: NAT instances increase ops/cost.
- B: NAT instances increase ops/cost.
- D: A second NAT gateway increases cost.


---

## Amazon EFS

### Question #252

A solutions architect needs to design a system to store client case files. The files are core company assets and are important. The number of files will grow over time. The files must be simultaneously accessible from multiple application servers that run on Amazon EC2 instances. The solution must have built-in redundancy. Which solution meets these requirements?

- A. Amazon Elastic File System (Amazon EFS)

- B. Amazon Elastic Block Store (Amazon EBS)

- C. Amazon S3 Glacier Deep Archive

- D. AWS Backup

**Correct:** A
**Why:** Amazon EFS provides shared, redundant NFS storage accessible concurrently by multiple EC2 instances.

**Incorrect:**
- B: EBS is block storage attached to a single AZ/instance at a time.
- C: Glacier Deep Archive is archival, not shared storage.
- D: AWS Backup is for backups, not primary shared storage.


---

---

### Question #277

A company provides an online service for posting video content and transcoding it for use by any mobile platform. The application architecture uses Amazon Elastic File System (Amazon EFS) Standard to collect and store the videos so that multiple Amazon EC2 Linux instances can access the video content for processing. As the popularity of the service has grown over time, the storage costs have become too expensive. Which storage solution is MOST cost-effective?

- A. Use AWS Storage Gateway for files to store and process the video content.

- B. Use AWS Storage Gateway for volumes to store and process the video content.

- C. Use Amazon EFS for storing the video content. Once processing is complete, transfer the files to Amazon Elastic Block Store (Amazon EBS).

- D. Use Amazon S3 for storing the video content. Move the files temporarily over to an Amazon Elastic Block Store (Amazon EBS) volume attached to the server for processing.

**Correct:** D
**Why:** Store content durably/cost‑effectively in S3 and move to EBS temporarily for processing on EC2.

**Incorrect:**
- A: Storage Gateway is not optimal for large‑scale processing cost.
- B: Storage Gateway is not optimal for large‑scale processing cost.
- C: Keeping in EFS Standard is costly for infrequently accessed media.


---

---

### Question #287

A company wants to migrate a Windows-based application from on premises to the AWS Cloud. The application has three tiers: an application tier, a business tier, and a database tier with Microsoft SQL Server. The company wants to use specic features of SQL Server such as native backups and Data Quality Services. The company also needs to share files for processing between the tiers. How should a solutions architect design the architecture to meet these requirements?

- A. Host all three tiers on Amazon EC2 instances. Use Amazon FSx File Gateway for file sharing between the tiers.

- B. Host all three tiers on Amazon EC2 instances. Use Amazon FSx for Windows File Server for file sharing between the tiers.

- C. Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon RDS. Use Amazon Elastic File System (Amazon EFS) for file sharing between the tiers.

- D. Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon RDS. Use a Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) volume for file sharing between the tiers.

**Correct:** B
**Why:** Keep SQL Server features by hosting DB on EC2; use FSx for Windows File Server for shared files; EC2 for app tiers.

**Incorrect:**
- C: RDS may not support required SQL Server features; EBS for file sharing is inappropriate.
- D: RDS may not support required SQL Server features; EBS for file sharing is inappropriate.


---

---

### Question #288

A company is migrating a Linux-based web server group to AWS. The web servers must access files in a shared file store for some content. The company must not make any changes to the application. What should a solutions architect do to meet these requirements?

- A. Create an Amazon S3 Standard bucket with access to the web servers.

- B. Configure an Amazon CloudFront distribution with an Amazon S3 bucket as the origin.

- C. Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on all web servers.

- D. Configure a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume to all web servers.

**Correct:** C
**Why:** EFS provides a POSIX shared file system for Linux without app changes.

**Incorrect:**
- A: S3/CloudFront are object/CDN; EBS is single‑attach.
- B: S3/CloudFront are object/CDN; EBS is single‑attach.
- D: S3/CloudFront are object/CDN; EBS is single‑attach.


---

---

### Question #336

A company hosts a multi-tier web application that uses an Amazon Aurora MySQL DB cluster for storage. The application tier is hosted on Amazon EC2 instances. The company’s IT security guidelines mandate that the database credentials be encrypted and rotated every 14 days. What should a solutions architect do to meet this requirement with the LEAST operational effort?

- A. Create a new AWS Key Management Service (AWS KMS) encryption key. Use AWS Secrets Manager to create a new secret that uses the KMS key with the appropriate credentials. Associate the secret with the Aurora DB cluster. Configure a custom rotation period of 14 days.

- B. Create two parameters in AWS Systems Manager Parameter Store: one for the user name as a string parameter and one that uses the SecureString type for the password. Select AWS Key Management Service (AWS KMS) encryption for the password parameter, and load these parameters in the application tier. Implement an AWS Lambda function that rotates the password every 14 days.

- C. Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system in all EC2 instances of the application tier. Restrict the access to the file on the file system so that the application can read the file and that only super users can modify the file. Implement an AWS Lambda function that rotates the key in Aurora every 14 days and writes new credentials into the file.

- D. Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon S3 bucket that the application uses to load the credentials. Download the file to the application regularly to ensure that the correct credentials are used. Implement an AWS Lambda function that rotates the Aurora credentials every 14 days and uploads these credentials to the file in the S3 bucket.

**Correct:** A
**Why:** Secrets Manager with KMS‑encrypted secret and built‑in rotation integrates with Aurora; set 14‑day rotation.

**Incorrect:**
- B: Parameter Store/EFS/S3 solutions require more glue code and ops.
- C: Parameter Store/EFS/S3 solutions require more glue code and ops.
- D: Parameter Store/EFS/S3 solutions require more glue code and ops.


---

---

### Question #344

A company has a Java application that uses Amazon Simple Queue Service (Amazon SQS) to parse messages. The application cannot parse messages that are larger than 256 KB in size. The company wants to implement a solution to give the application the ability to parse messages as large as 50 MB. Which solution will meet these requirements with the FEWEST changes to the code?

- A. Use the Amazon SQS Extended Client Library for Java to host messages that are larger than 256 KB in Amazon S3.

- B. Use Amazon EventBridge to post large messages from the application instead of Amazon SQS.

- C. Change the limit in Amazon SQS to handle messages that are larger than 256 KB.

- D. Store messages that are larger than 256 KB in Amazon Elastic File System (Amazon EFS). Configure Amazon SQS to reference this location in the messages.

**Correct:** A
**Why:** SQS Extended Client Library stores message payloads in S3 to overcome the 256 KB limit up to 2–50 MB.

**Incorrect:**
- B: EventBridge isn’t a queue; SQS cannot raise native limit; EFS indirection isn’t integrated.
- C: EventBridge isn’t a queue; SQS cannot raise native limit; EFS indirection isn’t integrated.
- D: EventBridge isn’t a queue; SQS cannot raise native limit; EFS indirection isn’t integrated.


---

---

### Question #357

A gaming company is moving its public scoreboard from a data center to the AWS Cloud. The company uses Amazon EC2 Windows Server instances behind an Application Load Balancer to host its dynamic application. The company needs a highly available storage solution for the application. The application consists of static files and dynamic server-side code. Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)

- A. Store the static files on Amazon S3. Use Amazon CloudFront to cache objects at the edge.

- B. Store the static files on Amazon S3. Use Amazon ElastiCache to cache objects at the edge.

- C. Store the server-side code on Amazon Elastic File System (Amazon EFS). Mount the EFS volume on each EC2 instance to share the files.

- D. Store the server-side code on Amazon FSx for Windows File Server. Mount the FSx for Windows File Server volume on each EC2 instance to share the files.

E. Store the server-side code on a General Purpose SSD (gp2) Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume on each EC2 instance to share the files.

**Correct:** A, D
**Why:** Store static files in S3 and cache with CloudFront; use FSx for Windows File Server to share server‑side code across Windows EC2 instances.

**Incorrect:**
- B: ElastiCache is not an edge CDN for static files.
- C: EFS is POSIX/NFS (Linux), not ideal for Windows.
- E: EBS volumes are single‑instance; not shareable across instances.


---

---

### Question #384

A company runs an application on Amazon EC2 Linux instances across multiple Availability Zones. The application needs a storage layer that is highly available and Portable Operating System Interface (POSIX)-compliant. The storage layer must provide maximum data durability and must be shareable across the EC2 instances. The data in the storage layer will be accessed frequently for the first 30 days and will be accessed infrequently after that time. Which solution will meet these requirements MOST cost-effectively?

- A. Use the Amazon S3 Standard storage class. Create an S3 Lifecycle policy to move infrequently accessed data to S3 Glacier.

- B. Use the Amazon S3 Standard storage class. Create an S3 Lifecycle policy to move infrequently accessed data to S3 Standard-Infrequent Access (S3 Standard-IA).

- C. Use the Amazon Elastic File System (Amazon EFS) Standard storage class. Create a lifecycle management policy to move infrequently accessed data to EFS Standard-Infrequent Access (EFS Standard-IA).

- D. Use the Amazon Elastic File System (Amazon EFS) One Zone storage class. Create a lifecycle management policy to move infrequently accessed data to EFS One Zone-Infrequent Access (EFS One Zone-IA).

**Correct:** C
**Why:** EFS provides shared, POSIX‑compliant, highly available storage; lifecycle moves infrequently accessed data to EFS Standard‑IA to save cost.

**Incorrect:**
- A: S3 is not POSIX‑compliant and not a shared filesystem for EC2.
- B: S3 is not POSIX‑compliant and not a shared filesystem for EC2.
- D: One Zone is not as highly available as required.


---

---

### Question #407

A company is implementing a shared storage solution for a gaming application that is hosted in the AWS Cloud. The company needs the ability to use Lustre clients to access data. The solution must be fully managed. Which solution meets these requirements?

- A. Create an AWS DataSync task that shares the data as a mountable file system. Mount the file system to the application server.

- B. Create an AWS Storage Gateway file gateway. Create a file share that uses the required client protocol. Connect the application server to the file share.

- C. Create an Amazon Elastic File System (Amazon EFS) file system, and configure it to support Lustre. Attach the file system to the origin server. Connect the application server to the file system.

- D. Create an Amazon FSx for Lustre file system. Attach the file system to the origin server. Connect the application server to the file system.

**Correct:** D
**Why:** Amazon FSx for Lustre is the fully managed Lustre file system compatible with Lustre clients.

**Incorrect:**
- A: DataSync, File Gateway, and EFS are not Lustre file systems.
- B: DataSync, File Gateway, and EFS are not Lustre file systems.
- C: DataSync, File Gateway, and EFS are not Lustre file systems.


---

---

### Question #409

A solutions architect must migrate a Windows Internet Information Services (IIS) web application to AWS. The application currently relies on a file share hosted in the user's on-premises network-attached storage (NAS). The solutions architect has proposed migrating the IIS web servers to Amazon EC2 instances in multiple Availability Zones that are connected to the storage solution, and conguring an Elastic Load Balancer attached to the instances. Which replacement to the on-premises file share is MOST resilient and durable?

- A. Migrate the file share to Amazon RDS.

- B. Migrate the file share to AWS Storage Gateway.

- C. Migrate the file share to Amazon FSx for Windows File Server.

- D. Migrate the file share to Amazon Elastic File System (Amazon EFS).

**Correct:** C
**Why:** FSx for Windows File Server provides fully managed SMB shares for Windows IIS with HA/durability.

**Incorrect:**
- A: RDS is a database, not a file share.
- B: Storage Gateway is for hybrid access, not the most resilient/durable for primary shared storage.
- D: EFS is NFS (Linux), not SMB.


---

---

### Question #421

A company runs a highly available SFTP service. The SFTP service uses two Amazon EC2 Linux instances that run with elastic IP addresses to accept trac from trusted IP sources on the internet. The SFTP service is backed by shared storage that is attached to the instances. User accounts are created and managed as Linux users in the SFTP servers. The company wants a serverless option that provides high IOPS performance and highly congurable security. The company also wants to maintain control over user permissions. Which solution will meet these requirements?

- A. Create an encrypted Amazon Elastic Block Store (Amazon EBS) volume. Create an AWS Transfer Family SFTP service with a public endpoint that allows only trusted IP addresses. Attach the EBS volume to the SFTP service endpoint. Grant users access to the SFTP service.

- B. Create an encrypted Amazon Elastic File System (Amazon EFS) volume. Create an AWS Transfer Family SFTP service with elastic IP addresses and a VPC endpoint that has internet-facing access. Attach a security group to the endpoint that allows only trusted IP addresses. Attach the EFS volume to the SFTP service endpoint. Grant users access to the SFTP service.

- C. Create an Amazon S3 bucket with default encryption enabled. Create an AWS Transfer Family SFTP service with a public endpoint that allows only trusted IP addresses. Attach the S3 bucket to the SFTP service endpoint. Grant users access to the SFTP service.

- D. Create an Amazon S3 bucket with default encryption enabled. Create an AWS Transfer Family SFTP service with a VPC endpoint that has internal access in a private subnet. Attach a security group that allows only trusted IP addresses. Attach the S3 bucket to the SFTP service endpoint. Grant users access to the SFTP service.

**Correct:** C
**Why:** Transfer Family with an S3 backend is serverless, scalable, and supports IP allow lists; S3 delivers high throughput/IOPS.

**Incorrect:**
- A: You cannot attach EBS/EFS directly to Transfer Family.
- B: You cannot attach EBS/EFS directly to Transfer Family.
- D: A private VPC endpoint is internal; trusted public IPs could not reach it.


---

---

### Question #475

A company is designing a containerized application that will use Amazon Elastic Container Service (Amazon ECS). The application needs to access a shared file system that is highly durable and can recover data to another AWS Region with a recovery point objective (RPO) of 8 hours. The file system needs to provide a mount target m each Availability Zone within a Region. A solutions architect wants to use AWS Backup to manage the replication to another Region. Which solution will meet these requirements?

- A. Amazon FSx for Windows File Server with a Multi-AZ deployment

- B. Amazon FSx for NetApp ONTAP with a Multi-AZ deployment

- C. Amazon Elastic File System (Amazon EFS) with the Standard storage class

- D. Amazon FSx for OpenZFS

**Correct:** C
**Why:** EFS provides regional, multi‑AZ mount targets and integrates with AWS Backup for cross‑Region backups (RPO control).

**Incorrect:**
- A: FSx options have different characteristics and restore/RPO models.
- B: FSx options have different characteristics and restore/RPO models.
- D: FSx options have different characteristics and restore/RPO models.


---

---

### Question #487

A company seeks a storage solution for its application. The solution must be highly available and scalable. The solution also must function as a file system be mountable by multiple Linux instances in AWS and on premises through native protocols, and have no minimum size requirements. The company has set up a Site-to-Site VPN for access from its on-premises network to its VPC. Which storage solution meets these requirements?

- A. Amazon FSx Multi-AZ deployments

- B. Amazon Elastic Block Store (Amazon EBS) Multi-Attach volumes

- C. Amazon Elastic File System (Amazon EFS) with multiple mount targets

- D. Amazon Elastic File System (Amazon EFS) with a single mount target and multiple access points

**Correct:** C
**Why:** EFS is a scalable, highly available NFS file system with multiple mount targets; accessible from on‑prem via VPN.

**Incorrect:**
- A: FSx options/EBS don’t match NFS multi‑attach across many clients.
- B: FSx options/EBS don’t match NFS multi‑attach across many clients.
- D: FSx options/EBS don’t match NFS multi‑attach across many clients.


---

---

### Question #496

A company uses on-premises servers to host its applications. The company is running out of storage capacity. The applications use both block storage and NFS storage. The company needs a high-performing solution that supports local caching without re-architecting its existing applications. Which combination of actions should a solutions architect take to meet these requirements? (Choose two.)

- A. Mount Amazon S3 as a file system to the on-premises servers.

- B. Deploy an AWS Storage Gateway file gateway to replace NFS storage.

- C. Deploy AWS Snowball Edge to provision NFS mounts to on-premises servers.

- D. Deploy an AWS Storage Gateway volume gateway to replace the block storage.

E. Deploy Amazon Elastic File System (Amazon EFS) volumes and mount them to on-premises servers.

**Correct:** B, D
**Why:** File Gateway replaces NFS with local cache; Volume Gateway replaces block storage with cached volumes—no app re‑architecture.

**Incorrect:**
- A: S3 is not mounted natively.
- C: Snowball Edge is for migration, not ongoing storage.
- E: EFS is NFS in AWS, not a cached on‑prem replacement.


---

## Amazon FSx

### Question #260

A company’s compliance team needs to move its file shares to AWS. The shares run on a Windows Server SMB file share. A self-managed on- premises Active Directory controls access to the files and folders. The company wants to use Amazon FSx for Windows File Server as part of the solution. The company must ensure that the on-premises Active Directory groups restrict access to the FSx for Windows File Server SMB compliance shares, folders, and files after the move to AWS. The company has created an FSx for Windows File Server file system. Which solution will meet these requirements?

- A. Create an Active Directory Connector to connect to the Active Directory. Map the Active Directory groups to IAM groups to restrict access.

- B. Assign a tag with a Restrict tag key and a Compliance tag value. Map the Active Directory groups to IAM groups to restrict access.

- C. Create an IAM service-linked role that is linked directly to FSx for Windows File Server to restrict access.

- D. Join the file system to the Active Directory to restrict access.

**Correct:** D
**Why:** Join FSx for Windows File Server to on‑prem AD to continue using existing AD groups/NTFS ACLs.

**Incorrect:**
- A: AD Connector/tags/service‑linked roles don’t enforce SMB file permissions.
- B: AD Connector/tags/service‑linked roles don’t enforce SMB file permissions.
- C: AD Connector/tags/service‑linked roles don’t enforce SMB file permissions.


---

---

### Question #283

A research company runs experiments that are powered by a simulation application and a visualization application. The simulation application runs on Linux and outputs intermediate data to an NFS share every 5 minutes. The visualization application is a Windows desktop application that displays the simulation output and requires an SMB file system. The company maintains two synchronized file systems. This strategy is causing data duplication and inecient resource usage. The company needs to migrate the applications to AWS without making code changes to either application. Which solution will meet these requirements?

- A. Migrate both applications to AWS Lambda. Create an Amazon S3 bucket to exchange data between the applications.

- B. Migrate both applications to Amazon Elastic Container Service (Amazon ECS). Configure Amazon FSx File Gateway for storage.

- C. Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization application to Windows EC2 instances. Configure Amazon Simple Queue Service (Amazon SQS) to exchange data between the applications.

- D. Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization application to Windows EC2 instances. Configure Amazon FSx for NetApp ONTAP for storage.

**Correct:** D
**Why:** FSx for NetApp ONTAP supports both NFS and SMB on the same data, removing duplicate storage and maintaining app compatibility.

**Incorrect:**
- A: Lambda/ECS + File Gateway or SQS don’t solve multi‑protocol file sharing.
- B: Lambda/ECS + File Gateway or SQS don’t solve multi‑protocol file sharing.
- C: Lambda/ECS + File Gateway or SQS don’t solve multi‑protocol file sharing.


---

---

### Question #287

A company wants to migrate a Windows-based application from on premises to the AWS Cloud. The application has three tiers: an application tier, a business tier, and a database tier with Microsoft SQL Server. The company wants to use specic features of SQL Server such as native backups and Data Quality Services. The company also needs to share files for processing between the tiers. How should a solutions architect design the architecture to meet these requirements?

- A. Host all three tiers on Amazon EC2 instances. Use Amazon FSx File Gateway for file sharing between the tiers.

- B. Host all three tiers on Amazon EC2 instances. Use Amazon FSx for Windows File Server for file sharing between the tiers.

- C. Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon RDS. Use Amazon Elastic File System (Amazon EFS) for file sharing between the tiers.

- D. Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon RDS. Use a Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) volume for file sharing between the tiers.

**Correct:** B
**Why:** Keep SQL Server features by hosting DB on EC2; use FSx for Windows File Server for shared files; EC2 for app tiers.

**Incorrect:**
- C: RDS may not support required SQL Server features; EBS for file sharing is inappropriate.
- D: RDS may not support required SQL Server features; EBS for file sharing is inappropriate.


---

---

### Question #299

A research laboratory needs to process approximately 8 TB of data. The laboratory requires sub-millisecond latencies and a minimum throughput of 6 GBps for the storage subsystem. Hundreds of Amazon EC2 instances that run Amazon Linux will distribute and process the data. Which solution will meet the performance requirements?

- A. Create an Amazon FSx for NetApp ONTAP file system. Sat each volume’ tiering policy to ALL. Import the raw data into the file system. Mount the la system on the EC2 instances.

- B. Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system that uses persistent SSD storage. Select the option to import data from and export data to Amazon S3. Mount the file system on the EC2 instances.

- C. Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system that uses persistent HDD storage. Select the option to import data from and export data to Amazon S3. Mount the file system on the EC2 instances.

- D. Create an Amazon FSx for NetApp ONTAP file system. Set each volume’s tiering policy to NONE. Import the raw data into the file system. Mount the file system on the EC2 instances.

**Correct:** B
**Why:** FSx for Lustre persistent SSD with S3 integration meets sub‑ms latency and high throughput across many EC2 instances.

**Incorrect:**
- A: NetApp ONTAP tiering/none policies don’t match performance requirements.
- C: HDD tier insufficient for performance.
- D: NetApp ONTAP tiering/none policies don’t match performance requirements.


---

---

### Question #301

A university research laboratory needs to migrate 30 TB of data from an on-premises Windows file server to Amazon FSx for Windows File Server. The laboratory has a 1 Gbps network link that many other departments in the university share. The laboratory wants to implement a data migration service that will maximize the performance of the data transfer. However, the laboratory needs to be able to control the amount of bandwidth that the service uses to minimize the impact on other departments. The data migration must take place within the next 5 days. Which AWS solution will meet these requirements?

- A. AWS Snowcone

- B. Amazon FSx File Gateway

- C. AWS DataSync

- D. AWS Transfer Family

**Correct:** C
**Why:** AWS DataSync supports bandwidth throttling and high‑performance transfer to FSx Windows within the 5‑day window.

**Incorrect:**
- A: Snowcone/FSx File Gateway/Transfer Family are not optimal for bulk controlled‑bandwidth migration.
- B: Snowcone/FSx File Gateway/Transfer Family are not optimal for bulk controlled‑bandwidth migration.
- D: Snowcone/FSx File Gateway/Transfer Family are not optimal for bulk controlled‑bandwidth migration.


---

---

### Question #305

A company is designing a shared storage solution for a gaming application that is hosted in the AWS Cloud. The company needs the ability to use SMB clients to access data. The solution must be fully managed. Which AWS solution meets these requirements?

- A. Create an AWS DataSync task that shares the data as a mountable file system. Mount the file system to the application server.

- B. Create an Amazon EC2 Windows instance. Install and configure a Windows file share role on the instance. Connect the application server to the file share.

- C. Create an Amazon FSx for Windows File Server file system. Attach the file system to the origin server. Connect the application server to the file system.

- D. Create an Amazon S3 bucket. Assign an IAM role to the application to grant access to the S3 bucket. Mount the S3 bucket to the application server.

**Correct:** C
**Why:** FSx for Windows File Server provides managed SMB file shares.

**Incorrect:**
- A: DataSync/Tape/Direct S3 are not SMB file servers.
- B: DataSync/Tape/Direct S3 are not SMB file servers.
- D: DataSync/Tape/Direct S3 are not SMB file servers.


---

---

### Question #332

A company needs to provide its employees with secure access to condential and sensitive files. The company wants to ensure that the files can be accessed only by authorized users. The files must be downloaded securely to the employees’ devices. The files are stored in an on-premises Windows file server. However, due to an increase in remote usage, the file server is running out of capacity. . Which solution will meet these requirements?

- A. Migrate the file server to an Amazon EC2 instance in a public subnet. Configure the security group to limit inbound trac to the employees’ IP addresses.

- B. Migrate the files to an Amazon FSx for Windows File Server file system. Integrate the Amazon FSx file system with the on-premises Active Directory. Configure AWS Client VPN.

- C. Migrate the files to Amazon S3, and create a private VPC endpoint. Create a signed URL to allow download.

- D. Migrate the files to Amazon S3, and create a public VPC endpoint. Allow employees to sign on with AWS IAM Identity Center (AWS Single Sign-On).

**Correct:** B
**Why:** FSx for Windows with on‑prem AD integration preserves permissions; Client VPN provides secure remote access and downloads.

**Incorrect:**
- A: Public EC2, S3 signed URLs, or public endpoints don’t integrate with AD or meet security requirements.
- C: Public EC2, S3 signed URLs, or public endpoints don’t integrate with AD or meet security requirements.
- D: Public EC2, S3 signed URLs, or public endpoints don’t integrate with AD or meet security requirements.


---

---

### Question #346

A company has an aging network-attached storage (NAS) array in its data center. The NAS array presents SMB shares and NFS shares to client workstations. The company does not want to purchase a new NAS array. The company also does not want to incur the cost of renewing the NAS array’s support contract. Some of the data is accessed frequently, but much of the data is inactive. A solutions architect needs to implement a solution that migrates the data to Amazon S3, uses S3 Lifecycle policies, and maintains the same look and feel for the client workstations. The solutions architect has identied AWS Storage Gateway as part of the solution. Which type of storage gateway should the solutions architect provision to meet these requirements?

- A. Volume Gateway

- B. Tape Gateway

- C. Amazon FSx File Gateway

- D. Amazon S3 File Gateway

**Correct:** D
**Why:** S3 File Gateway presents SMB/NFS backed by S3, enables lifecycle policies, and preserves client experience.

**Incorrect:**
- A: Volume/Tape/FSx File Gateway (nonexistent) aren’t appropriate.
- B: Volume/Tape/FSx File Gateway (nonexistent) aren’t appropriate.
- C: Volume/Tape/FSx File Gateway (nonexistent) aren’t appropriate.


---

---

### Question #357

A gaming company is moving its public scoreboard from a data center to the AWS Cloud. The company uses Amazon EC2 Windows Server instances behind an Application Load Balancer to host its dynamic application. The company needs a highly available storage solution for the application. The application consists of static files and dynamic server-side code. Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)

- A. Store the static files on Amazon S3. Use Amazon CloudFront to cache objects at the edge.

- B. Store the static files on Amazon S3. Use Amazon ElastiCache to cache objects at the edge.

- C. Store the server-side code on Amazon Elastic File System (Amazon EFS). Mount the EFS volume on each EC2 instance to share the files.

- D. Store the server-side code on Amazon FSx for Windows File Server. Mount the FSx for Windows File Server volume on each EC2 instance to share the files.

E. Store the server-side code on a General Purpose SSD (gp2) Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume on each EC2 instance to share the files.

**Correct:** A, D
**Why:** Store static files in S3 and cache with CloudFront; use FSx for Windows File Server to share server‑side code across Windows EC2 instances.

**Incorrect:**
- B: ElastiCache is not an edge CDN for static files.
- C: EFS is POSIX/NFS (Linux), not ideal for Windows.
- E: EBS volumes are single‑instance; not shareable across instances.


---

---

### Question #407

A company is implementing a shared storage solution for a gaming application that is hosted in the AWS Cloud. The company needs the ability to use Lustre clients to access data. The solution must be fully managed. Which solution meets these requirements?

- A. Create an AWS DataSync task that shares the data as a mountable file system. Mount the file system to the application server.

- B. Create an AWS Storage Gateway file gateway. Create a file share that uses the required client protocol. Connect the application server to the file share.

- C. Create an Amazon Elastic File System (Amazon EFS) file system, and configure it to support Lustre. Attach the file system to the origin server. Connect the application server to the file system.

- D. Create an Amazon FSx for Lustre file system. Attach the file system to the origin server. Connect the application server to the file system.

**Correct:** D
**Why:** Amazon FSx for Lustre is the fully managed Lustre file system compatible with Lustre clients.

**Incorrect:**
- A: DataSync, File Gateway, and EFS are not Lustre file systems.
- B: DataSync, File Gateway, and EFS are not Lustre file systems.
- C: DataSync, File Gateway, and EFS are not Lustre file systems.


---

---

### Question #409

A solutions architect must migrate a Windows Internet Information Services (IIS) web application to AWS. The application currently relies on a file share hosted in the user's on-premises network-attached storage (NAS). The solutions architect has proposed migrating the IIS web servers to Amazon EC2 instances in multiple Availability Zones that are connected to the storage solution, and conguring an Elastic Load Balancer attached to the instances. Which replacement to the on-premises file share is MOST resilient and durable?

- A. Migrate the file share to Amazon RDS.

- B. Migrate the file share to AWS Storage Gateway.

- C. Migrate the file share to Amazon FSx for Windows File Server.

- D. Migrate the file share to Amazon Elastic File System (Amazon EFS).

**Correct:** C
**Why:** FSx for Windows File Server provides fully managed SMB shares for Windows IIS with HA/durability.

**Incorrect:**
- A: RDS is a database, not a file share.
- B: Storage Gateway is for hybrid access, not the most resilient/durable for primary shared storage.
- D: EFS is NFS (Linux), not SMB.


---

---

### Question #475

A company is designing a containerized application that will use Amazon Elastic Container Service (Amazon ECS). The application needs to access a shared file system that is highly durable and can recover data to another AWS Region with a recovery point objective (RPO) of 8 hours. The file system needs to provide a mount target m each Availability Zone within a Region. A solutions architect wants to use AWS Backup to manage the replication to another Region. Which solution will meet these requirements?

- A. Amazon FSx for Windows File Server with a Multi-AZ deployment

- B. Amazon FSx for NetApp ONTAP with a Multi-AZ deployment

- C. Amazon Elastic File System (Amazon EFS) with the Standard storage class

- D. Amazon FSx for OpenZFS

**Correct:** C
**Why:** EFS provides regional, multi‑AZ mount targets and integrates with AWS Backup for cross‑Region backups (RPO control).

**Incorrect:**
- A: FSx options have different characteristics and restore/RPO models.
- B: FSx options have different characteristics and restore/RPO models.
- D: FSx options have different characteristics and restore/RPO models.


---

---

### Question #487

A company seeks a storage solution for its application. The solution must be highly available and scalable. The solution also must function as a file system be mountable by multiple Linux instances in AWS and on premises through native protocols, and have no minimum size requirements. The company has set up a Site-to-Site VPN for access from its on-premises network to its VPC. Which storage solution meets these requirements?

- A. Amazon FSx Multi-AZ deployments

- B. Amazon Elastic Block Store (Amazon EBS) Multi-Attach volumes

- C. Amazon Elastic File System (Amazon EFS) with multiple mount targets

- D. Amazon Elastic File System (Amazon EFS) with a single mount target and multiple access points

**Correct:** C
**Why:** EFS is a scalable, highly available NFS file system with multiple mount targets; accessible from on‑prem via VPN.

**Incorrect:**
- A: FSx options/EBS don’t match NFS multi‑attach across many clients.
- B: FSx options/EBS don’t match NFS multi‑attach across many clients.
- D: FSx options/EBS don’t match NFS multi‑attach across many clients.


---

---

### Question #500

A company has multiple Windows file servers on premises. The company wants to migrate and consolidate its files into an Amazon FSx for Windows File Server file system. File permissions must be preserved to ensure that access rights do not change. Which solutions will meet these requirements? (Choose two.)

- A. Deploy AWS DataSync agents on premises. Schedule DataSync tasks to transfer the data to the FSx for Windows File Server file system.

- B. Copy the shares on each file server into Amazon S3 buckets by using the AWS CLI. Schedule AWS DataSync tasks to transfer the data to the FSx for Windows File Server file system.

- C. Remove the drives from each file server. Ship the drives to AWS for import into Amazon S3. Schedule AWS DataSync tasks to transfer the data to the FSx for Windows File Server file system.

- D. Order an AWS Snowcone device. Connect the device to the on-premises network. Launch AWS DataSync agents on the device. Schedule DataSync tasks to transfer the data to the FSx for Windows File Server file system.

E. Order an AWS Snowball Edge Storage Optimized device. Connect the device to the on-premises network. Copy data to the device by using the AWS CLI. Ship the device back to AWS for import into Amazon S3. Schedule AWS DataSync tasks to transfer the data to the FSx for Windows File Server file system.

**Correct:** A, E
**Why:** DataSync agents preserve metadata/ACLs when migrating to FSx for Windows; for large data sets, Snowball Edge to S3 plus DataSync to FSx is efficient.

**Incorrect:**
- B: Copying to S3 with CLI loses Windows ACLs/metadata.
- C: Drives/Snowcone are impractical for large multi‑server migrations.
- D: Drives/Snowcone are impractical for large multi‑server migrations.


---

## Amazon Kinesis

### Question #255

A company has an ecommerce checkout workow that writes an order to a database and calls a service to process the payment. Users are experiencing timeouts during the checkout process. When users resubmit the checkout form, multiple unique orders are created for the same desired transaction. How should a solutions architect refactor this workow to prevent the creation of multiple orders?

- A. Configure the web application to send an order message to Amazon Kinesis Data Firehose. Set the payment service to retrieve the message from Kinesis Data Firehose and process the order.

- B. Create a rule in AWS CloudTrail to invoke an AWS Lambda function based on the logged application path request. Use Lambda to query the database, call the payment service, and pass in the order information.

- C. Store the order in the database. Send a message that includes the order number to Amazon Simple Notification Service (Amazon SNS). Set the payment service to poll Amazon SNS, retrieve the message, and process the order.

- D. Store the order in the database. Send a message that includes the order number to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the payment service to retrieve the message and process the order. Delete the message from the queue.

**Correct:** D
**Why:** Use an SQS FIFO queue with content-based deduplication to ensure exactly-once processing and preserve ordering.

**Incorrect:**
- A: Kinesis/SNS do not provide FIFO message de‑duplication semantics for this flow.
- B: Kinesis/SNS do not provide FIFO message de‑duplication semantics for this flow.
- C: Kinesis/SNS do not provide FIFO message de‑duplication semantics for this flow.


---

---

### Question #257

A company is building a solution that will report Amazon EC2 Auto Scaling events across all the applications in an AWS account. The company needs to use a serverless solution to store the EC2 Auto Scaling status data in Amazon S3. The company then will use the data in Amazon S3 to provide near-real-time updates in a dashboard. The solution must not affect the speed of EC2 instance launches. How should the company move the data to Amazon S3 to meet these requirements?

- A. Use an Amazon CloudWatch metric stream to send the EC2 Auto Scaling status data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.

- B. Launch an Amazon EMR cluster to collect the EC2 Auto Scaling status data and send the data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.

- C. Create an Amazon EventBridge rule to invoke an AWS Lambda function on a schedule. Configure the Lambda function to send the EC2 Auto Scaling status data directly to Amazon S3.

- D. Use a bootstrap script during the launch of an EC2 instance to install Amazon Kinesis Agent. Configure Kinesis Agent to collect the EC2 Auto Scaling status data and send the data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.

**Correct:** A
**Why:** CloudWatch metric streams → Kinesis Data Firehose → S3 is serverless and near real time without impacting launches.

**Incorrect:**
- B: EMR/Kinesis Agent add unnecessary infrastructure.
- C: Scheduled polling adds latency and complexity.
- D: EMR/Kinesis Agent add unnecessary infrastructure.


---

---

### Question #267

A company has one million users that use its mobile app. The company must analyze the data usage in near-real time. The company also must encrypt the data in near-real time and must store the data in a centralized location in Apache Parquet format for further processing. Which solution will meet these requirements with the LEAST operational overhead?

- A. Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon Kinesis Data Analytics application to analyze the data. Invoke an AWS Lambda function to send the data to the Kinesis Data Analytics application.

- B. Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon EMR cluster to analyze the data. Invoke an AWS Lambda function to send the data to the EMR cluster.

- C. Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create an Amazon EMR cluster to analyze the data.

- D. Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create an Amazon Kinesis Data Analytics application to analyze the data.

**Correct:** A
**Why:** Kinesis Data Streams → Kinesis Data Analytics for near real‑time analytics, Firehose to S3 with Parquet conversion; Athena for SQL.

**Incorrect:**
- B: Do not provide both real‑time analytics and Parquet delivery with least ops as cleanly.
- C: Do not provide both real‑time analytics and Parquet delivery with least ops as cleanly.
- D: Do not provide both real‑time analytics and Parquet delivery with least ops as cleanly.


---

---

### Question #292

A company is preparing a new data platform that will ingest real-time streaming data from multiple sources. The company needs to transform the data before writing the data to Amazon S3. The company needs the ability to use SQL to query the transformed data. Which solutions will meet these requirements? (Choose two.)

- A. Use Amazon Kinesis Data Streams to stream the data. Use Amazon Kinesis Data Analytics to transform the data. Use Amazon Kinesis Data Firehose to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.

- B. Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use AWS Glue to transform the data and to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.

- C. Use AWS Database Migration Service (AWS DMS) to ingest the data. Use Amazon EMR to transform the data and to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.

- D. Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use Amazon Kinesis Data Analytics to transform the data and to write the data to Amazon S3. Use the Amazon RDS query editor to query the transformed data from Amazon S3.

E. Use Amazon Kinesis Data Streams to stream the data. Use AWS Glue to transform the data. Use Amazon Kinesis Data Firehose to write the data to Amazon S3. Use the Amazon RDS query editor to query the transformed data from Amazon S3.

**Correct:** A, B
**Why:** Kinesis Data Streams + KDA + Firehose + S3 + Athena, or MSK + Glue + S3 + Athena both satisfy transform to S3 and SQL on data.

**Incorrect:**
- C: DMS/Incorrect query endpoints do not meet requirements fully.
- D: DMS/Incorrect query endpoints do not meet requirements fully.
- E: DMS/Incorrect query endpoints do not meet requirements fully.


---

---

### Question #311

A company is using AWS to design a web application that will process insurance quotes. Users will request quotes from the application. Quotes must be separated by quote type, must be responded to within 24 hours, and must not get lost. The solution must maximize operational eciency and must minimize maintenance. Which solution meets these requirements?

- A. Create multiple Amazon Kinesis data streams based on the quote type. Configure the web application to send messages to the proper data stream. Configure each backend group of application servers to use the Kinesis Client Library (KCL) to pool messages from its own data stream.

- B. Create an AWS Lambda function and an Amazon Simple Notification Service (Amazon SNS) topic for each quote type. Subscribe the Lambda function to its associated SNS topic. Configure the application to publish requests for quotes to the appropriate SNS topic.

- C. Create a single Amazon Simple Notification Service (Amazon SNS) topic. Subscribe Amazon Simple Queue Service (Amazon SQS) queues to the SNS topic. Configure SNS message filtering to publish messages to the proper SQS queue based on the quote type. Configure each backend application server to use its own SQS queue.

- D. Create multiple Amazon Kinesis Data Firehose delivery streams based on the quote type to deliver data streams to an Amazon OpenSearch Service cluster. Configure the application to send messages to the proper delivery stream. Configure each backend group of application servers to search for the messages from OpenSearch Service and process them accordingly.

**Correct:** C
**Why:** SNS to SQS with message filtering separates quote types; SQS ensures durability (no loss) and 24‑hour processing window.

**Incorrect:**
- A: Kinesis or Firehose/OpenSearch are not needed for durable request queues.
- B: SNS + Lambda per type adds code and lacks durable backlog.
- D: Kinesis or Firehose/OpenSearch are not needed for durable request queues.


---

---

### Question #320

A company is using a eet of Amazon EC2 instances to ingest data from on-premises data sources. The data is in JSON format and ingestion rates can be as high as 1 MB/s. When an EC2 instance is rebooted, the data in-ight is lost. The company’s data science team wants to query ingested data in near-real time. Which solution provides near-real-time data querying that is scalable with minimal data loss?

- A. Publish data to Amazon Kinesis Data Streams, Use Kinesis Data Analytics to query the data.

- B. Publish data to Amazon Kinesis Data Firehose with Amazon Redshift as the destination. Use Amazon Redshift to query the data.

- C. Store ingested data in an EC2 instance store. Publish data to Amazon Kinesis Data Firehose with Amazon S3 as the destination. Use Amazon Athena to query the data.

- D. Store ingested data in an Amazon Elastic Block Store (Amazon EBS) volume. Publish data to Amazon ElastiCache for Redis. Subscribe to the Redis channel to query the data.

**Correct:** A
**Why:** Kinesis Data Streams with Kinesis Data Analytics provides near‑real‑time querying with minimal loss and scalability.

**Incorrect:**
- B: Firehose→Redshift has more latency; instance store/EBS risk data loss; Redis pub/sub isn’t durable.
- C: Firehose→Redshift has more latency; instance store/EBS risk data loss; Redis pub/sub isn’t durable.
- D: Firehose→Redshift has more latency; instance store/EBS risk data loss; Redis pub/sub isn’t durable.


---

---

### Question #361

A company hosts a multiplayer gaming application on AWS. The company wants the application to read data with sub-millisecond latency and run one-time queries on historical data. Which solution will meet these requirements with the LEAST operational overhead?

- A. Use Amazon RDS for data that is frequently accessed. Run a periodic custom script to export the data to an Amazon S3 bucket.

- B. Store the data directly in an Amazon S3 bucket. Implement an S3 Lifecycle policy to move older data to S3 Glacier Deep Archive for long- term storage. Run one-time queries on the data in Amazon S3 by using Amazon Athena.

- C. Use Amazon DynamoDB with DynamoDB Accelerator (DAX) for data that is frequently accessed. Export the data to an Amazon S3 bucket by using DynamoDB table export. Run one-time queries on the data in Amazon S3 by using Amazon Athena.

- D. Use Amazon DynamoDB for data that is frequently accessed. Turn on streaming to Amazon Kinesis Data Streams. Use Amazon Kinesis Data Firehose to read the data from Kinesis Data Streams. Store the records in an Amazon S3 bucket.

**Correct:** C
**Why:** DynamoDB + DAX provides sub‑millisecond reads; export to S3 and query ad hoc with Athena for historical analysis with low ops.

**Incorrect:**
- A: Adds more moving parts and does not give sub‑ms reads as simply as DAX.
- B: S3 alone lacks sub‑ms latency for frequently accessed data.
- D: Adds more moving parts and does not give sub‑ms reads as simply as DAX.


---

---

### Question #362

A company uses a payment processing system that requires messages for a particular payment ID to be received in the same order that they were sent. Otherwise, the payments might be processed incorrectly. Which actions should a solutions architect take to meet this requirement? (Choose two.)

- A. Write the messages to an Amazon DynamoDB table with the payment ID as the partition key.

- B. Write the messages to an Amazon Kinesis data stream with the payment ID as the partition key.

- C. Write the messages to an Amazon ElastiCache for Memcached cluster with the payment ID as the key.

- D. Write the messages to an Amazon Simple Queue Service (Amazon SQS) queue. Set the message attribute to use the payment ID.

E. Write the messages to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the message group to use the payment ID.

**Correct:** B, E
**Why:** Kinesis shards preserve ordering by partition key; SQS FIFO preserves message order per message group.

**Incorrect:**
- A: DynamoDB does not enforce arrival/processing order.
- C: ElastiCache is not a message bus with ordering guarantees.
- D: Standard SQS queues do not guarantee ordering.


---

---

### Question #373

A company has an application that collects data from IoT sensors on automobiles. The data is streamed and stored in Amazon S3 through Amazon Kinesis Data Firehose. The data produces trillions of S3 objects each year. Each morning, the company uses the data from the previous 30 days to retrain a suite of machine learning (ML) models. Four times each year, the company uses the data from the previous 12 months to perform analysis and train other ML models. The data must be available with minimal delay for up to 1 year. After 1 year, the data must be retained for archival purposes. Which storage solution meets these requirements MOST cost-effectively?

- A. Use the S3 Intelligent-Tiering storage class. Create an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 1 year.

- B. Use the S3 Intelligent-Tiering storage class. Configure S3 Intelligent-Tiering to automatically move objects to S3 Glacier Deep Archive after 1 year.

- C. Use the S3 Standard-Infrequent Access (S3 Standard-IA) storage class. Create an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 1 year.

- D. Use the S3 Standard storage class. Create an S3 Lifecycle policy to transition objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days, and then to S3 Glacier Deep Archive after 1 year.

**Correct:** A
**Why:** S3 Intelligent‑Tiering optimizes cost for unknown/variable access up to 1 year; lifecycle to Glacier Deep Archive after 1 year minimizes cost for archival.

**Incorrect:**
- B: Intelligent‑Tiering does not auto‑move to Deep Archive; use lifecycle.
- C: Less optimal than Intelligent‑Tiering for highly variable access with trillions of objects.
- D: Less optimal than Intelligent‑Tiering for highly variable access with trillions of objects.


---

---

### Question #386

An ecommerce company is running a multi-tier application on AWS. The front-end and backend tiers both run on Amazon EC2, and the database runs on Amazon RDS for MySQL. The backend tier communicates with the RDS instance. There are frequent calls to return identical datasets from the database that are causing performance slowdowns. Which action should be taken to improve the performance of the backend?

- A. Implement Amazon SNS to store the database calls.

- B. Implement Amazon ElastiCache to cache the large datasets.

- C. Implement an RDS for MySQL read replica to cache database calls.

- D. Implement Amazon Kinesis Data Firehose to stream the calls to the database.

**Correct:** B
**Why:** ElastiCache (Redis/Memcached) caches repeated reads to reduce database load and improve performance.

**Incorrect:**
- A: SNS is a pub/sub service, not a cache.
- C: Read replicas help, but cache is better for repetitive identical datasets and lower latency.
- D: Kinesis Firehose is for data streaming, not query caching.


---

---

### Question #393

A payment processing company records all voice communication with its customers and stores the audio files in an Amazon S3 bucket. The company needs to capture the text from the audio files. The company must remove from the text any personally identiable information (PII) that belongs to customers. What should a solutions architect do to meet these requirements?

- A. Process the audio files by using Amazon Kinesis Video Streams. Use an AWS Lambda function to scan for known PII patterns.

- B. When an audio file is uploaded to the S3 bucket, invoke an AWS Lambda function to start an Amazon Textract task to analyze the call recordings.

- C. Configure an Amazon Transcribe transcription job with PII redaction turned on. When an audio file is uploaded to the S3 bucket, invoke an AWS Lambda function to start the transcription job. Store the output in a separate S3 bucket.

- D. Create an Amazon Connect contact flow that ingests the audio files with transcription turned on. Embed an AWS Lambda function to scan for known PII patterns. Use Amazon EventBridge to start the contact flow when an audio file is uploaded to the S3 bucket.

**Correct:** C
**Why:** Amazon Transcribe supports PII redaction; invoke jobs on upload and store sanitized text output separately.

**Incorrect:**
- A: Not appropriate services for transcription and PII redaction here.
- B: Textract is for documents, not audio.
- D: Not appropriate services for transcription and PII redaction here.


---

---

### Question #402

A company needs to ingest and handle large amounts of streaming data that its application generates. The application runs on Amazon EC2 instances and sends data to Amazon Kinesis Data Streams, which is congured with default settings. Every other day, the application consumes the data and writes the data to an Amazon S3 bucket for business intelligence (BI) processing. The company observes that Amazon S3 is not receiving all the data that the application sends to Kinesis Data Streams. What should a solutions architect do to resolve this issue?

- A. Update the Kinesis Data Streams default settings by modifying the data retention period.

- B. Update the application to use the Kinesis Producer Library (KPL) to send the data to Kinesis Data Streams.

- C. Update the number of Kinesis shards to handle the throughput of the data that is sent to Kinesis Data Streams.

- D. Turn on S3 Versioning within the S3 bucket to preserve every version of every object that is ingested in the S3 bucket.

**Correct:** A
**Why:** Default Kinesis Data Streams retention is 24 hours; consuming every other day requires a longer retention period.

**Incorrect:**
- B: KPL helps with throughput, not expired records.
- C: More shards won’t recover expired data.
- D: S3 Versioning does not affect Kinesis delivery.


---

---

### Question #489

An ecommerce company runs an application in the AWS Cloud that is integrated with an on-premises warehouse solution. The company uses Amazon Simple Notification Service (Amazon SNS) to send order messages to an on-premises HTTPS endpoint so the warehouse application can process the orders. The local data center team has detected that some of the order messages were not received. A solutions architect needs to retain messages that are not delivered and analyze the messages for up to 14 days. Which solution will meet these requirements with the LEAST development effort?

- A. Configure an Amazon SNS dead letter queue that has an Amazon Kinesis Data Stream target with a retention period of 14 days.

- B. Add an Amazon Simple Queue Service (Amazon SQS) queue with a retention period of 14 days between the application and Amazon SNS.

- C. Configure an Amazon SNS dead letter queue that has an Amazon Simple Queue Service (Amazon SQS) target with a retention period of 14 days.

- D. Configure an Amazon SNS dead letter queue that has an Amazon DynamoDB target with a TTL attribute set for a retention period of 14 days.

**Correct:** C
**Why:** Configure an SNS dead‑letter queue with SQS target and 14‑day retention to retain and analyze undelivered messages with minimal dev.

**Incorrect:**
- A: Kinesis/DynamoDB targets add complexity.
- B: Adding SQS in front of SNS changes the design.
- D: Kinesis/DynamoDB targets add complexity.


---

## Amazon Macie / Rekognition / Comprehend

### Question #278

A company wants to create an application to store employee data in a hierarchical structured relationship. The company needs a minimum-latency response to high-trac queries for the employee data and must protect any sensitive data. The company also needs to receive monthly email messages if any nancial information is present in the employee data. Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)

- A. Use Amazon Redshift to store the employee data in hierarchies. Unload the data to Amazon S3 every month.

- B. Use Amazon DynamoDB to store the employee data in hierarchies. Export the data to Amazon S3 every month.

- C. Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly events to AWS Lambda.

- D. Use Amazon Athena to analyze the employee data in Amazon S3. Integrate Athena with Amazon QuickSight to publish analysis dashboards and share the dashboards with users.

E. Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly notications through an Amazon Simple Notification Service (Amazon SNS) subscription.

**Correct:** B, E
**Why:** DynamoDB suits hierarchical, low‑latency queries; Macie + EventBridge → SNS can notify monthly when financial data is detected.

**Incorrect:**
- A: Redshift/Athena aren’t optimized for low‑latency hierarchical queries.
- C: Lambda trigger pattern is unnecessary for monthly notifications.
- D: Redshift/Athena aren’t optimized for low‑latency hierarchical queries.


---

---

### Question #315

A company experienced a breach that affected several applications in its on-premises data center. The attacker took advantage of vulnerabilities in the custom applications that were running on the servers. The company is now migrating its applications to run on Amazon EC2 instances. The company wants to implement a solution that actively scans for vulnerabilities on the EC2 instances and sends a report that details the ndings. Which solution will meet these requirements?

- A. Deploy AWS Shield to scan the EC2 instances for vulnerabilities. Create an AWS Lambda function to log any ndings to AWS CloudTrail.

- B. Deploy Amazon Macie and AWS Lambda functions to scan the EC2 instances for vulnerabilities. Log any ndings to AWS CloudTrail.

- C. Turn on Amazon GuardDuty. Deploy the GuardDuty agents to the EC2 instances. Configure an AWS Lambda function to automate the generation and distribution of reports that detail the ndings.

- D. Turn on Amazon Inspector. Deploy the Amazon Inspector agent to the EC2 instances. Configure an AWS Lambda function to automate the generation and distribution of reports that detail the ndings.

**Correct:** D
**Why:** Amazon Inspector scans EC2 for vulnerabilities; use Lambda/automation to generate and distribute reports.

**Incorrect:**
- A: Shield/Macie/GuardDuty are not host vulnerability scanners.
- B: Shield/Macie/GuardDuty are not host vulnerability scanners.
- C: Shield/Macie/GuardDuty are not host vulnerability scanners.


---

---

### Question #329

A security audit reveals that Amazon EC2 instances are not being patched regularly. A solutions architect needs to provide a solution that will run regular security scans across a large eet of EC2 instances. The solution should also patch the EC2 instances on a regular schedule and provide a report of each instance’s patch status. Which solution will meet these requirements?

- A. Set up Amazon Macie to scan the EC2 instances for software vulnerabilities. Set up a cron job on each EC2 instance to patch the instance on a regular schedule.

- B. Turn on Amazon GuardDuty in the account. Configure GuardDuty to scan the EC2 instances for software vulnerabilities. Set up AWS Systems Manager Session Manager to patch the EC2 instances on a regular schedule.

- C. Set up Amazon Detective to scan the EC2 instances for software vulnerabilities. Set up an Amazon EventBridge scheduled rule to patch the EC2 instances on a regular schedule.

- D. Turn on Amazon Inspector in the account. Configure Amazon Inspector to scan the EC2 instances for software vulnerabilities. Set up AWS Systems Manager Patch Manager to patch the EC2 instances on a regular schedule.

**Correct:** D
**Why:** Inspector for vulnerability scanning and Systems Manager Patch Manager for scheduled patching with compliance reports.

**Incorrect:**
- A: Macie/GuardDuty/Detective do not patch or scan OS packages.
- B: Macie/GuardDuty/Detective do not patch or scan OS packages.
- C: Macie/GuardDuty/Detective do not patch or scan OS packages.


---

---

### Question #359

A hospital needs to store patient records in an Amazon S3 bucket. The hospital’s compliance team must ensure that all protected health information (PHI) is encrypted in transit and at rest. The compliance team must administer the encryption key for data at rest. Which solution will meet these requirements?

- A. Create a public SSL/TLS certicate in AWS Certicate Manager (ACM). Associate the certicate with Amazon S3. Configure default encryption for each S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS). Assign the compliance team to manage the KMS keys.

- B. Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Configure default encryption for each S3 bucket to use server-side encryption with S3 managed encryption keys (SSE-S3). Assign the compliance team to manage the SSE-S3 keys.

- C. Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Configure default encryption for each S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS). Assign the compliance team to manage the KMS keys.

- D. Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Use Amazon Macie to protect the sensitive data that is stored in Amazon S3. Assign the compliance team to manage Macie.

**Correct:** C
**Why:** Enforce TLS in transit with aws:SecureTransport on the S3 bucket policy; use SSE‑KMS so the compliance team administers the CMK for data at rest.

**Incorrect:**
- A: ACM public certs aren’t attached directly to S3; also KMS administration is needed, not SSE‑S3.
- B: SSE‑S3 keys are managed by AWS, not the compliance team.
- D: Macie discovers PII; it does not fulfill encryption requirements.


---

---

### Question #393

A payment processing company records all voice communication with its customers and stores the audio files in an Amazon S3 bucket. The company needs to capture the text from the audio files. The company must remove from the text any personally identiable information (PII) that belongs to customers. What should a solutions architect do to meet these requirements?

- A. Process the audio files by using Amazon Kinesis Video Streams. Use an AWS Lambda function to scan for known PII patterns.

- B. When an audio file is uploaded to the S3 bucket, invoke an AWS Lambda function to start an Amazon Textract task to analyze the call recordings.

- C. Configure an Amazon Transcribe transcription job with PII redaction turned on. When an audio file is uploaded to the S3 bucket, invoke an AWS Lambda function to start the transcription job. Store the output in a separate S3 bucket.

- D. Create an Amazon Connect contact flow that ingests the audio files with transcription turned on. Embed an AWS Lambda function to scan for known PII patterns. Use Amazon EventBridge to start the contact flow when an audio file is uploaded to the S3 bucket.

**Correct:** C
**Why:** Amazon Transcribe supports PII redaction; invoke jobs on upload and store sanitized text output separately.

**Incorrect:**
- A: Not appropriate services for transcription and PII redaction here.
- B: Textract is for documents, not audio.
- D: Not appropriate services for transcription and PII redaction here.


---

---

### Question #493

A company wants to use articial intelligence (AI) to determine the quality of its customer service calls. The company currently manages calls in four different languages, including English. The company will offer new languages in the future. The company does not have the resources to regularly maintain machine learning (ML) models. The company needs to create written sentiment analysis reports from the customer service call recordings. The customer service call recording text must be translated into English. Which combination of steps will meet these requirements? (Choose three.)

- A. Use Amazon Comprehend to translate the audio recordings into English.

- B. Use Amazon Lex to create the written sentiment analysis reports.

- C. Use Amazon Polly to convert the audio recordings into text.

- D. Use Amazon Transcribe to convert the audio recordings in any language into text.

E. Use Amazon Translate to translate text in any language to English.

F. Use Amazon Comprehend to create the sentiment analysis reports.

**Correct:** D, E, F
**Why:** Transcribe -> text; Translate -> English; Comprehend -> sentiment analysis—no model maintenance required.

**Incorrect:**
- A: Comprehend doesn’t translate audio; Lex/Polly are not for this task.
- B: Comprehend doesn’t translate audio; Lex/Polly are not for this task.
- C: Comprehend doesn’t translate audio; Lex/Polly are not for this task.


---

---

### Question #495

A company is conducting an internal audit. The company wants to ensure that the data in an Amazon S3 bucket that is associated with the company’s AWS Lake Formation data lake does not contain sensitive customer or employee data. The company wants to discover personally identiable information (PII) or nancial information, including passport numbers and credit card numbers. Which solution will meet these requirements?

- A. Configure AWS Audit Manager on the account. Select the Payment Card Industry Data Security Standards (PCI DSS) for auditing.

- B. Configure Amazon S3 Inventory on the S3 bucket Configure Amazon Athena to query the inventory.

- C. Configure Amazon Macie to run a data discovery job that uses managed identiers for the required data types.

- D. Use Amazon S3 Select to run a report across the S3 bucket.

**Correct:** C
**Why:** Amazon Macie can scan S3 for PII/financial data with managed identifiers and produce findings.

**Incorrect:**
- A: Audit Manager/S3 Inventory/S3 Select do not detect PII patterns as Macie does.
- B: Audit Manager/S3 Inventory/S3 Select do not detect PII patterns as Macie does.
- D: Audit Manager/S3 Inventory/S3 Select do not detect PII patterns as Macie does.


---

## Amazon OpenSearch Service

### Question #311

A company is using AWS to design a web application that will process insurance quotes. Users will request quotes from the application. Quotes must be separated by quote type, must be responded to within 24 hours, and must not get lost. The solution must maximize operational eciency and must minimize maintenance. Which solution meets these requirements?

- A. Create multiple Amazon Kinesis data streams based on the quote type. Configure the web application to send messages to the proper data stream. Configure each backend group of application servers to use the Kinesis Client Library (KCL) to pool messages from its own data stream.

- B. Create an AWS Lambda function and an Amazon Simple Notification Service (Amazon SNS) topic for each quote type. Subscribe the Lambda function to its associated SNS topic. Configure the application to publish requests for quotes to the appropriate SNS topic.

- C. Create a single Amazon Simple Notification Service (Amazon SNS) topic. Subscribe Amazon Simple Queue Service (Amazon SQS) queues to the SNS topic. Configure SNS message filtering to publish messages to the proper SQS queue based on the quote type. Configure each backend application server to use its own SQS queue.

- D. Create multiple Amazon Kinesis Data Firehose delivery streams based on the quote type to deliver data streams to an Amazon OpenSearch Service cluster. Configure the application to send messages to the proper delivery stream. Configure each backend group of application servers to search for the messages from OpenSearch Service and process them accordingly.

**Correct:** C
**Why:** SNS to SQS with message filtering separates quote types; SQS ensures durability (no loss) and 24‑hour processing window.

**Incorrect:**
- A: Kinesis or Firehose/OpenSearch are not needed for durable request queues.
- B: SNS + Lambda per type adds code and lacks durable backlog.
- D: Kinesis or Firehose/OpenSearch are not needed for durable request queues.


---

---

### Question #432

An ecommerce company wants to use machine learning (ML) algorithms to build and train models. The company will use the models to visualize complex scenarios and to detect trends in customer data. The architecture team wants to integrate its ML models with a reporting platform to analyze the augmented data and use the data directly in its business intelligence dashboards. Which solution will meet these requirements with the LEAST operational overhead?

- A. Use AWS Glue to create an ML transform to build and train models. Use Amazon OpenSearch Service to visualize the data.

- B. Use Amazon SageMaker to build and train models. Use Amazon QuickSight to visualize the data.

- C. Use a pre-built ML Amazon Machine Image (AMI) from the AWS Marketplace to build and train models. Use Amazon OpenSearch Service to visualize the data.

- D. Use Amazon QuickSight to build and train models by using calculated elds. Use Amazon QuickSight to visualize the data.

**Correct:** B
**Why:** SageMaker builds/trains models with low ops; QuickSight provides native visualization and dashboarding over augmented data.

**Incorrect:**
- A: Glue transforms/Marketplace AMIs/QuickSight alone are not the right ML training path with least ops.
- C: Glue transforms/Marketplace AMIs/QuickSight alone are not the right ML training path with least ops.
- D: Glue transforms/Marketplace AMIs/QuickSight alone are not the right ML training path with least ops.


---

## Amazon QuickSight

### Question #278

A company wants to create an application to store employee data in a hierarchical structured relationship. The company needs a minimum-latency response to high-trac queries for the employee data and must protect any sensitive data. The company also needs to receive monthly email messages if any nancial information is present in the employee data. Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)

- A. Use Amazon Redshift to store the employee data in hierarchies. Unload the data to Amazon S3 every month.

- B. Use Amazon DynamoDB to store the employee data in hierarchies. Export the data to Amazon S3 every month.

- C. Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly events to AWS Lambda.

- D. Use Amazon Athena to analyze the employee data in Amazon S3. Integrate Athena with Amazon QuickSight to publish analysis dashboards and share the dashboards with users.

E. Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly notications through an Amazon Simple Notification Service (Amazon SNS) subscription.

**Correct:** B, E
**Why:** DynamoDB suits hierarchical, low‑latency queries; Macie + EventBridge → SNS can notify monthly when financial data is detected.

**Incorrect:**
- A: Redshift/Athena aren’t optimized for low‑latency hierarchical queries.
- C: Lambda trigger pattern is unnecessary for monthly notifications.
- D: Redshift/Athena aren’t optimized for low‑latency hierarchical queries.


---

---

### Question #280

A company is using Amazon CloudFront with its website. The company has enabled logging on the CloudFront distribution, and logs are saved in one of the company’s Amazon S3 buckets. The company needs to perform advanced analyses on the logs and build visualizations. What should a solutions architect do to meet these requirements?

- A. Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket. Visualize the results with AWS Glue.

- B. Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket. Visualize the results with Amazon QuickSight.

- C. Use standard SQL queries in Amazon DynamoDB to analyze the CloudFront logs in the S3 bucket. Visualize the results with AWS Glue.

- D. Use standard SQL queries in Amazon DynamoDB to analyze the CloudFront logs in the S3 bucket. Visualize the results with Amazon QuickSight.

**Correct:** B
**Why:** Use Athena to query CloudFront logs in S3 and QuickSight for visualizations.

**Incorrect:**
- A: Glue/DynamoDB are not needed for log analysis/visualization here.
- C: Glue/DynamoDB are not needed for log analysis/visualization here.
- D: Glue/DynamoDB are not needed for log analysis/visualization here.


---

---

### Question #341

A company has an Amazon S3 data lake that is governed by AWS Lake Formation. The company wants to create a visualization in Amazon QuickSight by joining the data in the data lake with operational data that is stored in an Amazon Aurora MySQL database. The company wants to enforce column-level authorization so that the company’s marketing team can access only a subset of columns in the database. Which solution will meet these requirements with the LEAST operational overhead?

- A. Use Amazon EMR to ingest the data directly from the database to the QuickSight SPICE engine. Include only the required columns.

- B. Use AWS Glue Studio to ingest the data from the database to the S3 data lake. Attach an IAM policy to the QuickSight users to enforce column-level access control. Use Amazon S3 as the data source in QuickSight.

- C. Use AWS Glue Elastic Views to create a materialized view for the database in Amazon S3. Create an S3 bucket policy to enforce column- level access control for the QuickSight users. Use Amazon S3 as the data source in QuickSight.

- D. Use a Lake Formation blueprint to ingest the data from the database to the S3 data lake. Use Lake Formation to enforce column-level access control for the QuickSight users. Use Amazon Athena as the data source in QuickSight.

**Correct:** D
**Why:** Lake Formation ingestion and column‑level permissions with Athena as the query engine integrate cleanly with QuickSight.

**Incorrect:**
- A: EMR/Glue Studio or Elastic Views/S3 policies don’t provide column‑level governance end‑to‑end as simply.
- B: EMR/Glue Studio or Elastic Views/S3 policies don’t provide column‑level governance end‑to‑end as simply.
- C: EMR/Glue Studio or Elastic Views/S3 policies don’t provide column‑level governance end‑to‑end as simply.


---

---

### Question #432

An ecommerce company wants to use machine learning (ML) algorithms to build and train models. The company will use the models to visualize complex scenarios and to detect trends in customer data. The architecture team wants to integrate its ML models with a reporting platform to analyze the augmented data and use the data directly in its business intelligence dashboards. Which solution will meet these requirements with the LEAST operational overhead?

- A. Use AWS Glue to create an ML transform to build and train models. Use Amazon OpenSearch Service to visualize the data.

- B. Use Amazon SageMaker to build and train models. Use Amazon QuickSight to visualize the data.

- C. Use a pre-built ML Amazon Machine Image (AMI) from the AWS Marketplace to build and train models. Use Amazon OpenSearch Service to visualize the data.

- D. Use Amazon QuickSight to build and train models by using calculated elds. Use Amazon QuickSight to visualize the data.

**Correct:** B
**Why:** SageMaker builds/trains models with low ops; QuickSight provides native visualization and dashboarding over augmented data.

**Incorrect:**
- A: Glue transforms/Marketplace AMIs/QuickSight alone are not the right ML training path with least ops.
- C: Glue transforms/Marketplace AMIs/QuickSight alone are not the right ML training path with least ops.
- D: Glue transforms/Marketplace AMIs/QuickSight alone are not the right ML training path with least ops.


---

## Amazon RDS

### Question #259

A company is implementing new data retention policies for all databases that run on Amazon RDS DB instances. The company must retain daily backups for a minimum period of 2 years. The backups must be consistent and restorable. Which solution should a solutions architect recommend to meet these requirements?

- A. Create a backup vault in AWS Backup to retain RDS backups. Create a new backup plan with a daily schedule and an expiration period of 2 years after creation. Assign the RDS DB instances to the backup plan.

- B. Configure a backup window for the RDS DB instances for daily snapshots. Assign a snapshot retention policy of 2 years to each RDS DB instance. Use Amazon Data Lifecycle Manager (Amazon DLM) to schedule snapshot deletions.

- C. Configure database transaction logs to be automatically backed up to Amazon CloudWatch Logs with an expiration period of 2 years.

- D. Configure an AWS Database Migration Service (AWS DMS) replication task. Deploy a replication instance, and configure a change data capture (CDC) task to stream database changes to Amazon S3 as the target. Configure S3 Lifecycle policies to delete the snapshots after 2 years.

**Correct:** A
**Why:** AWS Backup can enforce daily backups and retain for 2 years with minimal ops.

**Incorrect:**
- B: DLM is for EBS; RDS snapshots retention isn’t managed that way.
- C: CloudWatch logs/DMS exports are not consistent RDS backups.
- D: CloudWatch logs/DMS exports are not consistent RDS backups.


---

---

### Question #268

A gaming company has a web application that displays scores. The application runs on Amazon EC2 instances behind an Application Load Balancer. The application stores data in an Amazon RDS for MySQL database. Users are starting to experience long delays and interruptions that are caused by database read performance. The company wants to improve the user experience while minimizing changes to the application’s architecture. What should a solutions architect do to meet these requirements?

- A. Use Amazon ElastiCache in front of the database.

- B. Use RDS Proxy between the application and the database.

- C. Migrate the application from EC2 instances to AWS Lambda.

- D. Migrate the database from Amazon RDS for MySQL to Amazon DynamoDB.

**Correct:** A
**Why:** Add ElastiCache in front of RDS to reduce read pressure without major architecture change.

**Incorrect:**
- B: RDS Proxy assists connections, not read latency.
- C: Lambda/DynamoDB are re‑architectures.
- D: Lambda/DynamoDB are re‑architectures.


---

---

### Question #269

An ecommerce company has noticed performance degradation of its Amazon RDS based web application. The performance degradation is attributed to an increase in the number of read-only SQL queries triggered by business analysts. A solutions architect needs to solve the problem with minimal changes to the existing web application. What should the solutions architect recommend?

- A. Export the data to Amazon DynamoDB and have the business analysts run their queries.

- B. Load the data into Amazon ElastiCache and have the business analysts run their queries.

- C. Create a read replica of the primary database and have the business analysts run their queries.

- D. Copy the data into an Amazon Redshift cluster and have the business analysts run their queries.

**Correct:** C
**Why:** Offload analysts’ queries to a read replica to protect primary performance with minimal changes.

**Incorrect:**
- A: DynamoDB/ElastiCache/Redshift require more changes.
- B: DynamoDB/ElastiCache/Redshift require more changes.
- D: DynamoDB/ElastiCache/Redshift require more changes.


---

---

### Question #273

A rapidly growing ecommerce company is running its workloads in a single AWS Region. A solutions architect must create a disaster recovery (DR) strategy that includes a different AWS Region. The company wants its database to be up to date in the DR Region with the least possible latency. The remaining infrastructure in the DR Region needs to run at reduced capacity and must be able to scale up if necessary. Which solution will meet these requirements with the LOWEST recovery time objective (RTO)?

- A. Use an Amazon Aurora global database with a pilot light deployment.

- B. Use an Amazon Aurora global database with a warm standby deployment.

- C. Use an Amazon RDS Multi-AZ DB instance with a pilot light deployment.

- D. Use an Amazon RDS Multi-AZ DB instance with a warm standby deployment.

**Correct:** B
**Why:** Aurora Global Database with a warm standby (scaled‑down stack) in DR Region offers the lowest RTO and near‑real‑time replication.

**Incorrect:**
- A: Pilot light increases RTO.
- C: RDS Multi‑AZ/snapshots won’t meet lowest RTO cross‑Region.
- D: RDS Multi‑AZ/snapshots won’t meet lowest RTO cross‑Region.


---

---

### Question #276

A company has a multi-tier application deployed on several Amazon EC2 instances in an Auto Scaling group. An Amazon RDS for Oracle instance is the application’ s data layer that uses Oracle-specic PL/SQL functions. Trac to the application has been steadily increasing. This is causing the EC2 instances to become overloaded and the RDS instance to run out of storage. The Auto Scaling group does not have any scaling metrics and denes the minimum healthy instance count only. The company predicts that trac will continue to increase at a steady but unpredictable rate before leveling off. What should a solutions architect do to ensure the system can automatically scale for the increased trac? (Choose two.)

- A. Configure storage Auto Scaling on the RDS for Oracle instance.

- B. Migrate the database to Amazon Aurora to use Auto Scaling storage.

- C. Configure an alarm on the RDS for Oracle instance for low free storage space.

- D. Configure the Auto Scaling group to use the average CPU as the scaling metric.

E. Configure the Auto Scaling group to use the average free memory as the scaling metric.

**Correct:** A, D
**Why:** Enable RDS storage autoscaling and scale EC2 fleet on average CPU to handle increasing traffic automatically.

**Incorrect:**
- B: Aurora migration is heavier change.
- C: Alarm only or memory metric alone is insufficient as primary scaler.
- E: Alarm only or memory metric alone is insufficient as primary scaler.


---

---

### Question #278

A company wants to create an application to store employee data in a hierarchical structured relationship. The company needs a minimum-latency response to high-trac queries for the employee data and must protect any sensitive data. The company also needs to receive monthly email messages if any nancial information is present in the employee data. Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)

- A. Use Amazon Redshift to store the employee data in hierarchies. Unload the data to Amazon S3 every month.

- B. Use Amazon DynamoDB to store the employee data in hierarchies. Export the data to Amazon S3 every month.

- C. Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly events to AWS Lambda.

- D. Use Amazon Athena to analyze the employee data in Amazon S3. Integrate Athena with Amazon QuickSight to publish analysis dashboards and share the dashboards with users.

E. Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly notications through an Amazon Simple Notification Service (Amazon SNS) subscription.

**Correct:** B, E
**Why:** DynamoDB suits hierarchical, low‑latency queries; Macie + EventBridge → SNS can notify monthly when financial data is detected.

**Incorrect:**
- A: Redshift/Athena aren’t optimized for low‑latency hierarchical queries.
- C: Lambda trigger pattern is unnecessary for monthly notifications.
- D: Redshift/Athena aren’t optimized for low‑latency hierarchical queries.


---

---

### Question #281

A company runs a eet of web servers using an Amazon RDS for PostgreSQL DB instance. After a routine compliance check, the company sets a standard that requires a recovery point objective (RPO) of less than 1 second for all its production databases. Which solution meets these requirements?

- A. Enable a Multi-AZ deployment for the DB instance.

- B. Enable auto scaling for the DB instance in one Availability Zone.

- C. Configure the DB instance in one Availability Zone, and create multiple read replicas in a separate Availability Zone.

- D. Configure the DB instance in one Availability Zone, and configure AWS Database Migration Service (AWS DMS) change data capture (CDC) tasks.

**Correct:** A
**Why:** RDS Multi‑AZ uses synchronous replication to a standby, providing near‑zero RPO (<1s) and automatic failover.

**Incorrect:**
- B: Do not meet the <1‑second RPO requirement.
- C: Do not meet the <1‑second RPO requirement.
- D: Do not meet the <1‑second RPO requirement.


---

---

### Question #287

A company wants to migrate a Windows-based application from on premises to the AWS Cloud. The application has three tiers: an application tier, a business tier, and a database tier with Microsoft SQL Server. The company wants to use specic features of SQL Server such as native backups and Data Quality Services. The company also needs to share files for processing between the tiers. How should a solutions architect design the architecture to meet these requirements?

- A. Host all three tiers on Amazon EC2 instances. Use Amazon FSx File Gateway for file sharing between the tiers.

- B. Host all three tiers on Amazon EC2 instances. Use Amazon FSx for Windows File Server for file sharing between the tiers.

- C. Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon RDS. Use Amazon Elastic File System (Amazon EFS) for file sharing between the tiers.

- D. Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon RDS. Use a Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) volume for file sharing between the tiers.

**Correct:** B
**Why:** Keep SQL Server features by hosting DB on EC2; use FSx for Windows File Server for shared files; EC2 for app tiers.

**Incorrect:**
- C: RDS may not support required SQL Server features; EBS for file sharing is inappropriate.
- D: RDS may not support required SQL Server features; EBS for file sharing is inappropriate.


---

---

### Question #292

A company is preparing a new data platform that will ingest real-time streaming data from multiple sources. The company needs to transform the data before writing the data to Amazon S3. The company needs the ability to use SQL to query the transformed data. Which solutions will meet these requirements? (Choose two.)

- A. Use Amazon Kinesis Data Streams to stream the data. Use Amazon Kinesis Data Analytics to transform the data. Use Amazon Kinesis Data Firehose to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.

- B. Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use AWS Glue to transform the data and to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.

- C. Use AWS Database Migration Service (AWS DMS) to ingest the data. Use Amazon EMR to transform the data and to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.

- D. Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use Amazon Kinesis Data Analytics to transform the data and to write the data to Amazon S3. Use the Amazon RDS query editor to query the transformed data from Amazon S3.

E. Use Amazon Kinesis Data Streams to stream the data. Use AWS Glue to transform the data. Use Amazon Kinesis Data Firehose to write the data to Amazon S3. Use the Amazon RDS query editor to query the transformed data from Amazon S3.

**Correct:** A, B
**Why:** Kinesis Data Streams + KDA + Firehose + S3 + Athena, or MSK + Glue + S3 + Athena both satisfy transform to S3 and SQL on data.

**Incorrect:**
- C: DMS/Incorrect query endpoints do not meet requirements fully.
- D: DMS/Incorrect query endpoints do not meet requirements fully.
- E: DMS/Incorrect query endpoints do not meet requirements fully.


---

---

### Question #298

A company is running a critical business application on Amazon EC2 instances behind an Application Load Balancer. The EC2 instances run in an Auto Scaling group and access an Amazon RDS DB instance. The design did not pass an operational review because the EC2 instances and the DB instance are all located in a single Availability Zone. A solutions architect must update the design to use a second Availability Zone. Which solution will make the application highly available?

- A. Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance with connections to each network.

- B. Provision two subnets that extend across both Availability Zones. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance with connections to each network.

- C. Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance for Multi-AZ deployment.

- D. Provision a subnet that extends across both Availability Zones. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance for Multi-AZ deployment.

**Correct:** C
**Why:** Distribute EC2 across AZs via ASG and enable RDS Multi‑AZ for database HA.

**Incorrect:**
- A: Incorrect subnet constructs or missing Multi‑AZ DB.
- B: Incorrect subnet constructs or missing Multi‑AZ DB.
- D: Incorrect subnet constructs or missing Multi‑AZ DB.


---

---

### Question #300

A company needs to migrate a legacy application from an on-premises data center to the AWS Cloud because of hardware capacity constraints. The application runs 24 hours a day, 7 days a week. The application’s database storage continues to grow over time. What should a solutions architect do to meet these requirements MOST cost-effectively?

- A. Migrate the application layer to Amazon EC2 Spot Instances. Migrate the data storage layer to Amazon S3.

- B. Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer to Amazon RDS On-Demand Instances.

- C. Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer to Amazon Aurora Reserved Instances.

- D. Migrate the application layer to Amazon EC2 On-Demand Instances. Migrate the data storage layer to Amazon RDS Reserved Instances.

**Correct:** C
**Why:** 24/7 app → EC2 RIs; Aurora RIs for growing database with storage autoscaling; cost‑effective for continuous use.

**Incorrect:**
- A: Spot unsuitable for 24/7.
- B: On‑Demand for DB or app increases cost.
- D: On‑Demand for DB or app increases cost.


---

---

### Question #308

A company has multiple AWS accounts that use consolidated billing. The company runs several active high performance Amazon RDS for Oracle On-Demand DB instances for 90 days. The company’s nance team has access to AWS Trusted Advisor in the consolidated billing account and all other AWS accounts. The nance team needs to use the appropriate AWS account to access the Trusted Advisor check recommendations for RDS. The nance team must review the appropriate Trusted Advisor check to reduce RDS costs. Which combination of steps should the nance team take to meet these requirements? (Choose two.)

- A. Use the Trusted Advisor recommendations from the account where the RDS instances are running.

- B. Use the Trusted Advisor recommendations from the consolidated billing account to see all RDS instance checks at the same time.

- C. Review the Trusted Advisor check for Amazon RDS Reserved Instance Optimization.

- D. Review the Trusted Advisor check for Amazon RDS Idle DB Instances.

E. Review the Trusted Advisor check for Amazon Redshift Reserved Node Optimization.

**Correct:** A, C
**Why:** View Trusted Advisor in the account where RDS runs and check the RDS Reserved Instance Optimization to reduce cost.

**Incorrect:**
- B: Consolidated billing account may not show member‑level service details for all checks.
- D: Idle DB/Redshift checks alone are insufficient or unrelated.
- E: Idle DB/Redshift checks alone are insufficient or unrelated.


---

---

### Question #314

A company has an on-premises MySQL database used by the global sales team with infrequent access patterns. The sales team requires the database to have minimal downtime. A database administrator wants to migrate this database to AWS without selecting a particular instance type in anticipation of more users in the future. Which service should a solutions architect recommend?

- A. Amazon Aurora MySQL

- B. Amazon Aurora Serverless for MySQL

- C. Amazon Redshift Spectrum

- D. Amazon RDS for MySQL

**Correct:** B
**Why:** Aurora Serverless for MySQL scales capacity automatically; supports minimal downtime migration and infrequent usage.

**Incorrect:**
- A: Fixed instances require capacity planning.
- C: Redshift Spectrum is for analytics, not OLTP.
- D: Fixed instances require capacity planning.


---

---

### Question #330

A company is planning to store data on Amazon RDS DB instances. The company must encrypt the data at rest. What should a solutions architect do to meet this requirement?

- A. Create a key in AWS Key Management Service (AWS KMS). Enable encryption for the DB instances.

- B. Create an encryption key. Store the key in AWS Secrets Manager. Use the key to encrypt the DB instances.

- C. Generate a certicate in AWS Certicate Manager (ACM). Enable SSL/TLS on the DB instances by using the certicate.

- D. Generate a certicate in AWS Identity and Access Management (IAM). Enable SSL/TLS on the DB instances by using the certicate.

**Correct:** A
**Why:** Use a KMS CMK and enable at‑rest encryption for RDS at creation/restore from encrypted snapshot.

**Incorrect:**
- B: Secrets Manager/ACM/IAM certs don’t encrypt DB storage at rest.
- C: Secrets Manager/ACM/IAM certs don’t encrypt DB storage at rest.
- D: Secrets Manager/ACM/IAM certs don’t encrypt DB storage at rest.


---

---

### Question #337

A company has deployed a web application on AWS. The company hosts the backend database on Amazon RDS for MySQL with a primary DB instance and ve read replicas to support scaling needs. The read replicas must lag no more than 1 second behind the primary DB instance. The database routinely runs scheduled stored procedures. As trac on the website increases, the replicas experience additional lag during periods of peak load. A solutions architect must reduce the replication lag as much as possible. The solutions architect must minimize changes to the application code and must minimize ongoing operational overhead. Which solution will meet these requirements?

- A. Migrate the database to Amazon Aurora MySQL. Replace the read replicas with Aurora Replicas, and configure Aurora Auto Scaling. Replace the stored procedures with Aurora MySQL native functions.

- B. Deploy an Amazon ElastiCache for Redis cluster in front of the database. Modify the application to check the cache before the application queries the database. Replace the stored procedures with AWS Lambda functions.

- C. Migrate the database to a MySQL database that runs on Amazon EC2 instances. Choose large, compute optimized EC2 instances for all replica nodes. Maintain the stored procedures on the EC2 instances.

- D. Migrate the database to Amazon DynamoDB. Provision a large number of read capacity units (RCUs) to support the required throughput, and configure on-demand capacity scaling. Replace the stored procedures with DynamoDB streams.

**Correct:** A
**Why:** Aurora MySQL with Aurora Replicas and Auto Scaling reduces replica lag and operational overhead.

**Incorrect:**
- B: Cache/EC2 self‑managed or DynamoDB require more changes.
- C: Cache/EC2 self‑managed or DynamoDB require more changes.
- D: Cache/EC2 self‑managed or DynamoDB require more changes.


---

---

### Question #339

A company has a custom application with embedded credentials that retrieves information from an Amazon RDS MySQL DB instance. Management says the application must be made more secure with the least amount of programming effort. What should a solutions architect do to meet these requirements?

- A. Use AWS Key Management Service (AWS KMS) to create keys. Configure the application to load the database credentials from AWS KMS. Enable automatic key rotation.

- B. Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS Secrets Manager. Configure the application to load the database credentials from Secrets Manager. Create an AWS Lambda function that rotates the credentials in Secret Manager.

- C. Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS Secrets Manager. Configure the application to load the database credentials from Secrets Manager. Set up a credentials rotation schedule for the application user in the RDS for MySQL database using Secrets Manager.

- D. Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS Systems Manager Parameter Store. Configure the application to load the database credentials from Parameter Store. Set up a credentials rotation schedule for the application user in the RDS for MySQL database using Parameter Store.

**Correct:** C
**Why:** Store DB creds in Secrets Manager and enable automated rotation for RDS MySQL with minimal code changes.

**Incorrect:**
- A: KMS or Parameter Store lack integrated RDS rotation automation.
- B: Lambda rotation is not needed; use built‑in rotation.
- D: KMS or Parameter Store lack integrated RDS rotation automation.


---

---

### Question #343

A solutions architect is designing a company’s disaster recovery (DR) architecture. The company has a MySQL database that runs on an Amazon EC2 instance in a private subnet with scheduled backup. The DR design needs to include multiple AWS Regions. Which solution will meet these requirements with the LEAST operational overhead?

- A. Migrate the MySQL database to multiple EC2 instances. Configure a standby EC2 instance in the DR Region. Turn on replication.

- B. Migrate the MySQL database to Amazon RDS. Use a Multi-AZ deployment. Turn on read replication for the primary DB instance in the different Availability Zones.

- C. Migrate the MySQL database to an Amazon Aurora global database. Host the primary DB cluster in the primary Region. Host the secondary DB cluster in the DR Region.

- D. Store the scheduled backup of the MySQL database in an Amazon S3 bucket that is congured for S3 Cross-Region Replication (CRR). Use the data backup to restore the database in the DR Region.

**Correct:** C
**Why:** Aurora Global Database provides multi‑Region design with low ops and built‑in replication; best for DR across Regions.

**Incorrect:**
- A: EC2 self‑managed, single‑Region RDS Multi‑AZ, or CRR of backups offer higher RTO/RPO or more ops.
- B: EC2 self‑managed, single‑Region RDS Multi‑AZ, or CRR of backups offer higher RTO/RPO or more ops.
- D: EC2 self‑managed, single‑Region RDS Multi‑AZ, or CRR of backups offer higher RTO/RPO or more ops.


---

---

### Question #350

A company uses a 100 GB Amazon RDS for Microsoft SQL Server Single-AZ DB instance in the us-east-1 Region to store customer transactions. The company needs high availability and automatic recovery for the DB instance. The company must also run reports on the RDS database several times a year. The report process causes transactions to take longer than usual to post to the customers’ accounts. The company needs a solution that will improve the performance of the report process. Which combination of steps will meet these requirements? (Choose two.)

- A. Modify the DB instance from a Single-AZ DB instance to a Multi-AZ deployment.

- B. Take a snapshot of the current DB instance. Restore the snapshot to a new RDS deployment in another Availability Zone.

- C. Create a read replica of the DB instance in a different Availability Zone. Point all requests for reports to the read replica.

- D. Migrate the database to RDS Custom.

E. Use RDS Proxy to limit reporting requests to the maintenance window.

**Correct:** A, C
**Why:** Convert to Multi‑AZ for HA/recovery; use a read replica for reporting to offload reads and improve performance.

**Incorrect:**
- B: Restored snapshots aren’t HA; RDS Custom unnecessary; RDS Proxy doesn’t solve reporting load.
- D: Restored snapshots aren’t HA; RDS Custom unnecessary; RDS Proxy doesn’t solve reporting load.
- E: Restored snapshots aren’t HA; RDS Custom unnecessary; RDS Proxy doesn’t solve reporting load.


---

---

### Question #353

A company hosts a three-tier web application on Amazon EC2 instances in a single Availability Zone. The web application uses a self-managed MySQL database that is hosted on an EC2 instance to store data in an Amazon Elastic Block Store (Amazon EBS) volume. The MySQL database currently uses a 1 TB Provisioned IOPS SSD (io2) EBS volume. The company expects trac of 1,000 IOPS for both reads and writes at peak trac. The company wants to minimize any disruptions, stabilize performance, and reduce costs while retaining the capacity for double the IOPS. The company wants to move the database tier to a fully managed solution that is highly available and fault tolerant. Which solution will meet these requirements MOST cost-effectively?

- A. Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with an io2 Block Express EBS volume.

- B. Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with a General Purpose SSD (gp2) EBS volume.

- C. Use Amazon S3 Intelligent-Tiering access tiers.

- D. Use two large EC2 instances to host the database in active-passive mode.

**Correct:** B
**Why:** Multi-AZ Amazon RDS for MySQL with General Purpose SSD (gp2) provides stable, managed, highly available performance and is cost‑effective for ~1–2K IOPS needs.

**Incorrect:**
- A: io2 Block Express is not a typical/necessary choice for RDS here and is more expensive.
- C: S3 Intelligent‑Tiering is unrelated to database storage.
- D: Self-managed EC2 databases increase ops and reduce availability compared to RDS.


---

---

### Question #354

A company hosts a serverless application on AWS. The application uses Amazon API Gateway, AWS Lambda, and an Amazon RDS for PostgreSQL database. The company notices an increase in application errors that result from database connection timeouts during times of peak trac or unpredictable trac. The company needs a solution that reduces the application failures with the least amount of change to the code. What should a solutions architect do to meet these requirements?

- A. Reduce the Lambda concurrency rate.

- B. Enable RDS Proxy on the RDS DB instance.

- C. Resize the RDS DB instance class to accept more connections.

- D. Migrate the database to Amazon DynamoDB with on-demand scaling.

**Correct:** B
**Why:** RDS Proxy pools and reuses connections for Lambda, preventing connection storms/timeouts during bursts with minimal code change.

**Incorrect:**
- A: Reducing Lambda concurrency throttles the app and does not fix connection pooling.
- C: Upsizing may delay but not solve connection exhaustion.
- D: Migrating to DynamoDB is a large rewrite.


---

---

### Question #359

A hospital needs to store patient records in an Amazon S3 bucket. The hospital’s compliance team must ensure that all protected health information (PHI) is encrypted in transit and at rest. The compliance team must administer the encryption key for data at rest. Which solution will meet these requirements?

- A. Create a public SSL/TLS certicate in AWS Certicate Manager (ACM). Associate the certicate with Amazon S3. Configure default encryption for each S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS). Assign the compliance team to manage the KMS keys.

- B. Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Configure default encryption for each S3 bucket to use server-side encryption with S3 managed encryption keys (SSE-S3). Assign the compliance team to manage the SSE-S3 keys.

- C. Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Configure default encryption for each S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS). Assign the compliance team to manage the KMS keys.

- D. Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Use Amazon Macie to protect the sensitive data that is stored in Amazon S3. Assign the compliance team to manage Macie.

**Correct:** C
**Why:** Enforce TLS in transit with aws:SecureTransport on the S3 bucket policy; use SSE‑KMS so the compliance team administers the CMK for data at rest.

**Incorrect:**
- A: ACM public certs aren’t attached directly to S3; also KMS administration is needed, not SSE‑S3.
- B: SSE‑S3 keys are managed by AWS, not the compliance team.
- D: Macie discovers PII; it does not fulfill encryption requirements.


---

---

### Question #361

A company hosts a multiplayer gaming application on AWS. The company wants the application to read data with sub-millisecond latency and run one-time queries on historical data. Which solution will meet these requirements with the LEAST operational overhead?

- A. Use Amazon RDS for data that is frequently accessed. Run a periodic custom script to export the data to an Amazon S3 bucket.

- B. Store the data directly in an Amazon S3 bucket. Implement an S3 Lifecycle policy to move older data to S3 Glacier Deep Archive for long- term storage. Run one-time queries on the data in Amazon S3 by using Amazon Athena.

- C. Use Amazon DynamoDB with DynamoDB Accelerator (DAX) for data that is frequently accessed. Export the data to an Amazon S3 bucket by using DynamoDB table export. Run one-time queries on the data in Amazon S3 by using Amazon Athena.

- D. Use Amazon DynamoDB for data that is frequently accessed. Turn on streaming to Amazon Kinesis Data Streams. Use Amazon Kinesis Data Firehose to read the data from Kinesis Data Streams. Store the records in an Amazon S3 bucket.

**Correct:** C
**Why:** DynamoDB + DAX provides sub‑millisecond reads; export to S3 and query ad hoc with Athena for historical analysis with low ops.

**Incorrect:**
- A: Adds more moving parts and does not give sub‑ms reads as simply as DAX.
- B: S3 alone lacks sub‑ms latency for frequently accessed data.
- D: Adds more moving parts and does not give sub‑ms reads as simply as DAX.


---

---

### Question #362

A company uses a payment processing system that requires messages for a particular payment ID to be received in the same order that they were sent. Otherwise, the payments might be processed incorrectly. Which actions should a solutions architect take to meet this requirement? (Choose two.)

- A. Write the messages to an Amazon DynamoDB table with the payment ID as the partition key.

- B. Write the messages to an Amazon Kinesis data stream with the payment ID as the partition key.

- C. Write the messages to an Amazon ElastiCache for Memcached cluster with the payment ID as the key.

- D. Write the messages to an Amazon Simple Queue Service (Amazon SQS) queue. Set the message attribute to use the payment ID.

E. Write the messages to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the message group to use the payment ID.

**Correct:** B, E
**Why:** Kinesis shards preserve ordering by partition key; SQS FIFO preserves message order per message group.

**Incorrect:**
- A: DynamoDB does not enforce arrival/processing order.
- C: ElastiCache is not a message bus with ordering guarantees.
- D: Standard SQS queues do not guarantee ordering.


---

---

### Question #365

A company runs a web application that is backed by Amazon RDS. A new database administrator caused data loss by accidentally editing information in a database table. To help recover from this type of incident, the company wants the ability to restore the database to its state from 5 minutes before any change within the last 30 days. Which feature should the solutions architect include in the design to meet this requirement?

- A. Read replicas

- B. Manual snapshots

- C. Automated backups

- D. Multi-AZ deployments

**Correct:** C
**Why:** RDS automated backups with point‑in‑time recovery allow restores to any point in the last 30 days (e.g., 5 minutes before a change).

**Incorrect:**
- A: Read replicas are not for PITR.
- B: Manual snapshots are point‑in‑time only, not continuous across 30 days.
- D: Multi‑AZ is for HA, not restores to earlier points.


---

---

### Question #368

A solutions architect wants all new users to have specic complexity requirements and mandatory rotation periods for IAM user passwords. What should the solutions architect do to accomplish this?

- A. Set an overall password policy for the entire AWS account.

- B. Set a password policy for each IAM user in the AWS account.

- C. Use third-party vendor software to set password requirements.

- D. Attach an Amazon CloudWatch rule to the Create_newuser event to set the password with the appropriate requirements.

**Correct:** A
**Why:** The account‑level IAM password policy enforces complexity and rotation for all new IAM users.

**Incorrect:**
- B: Per‑user policies are not scalable.
- C: Third‑party tools are unnecessary.
- D: CloudWatch rule cannot enforce password policy on user creation.


---

---

### Question #372

A company wants to migrate an Oracle database to AWS. The database consists of a single table that contains millions of geographic information systems (GIS) images that are high resolution and are identied by a geographic code. When a natural disaster occurs, tens of thousands of images get updated every few minutes. Each geographic code has a single image or row that is associated with it. The company wants a solution that is highly available and scalable during such events. Which solution meets these requirements MOST cost-effectively?

- A. Store the images and geographic codes in a database table. Use Oracle running on an Amazon RDS Multi-AZ DB instance.

- B. Store the images in Amazon S3 buckets. Use Amazon DynamoDB with the geographic code as the key and the image S3 URL as the value.

- C. Store the images and geographic codes in an Amazon DynamoDB table. Configure DynamoDB Accelerator (DAX) during times of high load.

- D. Store the images in Amazon S3 buckets. Store geographic codes and image S3 URLs in a database table. Use Oracle running on an Amazon RDS Multi-AZ DB instance.

**Correct:** B
**Why:** Store images in S3 and use DynamoDB keyed by geographic code with S3 URLs; scales and is highly available during spikes.

**Incorrect:**
- A: RDS Oracle will struggle to scale writes/size cost‑effectively.
- C: Storing large images directly in DynamoDB is not appropriate.
- D: RDS Oracle will struggle to scale writes/size cost‑effectively.


---

---

### Question #376

A company has launched an Amazon RDS for MySQL DB instance. Most of the connections to the database come from serverless applications. Application trac to the database changes signicantly at random intervals. At times of high demand, users report that their applications experience database connection rejection errors. Which solution will resolve this issue with the LEAST operational overhead?

- A. Create a proxy in RDS Proxy. Configure the users’ applications to use the DB instance through RDS Proxy.

- B. Deploy Amazon ElastiCache for Memcached between the users’ applications and the DB instance.

- C. Migrate the DB instance to a different instance class that has higher I/O capacity. Configure the users’ applications to use the new DB instance.

- D. Configure Multi-AZ for the DB instance. Configure the users’ applications to switch between the DB instances.

**Correct:** A
**Why:** RDS Proxy manages connections and pools for spiky serverless traffic to RDS MySQL, avoiding connection rejections with minimal ops.

**Incorrect:**
- B: ElastiCache does not address DB connection limits.
- C: Upsizing does not fix pooling problems.
- D: Multi‑AZ is for HA, not connection pooling.


---

---

### Question #379

A company hosts a frontend application that uses an Amazon API Gateway API backend that is integrated with AWS Lambda. When the API receives requests, the Lambda function loads many libraries. Then the Lambda function connects to an Amazon RDS database, processes the data, and returns the data to the frontend application. The company wants to ensure that response latency is as low as possible for all its users with the fewest number of changes to the company's operations. Which solution will meet these requirements?

- A. Establish a connection between the frontend application and the database to make queries faster by bypassing the API.

- B. Configure provisioned concurrency for the Lambda function that handles the requests.

- C. Cache the results of the queries in Amazon S3 for faster retrieval of similar datasets.

- D. Increase the size of the database to increase the number of connections Lambda can establish at one time.

**Correct:** B
**Why:** Provisioned concurrency keeps Lambda execution environments warm, reducing cold starts (library load time) and latency with minimal changes.

**Incorrect:**
- A: Direct DB access from frontend bypasses API and is insecure.
- C: Caching S3 results doesn’t address dynamic queries and connections.
- D: Upsizing DB does not remove Lambda cold starts.


---

---

### Question #380

A company is migrating its on-premises workload to the AWS Cloud. The company already uses several Amazon EC2 instances and Amazon RDS DB instances. The company wants a solution that automatically starts and stops the EC2 instances and DB instances outside of business hours. The solution must minimize cost and infrastructure maintenance. Which solution will meet these requirements?

- A. Scale the EC2 instances by using elastic resize. Scale the DB instances to zero outside of business hours.

- B. Explore AWS Marketplace for partner solutions that will automatically start and stop the EC2 instances and DB instances on a schedule.

- C. Launch another EC2 instance. Configure a crontab schedule to run shell scripts that will start and stop the existing EC2 instances and DB instances on a schedule.

- D. Create an AWS Lambda function that will start and stop the EC2 instances and DB instances. Configure Amazon EventBridge to invoke the Lambda function on a schedule.

**Correct:** D
**Why:** An EventBridge‑scheduled Lambda can start/stop EC2 and RDS on a schedule with minimal cost and ops.

**Incorrect:**
- A: EC2 “elastic resize” and RDS scale‑to‑zero do not apply.
- B: Marketplace adds cost/complexity.
- C: Managing a cron EC2 instance adds ops.


---

---

### Question #381

A company hosts a three-tier web application that includes a PostgreSQL database. The database stores the metadata from documents. The company searches the metadata for key terms to retrieve documents that the company reviews in a report each month. The documents are stored in Amazon S3. The documents are usually written only once, but they are updated frequently. The reporting process takes a few hours with the use of relational queries. The reporting process must not prevent any document modications or the addition of new documents. A solutions architect needs to implement a solution to speed up the reporting process. Which solution will meet these requirements with the LEAST amount of change to the application code?

- A. Set up a new Amazon DocumentDB (with MongoDB compatibility) cluster that includes a read replica. Scale the read replica to generate the reports.

- B. Set up a new Amazon Aurora PostgreSQL DB cluster that includes an Aurora Replica. Issue queries to the Aurora Replica to generate the reports.

- C. Set up a new Amazon RDS for PostgreSQL Multi-AZ DB instance. Configure the reporting module to query the secondary RDS node so that the reporting module does not affect the primary node.

- D. Set up a new Amazon DynamoDB table to store the documents. Use a xed write capacity to support new document entries. Automatically scale the read capacity to support the reports.

**Correct:** B
**Why:** Aurora PostgreSQL with an Aurora Replica lets reports run against the replica without blocking writes; minimal code change (same engine).

**Incorrect:**
- A: DocumentDB or RDS Multi‑AZ do not solve read scaling as effectively.
- C: DocumentDB or RDS Multi‑AZ do not solve read scaling as effectively.
- D: DynamoDB would require a redesign.


---

---

### Question #386

An ecommerce company is running a multi-tier application on AWS. The front-end and backend tiers both run on Amazon EC2, and the database runs on Amazon RDS for MySQL. The backend tier communicates with the RDS instance. There are frequent calls to return identical datasets from the database that are causing performance slowdowns. Which action should be taken to improve the performance of the backend?

- A. Implement Amazon SNS to store the database calls.

- B. Implement Amazon ElastiCache to cache the large datasets.

- C. Implement an RDS for MySQL read replica to cache database calls.

- D. Implement Amazon Kinesis Data Firehose to stream the calls to the database.

**Correct:** B
**Why:** ElastiCache (Redis/Memcached) caches repeated reads to reduce database load and improve performance.

**Incorrect:**
- A: SNS is a pub/sub service, not a cache.
- C: Read replicas help, but cache is better for repetitive identical datasets and lower latency.
- D: Kinesis Firehose is for data streaming, not query caching.


---

---

### Question #388

A company is deploying a two-tier web application in a VPC. The web tier is using an Amazon EC2 Auto Scaling group with public subnets that span multiple Availability Zones. The database tier consists of an Amazon RDS for MySQL DB instance in separate private subnets. The web tier requires access to the database to retrieve product information. The web application is not working as intended. The web application reports that it cannot connect to the database. The database is conrmed to be up and running. All congurations for the network ACLs, security groups, and route tables are still in their default states. What should a solutions architect recommend to x the application?

- A. Add an explicit rule to the private subnet’s network ACL to allow trac from the web tier’s EC2 instances.

- B. Add a route in the VPC route table to allow trac between the web tier’s EC2 instances and the database tier.

- C. Deploy the web tier's EC2 instances and the database tier’s RDS instance into two separate VPCs, and configure VPC peering.

- D. Add an inbound rule to the security group of the database tier’s RDS instance to allow trac from the web tiers security group.

**Correct:** D
**Why:** By default, RDS SG needs an inbound rule from the web tier SG; this enables connectivity while keeping least privilege.

**Incorrect:**
- A: Default NACLs/route tables already allow VPC internal traffic appropriately.
- B: Default NACLs/route tables already allow VPC internal traffic appropriately.
- C: VPC peering is unnecessary for a single VPC.


---

---

### Question #389

A company has a large dataset for its online advertising business stored in an Amazon RDS for MySQL DB instance in a single Availability Zone. The company wants business reporting queries to run without impacting the write operations to the production DB instance. Which solution meets these requirements?

- A. Deploy RDS read replicas to process the business reporting queries.

- B. Scale out the DB instance horizontally by placing it behind an Elastic Load Balancer.

- C. Scale up the DB instance to a larger instance type to handle write operations and queries.

- D. Deploy the DB instance in multiple Availability Zones to process the business reporting queries.

**Correct:** A
**Why:** RDS read replicas offload read/reporting queries from the writer to avoid impacting write performance.

**Incorrect:**
- B: RDS isn’t load balanced like this; ELB is not for databases.
- C: Scaling up increases costs and may still impact writes.
- D: Multi‑AZ improves HA, not read scaling.


---

---

### Question #390

A company hosts a three-tier ecommerce application on a eet of Amazon EC2 instances. The instances run in an Auto Scaling group behind an Application Load Balancer (ALB). All ecommerce data is stored in an Amazon RDS for MariaDB Multi-AZ DB instance. The company wants to optimize customer session management during transactions. The application must store session data durably. Which solutions will meet these requirements? (Choose two.)

- A. Turn on the sticky sessions feature (session anity) on the ALB.

- B. Use an Amazon DynamoDB table to store customer session information.

- C. Deploy an Amazon Cognito user pool to manage user session information.

- D. Deploy an Amazon ElastiCache for Redis cluster to store customer session information.

E. Use AWS Systems Manager Application Manager in the application to manage user session information.

**Correct:** A, B
**Why:** Sticky sessions reduce cross‑instance churn during transactions; store session data durably in DynamoDB to survive instance failure.

**Incorrect:**
- C: Cognito manages auth, not app transaction sessions.
- D: ElastiCache Redis alone is not durable.
- E: Systems Manager Application Manager is not for session storage.


---

---

### Question #391

A company needs a backup strategy for its three-tier stateless web application. The web application runs on Amazon EC2 instances in an Auto Scaling group with a dynamic scaling policy that is congured to respond to scaling events. The database tier runs on Amazon RDS for PostgreSQL. The web application does not require temporary local storage on the EC2 instances. The company’s recovery point objective (RPO) is 2 hours. The backup strategy must maximize scalability and optimize resource utilization for this environment. Which solution will meet these requirements?

- A. Take snapshots of Amazon Elastic Block Store (Amazon EBS) volumes of the EC2 instances and database every 2 hours to meet the RPO.

- B. Configure a snapshot lifecycle policy to take Amazon Elastic Block Store (Amazon EBS) snapshots. Enable automated backups in Amazon RDS to meet the RPO.

- C. Retain the latest Amazon Machine Images (AMIs) of the web and application tiers. Enable automated backups in Amazon RDS and use point-in-time recovery to meet the RPO.

- D. Take snapshots of Amazon Elastic Block Store (Amazon EBS) volumes of the EC2 instances every 2 hours. Enable automated backups in Amazon RDS and use point-in-time recovery to meet the RPO.

**Correct:** C
**Why:** Keep latest AMIs for stateless tiers; enable RDS automated backups with PITR to meet a 2‑hour RPO without per‑instance EBS snapshots.

**Incorrect:**
- A: EC2 EBS snapshots are unnecessary for stateless instances.
- B: AMIs are more appropriate than frequent EBS snapshots for autoscaled stateless tiers.
- D: EC2 EBS snapshots are unnecessary for stateless instances.


---

---

### Question #392

A company wants to deploy a new public web application on AWS. The application includes a web server tier that uses Amazon EC2 instances. The application also includes a database tier that uses an Amazon RDS for MySQL DB instance. The application must be secure and accessible for global customers that have dynamic IP addresses. How should a solutions architect configure the security groups to meet these requirements?

- A. Configure the security group for the web servers to allow inbound trac on port 443 from 0.0.0.0/0. Configure the security group for the DB instance to allow inbound trac on port 3306 from the security group of the web servers.

- B. Configure the security group for the web servers to allow inbound trac on port 443 from the IP addresses of the customers. Configure the security group for the DB instance to allow inbound trac on port 3306 from the security group of the web servers.

- C. Configure the security group for the web servers to allow inbound trac on port 443 from the IP addresses of the customers. Configure the security group for the DB instance to allow inbound trac on port 3306 from the IP addresses of the customers.

- D. Configure the security group for the web servers to allow inbound trac on port 443 from 0.0.0.0/0. Configure the security group for the DB instance to allow inbound trac on port 3306 from 0.0.0.0/0.

**Correct:** A
**Why:** Allow HTTPS from anywhere for global access; restrict DB access to only the web tier SG on 3306.

**Incorrect:**
- B: Customer IPs are dynamic; not maintainable.
- C: Customer IPs are dynamic; not maintainable.
- D: Opening DB to the world is insecure.


---

---

### Question #393

A payment processing company records all voice communication with its customers and stores the audio files in an Amazon S3 bucket. The company needs to capture the text from the audio files. The company must remove from the text any personally identiable information (PII) that belongs to customers. What should a solutions architect do to meet these requirements?

- A. Process the audio files by using Amazon Kinesis Video Streams. Use an AWS Lambda function to scan for known PII patterns.

- B. When an audio file is uploaded to the S3 bucket, invoke an AWS Lambda function to start an Amazon Textract task to analyze the call recordings.

- C. Configure an Amazon Transcribe transcription job with PII redaction turned on. When an audio file is uploaded to the S3 bucket, invoke an AWS Lambda function to start the transcription job. Store the output in a separate S3 bucket.

- D. Create an Amazon Connect contact flow that ingests the audio files with transcription turned on. Embed an AWS Lambda function to scan for known PII patterns. Use Amazon EventBridge to start the contact flow when an audio file is uploaded to the S3 bucket.

**Correct:** C
**Why:** Amazon Transcribe supports PII redaction; invoke jobs on upload and store sanitized text output separately.

**Incorrect:**
- A: Not appropriate services for transcription and PII redaction here.
- B: Textract is for documents, not audio.
- D: Not appropriate services for transcription and PII redaction here.


---

---

### Question #394

A company is running a multi-tier ecommerce web application in the AWS Cloud. The application runs on Amazon EC2 instances with an Amazon RDS for MySQL Multi-AZ DB instance. Amazon RDS is congured with the latest generation DB instance with 2,000 GB of storage in a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume. The database performance affects the application during periods of high demand. A database administrator analyzes the logs in Amazon CloudWatch Logs and discovers that the application performance always degrades when the number of read and write IOPS is higher than 20,000. What should a solutions architect do to improve the application performance?

- A. Replace the volume with a magnetic volume.

- B. Increase the number of IOPS on the gp3 volume.

- C. Replace the volume with a Provisioned IOPS SSD (io2) volume.

- D. Replace the 2,000 GB gp3 volume with two 1,000 GB gp3 volumes.

**Correct:** C
**Why:** Move to Provisioned IOPS SSD (io2) to exceed 20K IOPS limits of gp3 and ensure consistent high IOPS for peak periods.

**Incorrect:**
- A: Magnetic volumes are low performance.
- B: gp3 max IOPS is 16K—insufficient for >20K IOPS.
- D: Splitting gp3 volumes does not increase single‑volume IOPS for RDS.


---

---

### Question #395

An IAM user made several configuration changes to AWS resources in their company's account during a production deployment last week. A solutions architect learned that a couple of security group rules are not congured as desired. The solutions architect wants to conrm which IAM user was responsible for making changes. Which service should the solutions architect use to nd the desired information?

- A. Amazon GuardDuty

- B. Amazon Inspector

- C. AWS CloudTrail

- D. AWS Cong

**Correct:** C
**Why:** AWS CloudTrail records API calls and identifies the IAM principal who made each change.

**Incorrect:**
- A: GuardDuty/Inspector/Config do not directly answer “who made the API change” as CloudTrail does.
- B: GuardDuty/Inspector/Config do not directly answer “who made the API change” as CloudTrail does.
- D: GuardDuty/Inspector/Config do not directly answer “who made the API change” as CloudTrail does.


---

---

### Question #397

An ecommerce company needs to run a scheduled daily job to aggregate and filter sales records for analytics. The company stores the sales records in an Amazon S3 bucket. Each object can be up to 10 GB in size. Based on the number of sales events, the job can take up to an hour to complete. The CPU and memory usage of the job are constant and are known in advance. A solutions architect needs to minimize the amount of operational effort that is needed for the job to run. Which solution meets these requirements?

- A. Create an AWS Lambda function that has an Amazon EventBridge notification. Schedule the EventBridge event to run once a day.

- B. Create an AWS Lambda function. Create an Amazon API Gateway HTTP API, and integrate the API with the function. Create an Amazon EventBridge scheduled event that calls the API and invokes the function.

- C. Create an Amazon Elastic Container Service (Amazon ECS) cluster with an AWS Fargate launch type. Create an Amazon EventBridge scheduled event that launches an ECS task on the cluster to run the job.

- D. Create an Amazon Elastic Container Service (Amazon ECS) cluster with an Amazon EC2 launch type and an Auto Scaling group with at least one EC2 instance. Create an Amazon EventBridge scheduled event that launches an ECS task on the cluster to run the job.

**Correct:** C
**Why:** ECS on Fargate with an EventBridge schedule runs the containerized job with known CPU/memory, minimizing ops (no servers to manage).

**Incorrect:**
- A: Lambda is not suitable for up to 10 GB objects and 1‑hour jobs.
- B: Lambda is not suitable for up to 10 GB objects and 1‑hour jobs.
- D: EC2 launch type requires managing instances/ASG.


---

---

### Question #401

A company wants to use the AWS Cloud to make an existing application highly available and resilient. The current version of the application resides in the company's data center. The application recently experienced data loss after a database server crashed because of an unexpected power outage. The company needs a solution that avoids any single points of failure. The solution must give the application the ability to scale to meet user demand. Which solution will meet these requirements?

- A. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon RDS DB instance in a Multi-AZ configuration.

- B. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group in a single Availability Zone. Deploy the database on an EC2 instance. Enable EC2 Auto Recovery.

- C. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon RDS DB instance with a read replica in a single Availability Zone. Promote the read replica to replace the primary DB instance if the primary DB instance fails.

- D. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Deploy the primary and secondary database servers on EC2 instances across multiple Availability Zones. Use Amazon Elastic Block Store (Amazon EBS) Multi-Attach to create shared storage between the instances.

**Correct:** A
**Why:** Multi-AZ Auto Scaling for EC2 app tier plus RDS Multi-AZ removes single points of failure and provides scalability.

**Incorrect:**
- B: Single AZ and self-managed DB are single points of failure.
- C: A single-AZ primary with read replica is not the same as Multi-AZ failover.
- D: EBS Multi-Attach is not for databases and adds complexity.


---

---

### Question #402

A company needs to ingest and handle large amounts of streaming data that its application generates. The application runs on Amazon EC2 instances and sends data to Amazon Kinesis Data Streams, which is congured with default settings. Every other day, the application consumes the data and writes the data to an Amazon S3 bucket for business intelligence (BI) processing. The company observes that Amazon S3 is not receiving all the data that the application sends to Kinesis Data Streams. What should a solutions architect do to resolve this issue?

- A. Update the Kinesis Data Streams default settings by modifying the data retention period.

- B. Update the application to use the Kinesis Producer Library (KPL) to send the data to Kinesis Data Streams.

- C. Update the number of Kinesis shards to handle the throughput of the data that is sent to Kinesis Data Streams.

- D. Turn on S3 Versioning within the S3 bucket to preserve every version of every object that is ingested in the S3 bucket.

**Correct:** A
**Why:** Default Kinesis Data Streams retention is 24 hours; consuming every other day requires a longer retention period.

**Incorrect:**
- B: KPL helps with throughput, not expired records.
- C: More shards won’t recover expired data.
- D: S3 Versioning does not affect Kinesis delivery.


---

---

### Question #406

A solutions architect is designing a two-tiered architecture that includes a public subnet and a database subnet. The web servers in the public subnet must be open to the internet on port 443. The Amazon RDS for MySQL DB instance in the database subnet must be accessible only to the web servers on port 3306. Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)

- A. Create a network ACL for the public subnet. Add a rule to deny outbound trac to 0.0.0.0/0 on port 3306.

- B. Create a security group for the DB instance. Add a rule to allow trac from the public subnet CIDR block on port 3306.

- C. Create a security group for the web servers in the public subnet. Add a rule to allow trac from 0.0.0.0/0 on port 443.

- D. Create a security group for the DB instance. Add a rule to allow trac from the web servers’ security group on port 3306.

E. Create a security group for the DB instance. Add a rule to deny all trac except trac from the web servers’ security group on port 3306.

**Correct:** C, D
**Why:** Allow 443 from the internet to web SG; allow 3306 to DB SG only from the web servers’ SG.

**Incorrect:**
- A: NACL outbound deny to 0.0.0.0/0 on 3306 is unnecessary and coarse.
- B: Allowing from subnet CIDR is broader than referencing the web SG.
- E: SGs are allow-lists; “deny” rules don’t apply.


---

---

### Question #409

A solutions architect must migrate a Windows Internet Information Services (IIS) web application to AWS. The application currently relies on a file share hosted in the user's on-premises network-attached storage (NAS). The solutions architect has proposed migrating the IIS web servers to Amazon EC2 instances in multiple Availability Zones that are connected to the storage solution, and conguring an Elastic Load Balancer attached to the instances. Which replacement to the on-premises file share is MOST resilient and durable?

- A. Migrate the file share to Amazon RDS.

- B. Migrate the file share to AWS Storage Gateway.

- C. Migrate the file share to Amazon FSx for Windows File Server.

- D. Migrate the file share to Amazon Elastic File System (Amazon EFS).

**Correct:** C
**Why:** FSx for Windows File Server provides fully managed SMB shares for Windows IIS with HA/durability.

**Incorrect:**
- A: RDS is a database, not a file share.
- B: Storage Gateway is for hybrid access, not the most resilient/durable for primary shared storage.
- D: EFS is NFS (Linux), not SMB.


---

---

### Question #411

A company has a web application with sporadic usage patterns. There is heavy usage at the beginning of each month, moderate usage at the start of each week, and unpredictable usage during the week. The application consists of a web server and a MySQL database server running inside the data center. The company would like to move the application to the AWS Cloud, and needs to select a cost-effective database platform that will not require database modications. Which solution will meet these requirements?

- A. Amazon DynamoDB

- B. Amazon RDS for MySQL

- C. MySQL-compatible Amazon Aurora Serverless

- D. MySQL deployed on Amazon EC2 in an Auto Scaling group

**Correct:** B
**Why:** Amazon RDS for MySQL is a managed, cost‑effective, compatible drop‑in with no application or schema changes.

**Incorrect:**
- A: DynamoDB would require a rewrite.
- C: Aurora Serverless can be cost‑effective but may require migration/compatibility testing; RDS MySQL is simpler.
- D: Self-managed MySQL on EC2 increases ops.


---

---

### Question #416

A rapidly growing global ecommerce company is hosting its web application on AWS. The web application includes static content and dynamic content. The website stores online transaction processing (OLTP) data in an Amazon RDS database The website’s users are experiencing slow page loads. Which combination of actions should a solutions architect take to resolve this issue? (Choose two.)

- A. Configure an Amazon Redshift cluster.

- B. Set up an Amazon CloudFront distribution.

- C. Host the dynamic web content in Amazon S3.

- D. Create a read replica for the RDS DB instance.

E. Configure a Multi-AZ deployment for the RDS DB instance.

**Correct:** B, D
**Why:** CloudFront caches static/dynamic content at edge; RDS read replica offloads reads, improving page load.

**Incorrect:**
- A: Redshift is for OLAP, not OLTP page rendering.
- C: Dynamic content doesn’t belong in S3.
- E: Multi‑AZ is HA, not performance.


---

---

### Question #420

A company wants to use an Amazon RDS for PostgreSQL DB cluster to simplify time-consuming database administrative tasks for production database workloads. The company wants to ensure that its database is highly available and will provide automatic failover support in most scenarios in less than 40 seconds. The company wants to ooad reads off of the primary instance and keep costs as low as possible. Which solution will meet these requirements?

- A. Use an Amazon RDS Multi-AZ DB instance deployment. Create one read replica and point the read workload to the read replica.

- B. Use an Amazon RDS Multi-AZ DB duster deployment Create two read replicas and point the read workload to the read replicas.

- C. Use an Amazon RDS Multi-AZ DB instance deployment. Point the read workload to the secondary instances in the Multi-AZ pair.

- D. Use an Amazon RDS Multi-AZ DB cluster deployment Point the read workload to the reader endpoint.

**Correct:** D
**Why:** RDS Multi‑AZ DB cluster provides fast automatic failover (<~40s) and a reader endpoint to offload reads cost‑effectively.

**Incorrect:**
- A: Multi‑AZ instance deployment lacks reader endpoint/read scaling.
- B: Extra read replicas are unnecessary; use the cluster’s reader endpoint.
- C: Multi‑AZ instance deployment lacks reader endpoint/read scaling.


---

---

### Question #427

A solutions architect is implementing a complex Java application with a MySQL database. The Java application must be deployed on Apache Tomcat and must be highly available. What should the solutions architect do to meet these requirements?

- A. Deploy the application in AWS Lambda. Configure an Amazon API Gateway API to connect with the Lambda functions.

- B. Deploy the application by using AWS Elastic Beanstalk. Configure a load-balanced environment and a rolling deployment policy.

- C. Migrate the database to Amazon ElastiCache. Configure the ElastiCache security group to allow access from the application.

- D. Launch an Amazon EC2 instance. Install a MySQL server on the EC2 instance. Configure the application on the server. Create an AMI. Use the AMI to create a launch template with an Auto Scaling group.

**Correct:** B
**Why:** Elastic Beanstalk for Tomcat with load‑balanced, rolling deployments meets HA needs with low ops; use RDS for MySQL.

**Incorrect:**
- A: Lambda is not for Tomcat Java web apps.
- C: ElastiCache is not a database.
- D: DIY EC2/ASG raises operational burden.


---

---

### Question #431

A company has developed a new video game as a web application. The application is in a three-tier architecture in a VPC with Amazon RDS for MySQL in the database layer. Several players will compete concurrently online. The game’s developers want to display a top-10 scoreboard in near- real time and offer the ability to stop and restore the game while preserving the current scores. What should a solutions architect do to meet these requirements?

- A. Set up an Amazon ElastiCache for Memcached cluster to cache the scores for the web application to display.

- B. Set up an Amazon ElastiCache for Redis cluster to compute and cache the scores for the web application to display.

- C. Place an Amazon CloudFront distribution in front of the web application to cache the scoreboard in a section of the application.

- D. Create a read replica on Amazon RDS for MySQL to run queries to compute the scoreboard and serve the read trac to the web application.

**Correct:** B
**Why:** ElastiCache for Redis supports sorted sets for near‑real‑time leaderboards and can persist to preserve scores.

**Incorrect:**
- A: Memcached lacks persistence and sorted‑set operations.
- C: CloudFront caches static content, not compute leaderboards.
- D: RDS read replica queries add latency and load.


---

---

### Question #432

An ecommerce company wants to use machine learning (ML) algorithms to build and train models. The company will use the models to visualize complex scenarios and to detect trends in customer data. The architecture team wants to integrate its ML models with a reporting platform to analyze the augmented data and use the data directly in its business intelligence dashboards. Which solution will meet these requirements with the LEAST operational overhead?

- A. Use AWS Glue to create an ML transform to build and train models. Use Amazon OpenSearch Service to visualize the data.

- B. Use Amazon SageMaker to build and train models. Use Amazon QuickSight to visualize the data.

- C. Use a pre-built ML Amazon Machine Image (AMI) from the AWS Marketplace to build and train models. Use Amazon OpenSearch Service to visualize the data.

- D. Use Amazon QuickSight to build and train models by using calculated elds. Use Amazon QuickSight to visualize the data.

**Correct:** B
**Why:** SageMaker builds/trains models with low ops; QuickSight provides native visualization and dashboarding over augmented data.

**Incorrect:**
- A: Glue transforms/Marketplace AMIs/QuickSight alone are not the right ML training path with least ops.
- C: Glue transforms/Marketplace AMIs/QuickSight alone are not the right ML training path with least ops.
- D: Glue transforms/Marketplace AMIs/QuickSight alone are not the right ML training path with least ops.


---

---

### Question #436

A company moved its on-premises PostgreSQL database to an Amazon RDS for PostgreSQL DB instance. The company successfully launched a new product. The workload on the database has increased. The company wants to accommodate the larger workload without adding infrastructure. Which solution will meet these requirements MOST cost-effectively?

- A. Buy reserved DB instances for the total workload. Make the Amazon RDS for PostgreSQL DB instance larger.

- B. Make the Amazon RDS for PostgreSQL DB instance a Multi-AZ DB instance.

- C. Buy reserved DB instances for the total workload. Add another Amazon RDS for PostgreSQL DB instance.

- D. Make the Amazon RDS for PostgreSQL DB instance an on-demand DB instance.

**Correct:** A
**Why:** Increase instance size for more capacity and buy reserved DB instances to lower cost without adding additional components.

**Incorrect:**
- B: Multi‑AZ adds cost and infrastructure without increasing capacity.
- C: Adding another instance adds infrastructure.
- D: On‑Demand increases cost unpredictably.


---

---

### Question #438

A company wants to share accounting data with an external auditor. The data is stored in an Amazon RDS DB instance that resides in a private subnet. The auditor has its own AWS account and requires its own copy of the database. What is the MOST secure way for the company to share the database with the auditor?

- A. Create a read replica of the database. Configure IAM standard database authentication to grant the auditor access.

- B. Export the database contents to text files. Store the files in an Amazon S3 bucket. Create a new IAM user for the auditor. Grant the user access to the S3 bucket.

- C. Copy a snapshot of the database to an Amazon S3 bucket. Create an IAM user. Share the user's keys with the auditor to grant access to the object in the S3 bucket.

- D. Create an encrypted snapshot of the database. Share the snapshot with the auditor. Allow access to the AWS Key Management Service (AWS KMS) encryption key.

**Correct:** D
**Why:** Share an encrypted snapshot with the auditor and permit access to the KMS key; the auditor restores into its own account.

**Incorrect:**
- A: Read replica does not cross accounts easily and IAM DB auth is separate.
- B: Text export or snapshot in S3 is less secure/more work.
- C: Text export or snapshot in S3 is less secure/more work.


---

---

### Question #440

A company used an Amazon RDS for MySQL DB instance during application testing. Before terminating the DB instance at the end of the test cycle, a solutions architect created two backups. The solutions architect created the first backup by using the mysqldump utility to create a database dump. The solutions architect created the second backup by enabling the nal DB snapshot option on RDS termination. The company is now planning for a new test cycle and wants to create a new DB instance from the most recent backup. The company has chosen a MySQL-compatible edition ofAmazon Aurora to host the DB instance. Which solutions will create the new DB instance? (Choose two.)

- A. Import the RDS snapshot directly into Aurora.

- B. Upload the RDS snapshot to Amazon S3. Then import the RDS snapshot into Aurora.

- C. Upload the database dump to Amazon S3. Then import the database dump into Aurora.

- D. Use AWS Database Migration Service (AWS DMS) to import the RDS snapshot into Aurora.

E. Upload the database dump to Amazon S3. Then use AWS Database Migration Service (AWS DMS) to import the database dump into Aurora.

**Correct:** C, E
**Why:** Import from mysqldump in S3 to Aurora or use DMS to load from the dump; you cannot import an RDS snapshot directly into Aurora.

**Incorrect:**
- A: Snapshots are not directly importable to Aurora; DMS ingests data, not snapshots.
- B: Snapshots are not directly importable to Aurora; DMS ingests data, not snapshots.
- D: Snapshots are not directly importable to Aurora; DMS ingests data, not snapshots.


---

---

### Question #444

A company has hired a solutions architect to design a reliable architecture for its application. The application consists of one Amazon RDS DB instance and two manually provisioned Amazon EC2 instances that run web servers. The EC2 instances are located in a single Availability Zone. An employee recently deleted the DB instance, and the application was unavailable for 24 hours as a result. The company is concerned with the overall reliability of its environment. What should the solutions architect do to maximize reliability of the application's infrastructure?

- A. Delete one EC2 instance and enable termination protection on the other EC2 instance. Update the DB instance to be Multi-AZ, and enable deletion protection.

- B. Update the DB instance to be Multi-AZ, and enable deletion protection. Place the EC2 instances behind an Application Load Balancer, and run them in an EC2 Auto Scaling group across multiple Availability Zones.

- C. Create an additional DB instance along with an Amazon API Gateway and an AWS Lambda function. Configure the application to invoke the Lambda function through API Gateway. Have the Lambda function write the data to the two DB instances.

- D. Place the EC2 instances in an EC2 Auto Scaling group that has multiple subnets located in multiple Availability Zones. Use Spot Instances instead of On-Demand Instances. Set up Amazon CloudWatch alarms to monitor the health of the instances Update the DB instance to be Multi-AZ, and enable deletion protection.

**Correct:** B
**Why:** Protect RDS with Multi‑AZ and deletion protection; place EC2 in ALB+Auto Scaling across AZs to maximize reliability.

**Incorrect:**
- A: Either reduce availability or add unnecessary components/cost.
- C: Either reduce availability or add unnecessary components/cost.
- D: Either reduce availability or add unnecessary components/cost.


---

---

### Question #449

A company runs its application on an Oracle database. The company plans to quickly migrate to AWS because of limited resources for the database, backup administration, and data center maintenance. The application uses third-party database features that require privileged access. Which solution will help the company migrate the database to AWS MOST cost-effectively?

- A. Migrate the database to Amazon RDS for Oracle. Replace third-party features with cloud services.

- B. Migrate the database to Amazon RDS Custom for Oracle. Customize the database settings to support third-party features.

- C. Migrate the database to an Amazon EC2 Amazon Machine Image (AMI) for Oracle. Customize the database settings to support third-party features.

- D. Migrate the database to Amazon RDS for PostgreSQL by rewriting the application code to remove dependency on Oracle APEX.

**Correct:** B
**Why:** RDS Custom for Oracle allows privileged access/customization for third‑party features with managed backups/patching and lower ops than EC2.

**Incorrect:**
- A: RDS for Oracle restricts privileged access needed for third‑party features.
- C: EC2 is higher ops/cost to manage.
- D: Rewriting to PostgreSQL increases effort and risk.


---

---

### Question #450

A company has a three-tier web application that is in a single server. The company wants to migrate the application to the AWS Cloud. The company also wants the application to align with the AWS Well-Architected Framework and to be consistent with AWS recommended best practices for security, scalability, and resiliency. Which combination of solutions will meet these requirements? (Choose three.)

- A. Create a VPC across two Availability Zones with the application's existing architecture. Host the application with existing architecture on an Amazon EC2 instance in a private subnet in each Availability Zone with EC2 Auto Scaling groups. Secure the EC2 instance with security groups and network access control lists (network ACLs).

- B. Set up security groups and network access control lists (network ACLs) to control access to the database layer. Set up a single Amazon RDS database in a private subnet.

- C. Create a VPC across two Availability Zones. Refactor the application to host the web tier, application tier, and database tier. Host each tier on its own private subnet with Auto Scaling groups for the web tier and application tier.

- D. Use a single Amazon RDS database. Allow database access only from the application tier security group.

E. Use Elastic Load Balancers in front of the web tier. Control access by using security groups containing references to each layer's security groups.

F. Use an Amazon RDS database Multi-AZ cluster deployment in private subnets. Allow database access only from application tier security groups.

**Correct:** C, E, F
**Why:** Refactor into three tiers across two AZs; use ELBs for the web tier; and use RDS Multi‑AZ in private subnets with SGs restricting access from the app tier.

**Incorrect:**
- A: Single RDS or single‑AZ design and flat architectures don’t meet best practices for resiliency/security.
- B: Single RDS or single‑AZ design and flat architectures don’t meet best practices for resiliency/security.
- D: Single RDS or single‑AZ design and flat architectures don’t meet best practices for resiliency/security.


---

---

### Question #451

A company is migrating its applications and databases to the AWS Cloud. The company will use Amazon Elastic Container Service (Amazon ECS), AWS Direct Connect, and Amazon RDS. Which activities will be managed by the company's operational team? (Choose three.)

- A. Management of the Amazon RDS infrastructure layer, operating system, and platforms

- B. Creation of an Amazon RDS DB instance and conguring the scheduled maintenance window

- C. Configuration of additional software components on Amazon ECS for monitoring, patch management, log management, and host intrusion detection

- D. Installation of patches for all minor and major database versions for Amazon RDS

E. Ensure the physical security of the Amazon RDS infrastructure in the data center

F. Encryption of the data that moves in transit through Direct Connect

**Correct:** B, C, F
**Why:** Customers create RDS instances and set maintenance windows (B), manage ECS host/agent and add monitoring/IDS (C when using EC2 launch type), and must encrypt data in transit over Direct Connect (F).

**Incorrect:**
- A: AWS manages RDS infrastructure/OS/platform and physical security.
- D: RDS applies minor patches automatically during the window; customers schedule, but do not install “all patches” directly.
- E: AWS manages RDS infrastructure/OS/platform and physical security.


---

---

### Question #455

A company uses AWS Organizations. The company wants to operate some of its AWS accounts with different budgets. The company wants to receive alerts and automatically prevent provisioning of additional resources on AWS accounts when the allocated budget threshold is met during a specic period. Which combination of solutions will meet these requirements? (Choose three.)

- A. Use AWS Budgets to create a budget. Set the budget amount under the Cost and Usage Reports section of the required AWS accounts.

- B. Use AWS Budgets to create a budget. Set the budget amount under the Billing dashboards of the required AWS accounts.

- C. Create an IAM user for AWS Budgets to run budget actions with the required permissions.

- D. Create an IAM role for AWS Budgets to run budget actions with the required permissions.

E. Add an alert to notify the company when each account meets its budget threshold. Add a budget action that selects the IAM identity created with the appropriate cong rule to prevent provisioning of additional resources.

F. Add an alert to notify the company when each account meets its budget threshold. Add a budget action that selects the IAM identity created with the appropriate service control policy (SCP) to prevent provisioning of additional resources.

**Correct:** B, D, F
**Why:** Create budgets in AWS Budgets (B); create an IAM role for Budgets to execute actions (D); use a Budgets action to apply an SCP to prevent further provisioning when thresholds are met (F).

**Incorrect:**
- A: “Cost and Usage Reports” section is not where you set the budget amount.
- C: A user is not ideal; use a role for Budgets actions.
- E: AWS Config is unrelated to Budgets actions.


---

---

### Question #458

A solutions architect is designing a RESTAPI in Amazon API Gateway for a cash payback service. The application requires 1 GB of memory and 2 GB of storage for its computation resources. The application will require that the data is in a relational format. Which additional combination ofAWS services will meet these requirements with the LEAST administrative effort? (Choose two.)

- A. Amazon EC2

- B. AWS Lambda

- C. Amazon RDS

- D. Amazon DynamoDB

E. Amazon Elastic Kubernetes Services (Amazon EKS)

**Correct:** B, C
**Why:** Lambda can be sized to 1 GB memory and sufficient ephemeral storage; use Amazon RDS for relational storage—minimal admin.

**Incorrect:**
- A: EC2/EKS add operational overhead.
- D: DynamoDB is non‑relational.
- E: EC2/EKS add operational overhead.


---

---

### Question #464

A company hosts an online shopping application that stores all orders in an Amazon RDS for PostgreSQL Single-AZ DB instance. Management wants to eliminate single points of failure and has asked a solutions architect to recommend an approach to minimize database downtime without requiring any changes to the application code. Which solution meets these requirements?

- A. Convert the existing database instance to a Multi-AZ deployment by modifying the database instance and specifying the Multi-AZ option.

- B. Create a new RDS Multi-AZ deployment. Take a snapshot of the current RDS instance and restore the new Multi-AZ deployment with the snapshot.

- C. Create a read-only replica of the PostgreSQL database in another Availability Zone. Use Amazon Route 53 weighted record sets to distribute requests across the databases.

- D. Place the RDS for PostgreSQL database in an Amazon EC2 Auto Scaling group with a minimum group size of two. Use Amazon Route 53 weighted record sets to distribute requests across instances.

**Correct:** A
**Why:** Modifying the RDS instance to Multi‑AZ provides automatic failover with minimal downtime and no app changes.

**Incorrect:**
- B: Spinning up a new deployment adds downtime.
- C: Read replicas/EC2 ASG do not provide transparent failover for writes.
- D: Read replicas/EC2 ASG do not provide transparent failover for writes.


---

---

### Question #466

A company designed a stateless two-tier application that uses Amazon EC2 in a single Availability Zone and an Amazon RDS Multi-AZ DB instance. New company management wants to ensure the application is highly available. What should a solutions architect do to meet this requirement?

- A. Configure the application to use Multi-AZ EC2 Auto Scaling and create an Application Load Balancer

- B. Configure the application to take snapshots of the EC2 instances and send them to a different AWS Region

- C. Configure the application to use Amazon Route 53 latency-based routing to feed requests to the application

- D. Configure Amazon Route 53 rules to handle incoming requests and create a Multi-AZ Application Load Balancer

**Correct:** A
**Why:** Make the stateless tier highly available with Multi‑AZ Auto Scaling and an ALB; DB is already Multi‑AZ.

**Incorrect:**
- B: Snapshots/Route 53 alone do not provide app‑tier HA.
- C: Snapshots/Route 53 alone do not provide app‑tier HA.
- D: Snapshots/Route 53 alone do not provide app‑tier HA.


---

---

### Question #479

A company is making a prototype of the infrastructure for its new website by manually provisioning the necessary infrastructure. This infrastructure includes an Auto Scaling group, an Application Load Balancer and an Amazon RDS database. After the configuration has been thoroughly validated, the company wants the capability to immediately deploy the infrastructure for development and production use in two Availability Zones in an automated fashion. What should a solutions architect recommend to meet these requirements?

- A. Use AWS Systems Manager to replicate and provision the prototype infrastructure in two Availability Zones

- B. Dene the infrastructure as a template by using the prototype infrastructure as a guide. Deploy the infrastructure with AWS CloudFormation.

- C. Use AWS Cong to record the inventory of resources that are used in the prototype infrastructure. Use AWS Cong to deploy the prototype infrastructure into two Availability Zones.

- D. Use AWS Elastic Beanstalk and configure it to use an automated reference to the prototype infrastructure to automatically deploy new environments in two Availability Zones.

**Correct:** B
**Why:** Define the infra in CloudFormation templates and deploy to multiple AZs automatically.

**Incorrect:**
- A: Systems Manager/Config/Elastic Beanstalk are not ideal for full infra replication here.
- C: Systems Manager/Config/Elastic Beanstalk are not ideal for full infra replication here.
- D: Systems Manager/Config/Elastic Beanstalk are not ideal for full infra replication here.


---

---

### Question #481

A company hosts a three-tier web application in the AWS Cloud. A Multi-AZAmazon RDS for MySQL server forms the database layer Amazon ElastiCache forms the cache layer. The company wants a caching strategy that adds or updates data in the cache when a customer adds an item to the database. The data in the cache must always match the data in the database. Which solution will meet these requirements?

- A. Implement the lazy loading caching strategy

- B. Implement the write-through caching strategy

- C. Implement the adding TTL caching strategy

- D. Implement the AWS AppCong caching strategy

**Correct:** B
**Why:** Write‑through writes to cache and DB simultaneously, keeping cache and DB in sync.

**Incorrect:**
- A: Lazy loading may serve stale data until cache refresh.
- C: TTL/AppConfig strategies don’t ensure immediate consistency.
- D: TTL/AppConfig strategies don’t ensure immediate consistency.


---

---

### Question #486

A company is building a three-tier application on AWS. The presentation tier will serve a static website The logic tier is a containerized application. This application will store data in a relational database. The company wants to simplify deployment and to reduce operational costs. Which solution will meet these requirements?

- A. Use Amazon S3 to host static content. Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for compute power. Use a managed Amazon RDS cluster for the database.

- B. Use Amazon CloudFront to host static content. Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 for compute power. Use a managed Amazon RDS cluster for the database.

- C. Use Amazon S3 to host static content. Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for compute power. Use a managed Amazon RDS cluster for the database.

- D. Use Amazon EC2 Reserved Instances to host static content. Use Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 for compute power. Use a managed Amazon RDS cluster for the database.

**Correct:** A
**Why:** S3 static hosting + ECS Fargate for containers + managed RDS minimizes ops and simplifies deployment.

**Incorrect:**
- B: CloudFront alone doesn’t host; EKS/EC2 increase ops.
- C: CloudFront alone doesn’t host; EKS/EC2 increase ops.
- D: CloudFront alone doesn’t host; EKS/EC2 increase ops.


---

---

### Question #490

A gaming company uses Amazon DynamoDB to store user information such as geographic location, player data, and leaderboards. The company needs to configure continuous backups to an Amazon S3 bucket with a minimal amount of coding. The backups must not affect availability of the application and must not affect the read capacity units (RCUs) that are dened for the table. Which solution meets these requirements?

- A. Use an Amazon EMR cluster. Create an Apache Hive job to back up the data to Amazon S3.

- B. Export the data directly from DynamoDB to Amazon S3 with continuous backups. Turn on point-in-time recovery for the table.

- C. Configure Amazon DynamoDB Streams. Create an AWS Lambda function to consume the stream and export the data to an Amazon S3 bucket.

- D. Create an AWS Lambda function to export the data from the database tables to Amazon S3 on a regular basis. Turn on point-in-time recovery for the table.

**Correct:** B
**Why:** DynamoDB table export to S3 provides continuous, server‑side exports without consuming RCUs and with no downtime; enable PITR.

**Incorrect:**
- A: EMR/Lambda/Streams add code/ops and may affect capacity.
- C: EMR/Lambda/Streams add code/ops and may affect capacity.
- D: EMR/Lambda/Streams add code/ops and may affect capacity.


---

---

### Question #495

A company is conducting an internal audit. The company wants to ensure that the data in an Amazon S3 bucket that is associated with the company’s AWS Lake Formation data lake does not contain sensitive customer or employee data. The company wants to discover personally identiable information (PII) or nancial information, including passport numbers and credit card numbers. Which solution will meet these requirements?

- A. Configure AWS Audit Manager on the account. Select the Payment Card Industry Data Security Standards (PCI DSS) for auditing.

- B. Configure Amazon S3 Inventory on the S3 bucket Configure Amazon Athena to query the inventory.

- C. Configure Amazon Macie to run a data discovery job that uses managed identiers for the required data types.

- D. Use Amazon S3 Select to run a report across the S3 bucket.

**Correct:** C
**Why:** Amazon Macie can scan S3 for PII/financial data with managed identifiers and produce findings.

**Incorrect:**
- A: Audit Manager/S3 Inventory/S3 Select do not detect PII patterns as Macie does.
- B: Audit Manager/S3 Inventory/S3 Select do not detect PII patterns as Macie does.
- D: Audit Manager/S3 Inventory/S3 Select do not detect PII patterns as Macie does.


---

## Amazon Redshift

### Question #269

An ecommerce company has noticed performance degradation of its Amazon RDS based web application. The performance degradation is attributed to an increase in the number of read-only SQL queries triggered by business analysts. A solutions architect needs to solve the problem with minimal changes to the existing web application. What should the solutions architect recommend?

- A. Export the data to Amazon DynamoDB and have the business analysts run their queries.

- B. Load the data into Amazon ElastiCache and have the business analysts run their queries.

- C. Create a read replica of the primary database and have the business analysts run their queries.

- D. Copy the data into an Amazon Redshift cluster and have the business analysts run their queries.

**Correct:** C
**Why:** Offload analysts’ queries to a read replica to protect primary performance with minimal changes.

**Incorrect:**
- A: DynamoDB/ElastiCache/Redshift require more changes.
- B: DynamoDB/ElastiCache/Redshift require more changes.
- D: DynamoDB/ElastiCache/Redshift require more changes.


---

---

### Question #278

A company wants to create an application to store employee data in a hierarchical structured relationship. The company needs a minimum-latency response to high-trac queries for the employee data and must protect any sensitive data. The company also needs to receive monthly email messages if any nancial information is present in the employee data. Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)

- A. Use Amazon Redshift to store the employee data in hierarchies. Unload the data to Amazon S3 every month.

- B. Use Amazon DynamoDB to store the employee data in hierarchies. Export the data to Amazon S3 every month.

- C. Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly events to AWS Lambda.

- D. Use Amazon Athena to analyze the employee data in Amazon S3. Integrate Athena with Amazon QuickSight to publish analysis dashboards and share the dashboards with users.

E. Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly notications through an Amazon Simple Notification Service (Amazon SNS) subscription.

**Correct:** B, E
**Why:** DynamoDB suits hierarchical, low‑latency queries; Macie + EventBridge → SNS can notify monthly when financial data is detected.

**Incorrect:**
- A: Redshift/Athena aren’t optimized for low‑latency hierarchical queries.
- C: Lambda trigger pattern is unnecessary for monthly notifications.
- D: Redshift/Athena aren’t optimized for low‑latency hierarchical queries.


---

---

### Question #308

A company has multiple AWS accounts that use consolidated billing. The company runs several active high performance Amazon RDS for Oracle On-Demand DB instances for 90 days. The company’s nance team has access to AWS Trusted Advisor in the consolidated billing account and all other AWS accounts. The nance team needs to use the appropriate AWS account to access the Trusted Advisor check recommendations for RDS. The nance team must review the appropriate Trusted Advisor check to reduce RDS costs. Which combination of steps should the nance team take to meet these requirements? (Choose two.)

- A. Use the Trusted Advisor recommendations from the account where the RDS instances are running.

- B. Use the Trusted Advisor recommendations from the consolidated billing account to see all RDS instance checks at the same time.

- C. Review the Trusted Advisor check for Amazon RDS Reserved Instance Optimization.

- D. Review the Trusted Advisor check for Amazon RDS Idle DB Instances.

E. Review the Trusted Advisor check for Amazon Redshift Reserved Node Optimization.

**Correct:** A, C
**Why:** View Trusted Advisor in the account where RDS runs and check the RDS Reserved Instance Optimization to reduce cost.

**Incorrect:**
- B: Consolidated billing account may not show member‑level service details for all checks.
- D: Idle DB/Redshift checks alone are insufficient or unrelated.
- E: Idle DB/Redshift checks alone are insufficient or unrelated.


---

---

### Question #314

A company has an on-premises MySQL database used by the global sales team with infrequent access patterns. The sales team requires the database to have minimal downtime. A database administrator wants to migrate this database to AWS without selecting a particular instance type in anticipation of more users in the future. Which service should a solutions architect recommend?

- A. Amazon Aurora MySQL

- B. Amazon Aurora Serverless for MySQL

- C. Amazon Redshift Spectrum

- D. Amazon RDS for MySQL

**Correct:** B
**Why:** Aurora Serverless for MySQL scales capacity automatically; supports minimal downtime migration and infrequent usage.

**Incorrect:**
- A: Fixed instances require capacity planning.
- C: Redshift Spectrum is for analytics, not OLTP.
- D: Fixed instances require capacity planning.


---

---

### Question #317

A company uses a legacy application to produce data in CSV format. The legacy application stores the output data in Amazon S3. The company is deploying a new commercial off-the-shelf (COTS) application that can perform complex SQL queries to analyze data that is stored in Amazon Redshift and Amazon S3 only. However, the COTS application cannot process the .csv files that the legacy application produces. The company cannot update the legacy application to produce data in another format. The company needs to implement a solution so that the COTS application can use the data that the legacy application produces. Which solution will meet these requirements with the LEAST operational overhead?

- A. Create an AWS Glue extract, transform, and load (ETL) job that runs on a schedule. Configure the ETL job to process the .csv files and store the processed data in Amazon Redshift.

- B. Develop a Python script that runs on Amazon EC2 instances to convert the .csv files to .sql files. Invoke the Python script on a cron schedule to store the output files in Amazon S3.

- C. Create an AWS Lambda function and an Amazon DynamoDB table. Use an S3 event to invoke the Lambda function. Configure the Lambda function to perform an extract, transform, and load (ETL) job to process the .csv files and store the processed data in the DynamoDB table.

- D. Use Amazon EventBridge to launch an Amazon EMR cluster on a weekly schedule. Configure the EMR cluster to perform an extract, transform, and load (ETL) job to process the .csv files and store the processed data in an Amazon Redshift table.

**Correct:** A
**Why:** Glue ETL job on a schedule to process CSVs and load into Redshift with least ops.

**Incorrect:**
- B: EC2 scripts, DynamoDB, or EMR add ops for the use case.
- C: EC2 scripts, DynamoDB, or EMR add ops for the use case.
- D: EC2 scripts, DynamoDB, or EMR add ops for the use case.


---

---

### Question #320

A company is using a eet of Amazon EC2 instances to ingest data from on-premises data sources. The data is in JSON format and ingestion rates can be as high as 1 MB/s. When an EC2 instance is rebooted, the data in-ight is lost. The company’s data science team wants to query ingested data in near-real time. Which solution provides near-real-time data querying that is scalable with minimal data loss?

- A. Publish data to Amazon Kinesis Data Streams, Use Kinesis Data Analytics to query the data.

- B. Publish data to Amazon Kinesis Data Firehose with Amazon Redshift as the destination. Use Amazon Redshift to query the data.

- C. Store ingested data in an EC2 instance store. Publish data to Amazon Kinesis Data Firehose with Amazon S3 as the destination. Use Amazon Athena to query the data.

- D. Store ingested data in an Amazon Elastic Block Store (Amazon EBS) volume. Publish data to Amazon ElastiCache for Redis. Subscribe to the Redis channel to query the data.

**Correct:** A
**Why:** Kinesis Data Streams with Kinesis Data Analytics provides near‑real‑time querying with minimal loss and scalability.

**Incorrect:**
- B: Firehose→Redshift has more latency; instance store/EBS risk data loss; Redis pub/sub isn’t durable.
- C: Firehose→Redshift has more latency; instance store/EBS risk data loss; Redis pub/sub isn’t durable.
- D: Firehose→Redshift has more latency; instance store/EBS risk data loss; Redis pub/sub isn’t durable.


---

---

### Question #416

A rapidly growing global ecommerce company is hosting its web application on AWS. The web application includes static content and dynamic content. The website stores online transaction processing (OLTP) data in an Amazon RDS database The website’s users are experiencing slow page loads. Which combination of actions should a solutions architect take to resolve this issue? (Choose two.)

- A. Configure an Amazon Redshift cluster.

- B. Set up an Amazon CloudFront distribution.

- C. Host the dynamic web content in Amazon S3.

- D. Create a read replica for the RDS DB instance.

E. Configure a Multi-AZ deployment for the RDS DB instance.

**Correct:** B, D
**Why:** CloudFront caches static/dynamic content at edge; RDS read replica offloads reads, improving page load.

**Incorrect:**
- A: Redshift is for OLAP, not OLTP page rendering.
- C: Dynamic content doesn’t belong in S3.
- E: Multi‑AZ is HA, not performance.


---

## Amazon Route 53

### Question #264

A company has a web application hosted over 10 Amazon EC2 instances with trac directed by Amazon Route 53. The company occasionally experiences a timeout error when attempting to browse the application. The networking team nds that some DNS queries return IP addresses of unhealthy instances, resulting in the timeout error. What should a solutions architect implement to overcome these timeout errors?

- A. Create a Route 53 simple routing policy record for each EC2 instance. Associate a health check with each record.

- B. Create a Route 53 failover routing policy record for each EC2 instance. Associate a health check with each record.

- C. Create an Amazon CloudFront distribution with EC2 instances as its origin. Associate a health check with the EC2 instances.

- D. Create an Application Load Balancer (ALB) with a health check in front of the EC2 instances. Route to the ALB from Route 53.

**Correct:** D
**Why:** Put an ALB with health checks in front of instances and route DNS to the ALB to avoid returning unhealthy instance IPs.

**Incorrect:**
- A: Route 53 alone still returns unhealthy A records without ALB health integration.
- B: Route 53 alone still returns unhealthy A records without ALB health integration.
- C: CloudFront is for CDN, not origin health for EC2 fleet.


---

---

### Question #272

A company serves a dynamic website from a eet of Amazon EC2 instances behind an Application Load Balancer (ALB). The website needs to support multiple languages to serve customers around the world. The website’s architecture is running in the us-west-1 Region and is exhibiting high request latency for users that are located in other parts of the world. The website needs to serve requests quickly and eciently regardless of a user’s location. However, the company does not want to recreate the existing architecture across multiple Regions. What should a solutions architect do to meet these requirements?

- A. Replace the existing architecture with a website that is served from an Amazon S3 bucket. Configure an Amazon CloudFront distribution with the S3 bucket as the origin. Set the cache behavior settings to cache based on the Accept-Language request header.

- B. Configure an Amazon CloudFront distribution with the ALB as the origin. Set the cache behavior settings to cache based on the Accept- Language request header.

- C. Create an Amazon API Gateway API that is integrated with the ALB. Configure the API to use the HTTP integration type. Set up an API Gateway stage to enable the API cache based on the Accept-Language request header.

- D. Launch an EC2 instance in each additional Region and configure NGINX to act as a cache server for that Region. Put all the EC2 instances and the ALB behind an Amazon Route 53 record set with a geolocation routing policy.

**Correct:** B
**Why:** CloudFront with ALB origin and cache based on Accept‑Language provides low latency globally without multi‑Region.

**Incorrect:**
- A: S3 static site or regional NGINX cache adds complexity or doesn’t fit dynamic origin.
- C: S3 static site or regional NGINX cache adds complexity or doesn’t fit dynamic origin.
- D: S3 static site or regional NGINX cache adds complexity or doesn’t fit dynamic origin.


---

---

### Question #294

An application that is hosted on Amazon EC2 instances needs to access an Amazon S3 bucket. Trac must not traverse the internet. How should a solutions architect configure access to meet these requirements?

- A. Create a private hosted zone by using Amazon Route 53.

- B. Set up a gateway VPC endpoint for Amazon S3 in the VPC.

- C. Configure the EC2 instances to use a NAT gateway to access the S3 bucket.

- D. Establish an AWS Site-to-Site VPN connection between the VPC and the S3 bucket.

**Correct:** B
**Why:** S3 gateway VPC endpoint keeps traffic private between EC2 and S3.

**Incorrect:**
- A: Private hosted zones/NAT/VPN are unnecessary for S3 private access.
- C: Private hosted zones/NAT/VPN are unnecessary for S3 private access.
- D: Private hosted zones/NAT/VPN are unnecessary for S3 private access.


---

---

### Question #323

A company’s facility has badge readers at every entrance throughout the building. When badges are scanned, the readers send a message over HTTPS to indicate who attempted to access that particular entrance. A solutions architect must design a system to process these messages from the sensors. The solution must be highly available, and the results must be made available for the company’s security team to analyze. Which system architecture should the solutions architect recommend?

- A. Launch an Amazon EC2 instance to serve as the HTTPS endpoint and to process the messages. Configure the EC2 instance to save the results to an Amazon S3 bucket.

- B. Create an HTTPS endpoint in Amazon API Gateway. Configure the API Gateway endpoint to invoke an AWS Lambda function to process the messages and save the results to an Amazon DynamoDB table.

- C. Use Amazon Route 53 to direct incoming sensor messages to an AWS Lambda function. Configure the Lambda function to process the messages and save the results to an Amazon DynamoDB table.

- D. Create a gateway VPC endpoint for Amazon S3. Configure a Site-to-Site VPN connection from the facility network to the VPC so that sensor data can be written directly to an S3 bucket by way of the VPC endpoint.

**Correct:** B
**Why:** API Gateway HTTPS endpoint invoking Lambda, storing to DynamoDB, is highly available and serverless.

**Incorrect:**
- A: EC2 endpoint or direct S3 via VPN increases ops/complexity.
- C: Route 53 cannot directly invoke code.
- D: EC2 endpoint or direct S3 via VPN increases ops/complexity.


---

---

### Question #367

A company is using Amazon Route 53 latency-based routing to route requests to its UDP-based application for users around the world. The application is hosted on redundant servers in the company's on-premises data centers in the United States, Asia, and Europe. The company’s compliance requirements state that the application must be hosted on premises. The company wants to improve the performance and availability of the application. What should a solutions architect do to meet these requirements?

- A. Configure three Network Load Balancers (NLBs) in the three AWS Regions to address the on-premises endpoints. Create an accelerator by using AWS Global Accelerator, and register the NLBs as its endpoints. Provide access to the application by using a CNAME that points to the accelerator DNS.

- B. Configure three Application Load Balancers (ALBs) in the three AWS Regions to address the on-premises endpoints. Create an accelerator by using AWS Global Accelerator, and register the ALBs as its endpoints. Provide access to the application by using a CNAME that points to the accelerator DNS.

- C. Configure three Network Load Balancers (NLBs) in the three AWS Regions to address the on-premises endpoints. In Route 53, create a latency-based record that points to the three NLBs, and use it as an origin for an Amazon CloudFront distribution. Provide access to the application by using a CNAME that points to the CloudFront DNS.

- D. Configure three Application Load Balancers (ALBs) in the three AWS Regions to address the on-premises endpoints. In Route 53, create a latency-based record that points to the three ALBs, and use it as an origin for an Amazon CloudFront distribution. Provide access to the application by using a CNAME that points to the CloudFront DNS.

**Correct:** A
**Why:** Place NLBs in three Regions that point to on‑prem endpoints; use Global Accelerator to improve performance/availability for UDP traffic while keeping hosting on‑prem.

**Incorrect:**
- B: ALB does not support UDP.
- C: CloudFront is HTTP/HTTPS and not for UDP; also adds unnecessary layers.
- D: CloudFront is HTTP/HTTPS and not for UDP; also adds unnecessary layers.


---

---

### Question #378

A company is developing a real-time multiplayer game that uses UDP for communications between the client and servers in an Auto Scaling group. Spikes in demand are anticipated during the day, so the game server platform must adapt accordingly. Developers want to store gamer scores and other non-relational data in a database solution that will scale without intervention. Which solution should a solutions architect recommend?

- A. Use Amazon Route 53 for trac distribution and Amazon Aurora Serverless for data storage.

- B. Use a Network Load Balancer for trac distribution and Amazon DynamoDB on-demand for data storage.

- C. Use a Network Load Balancer for trac distribution and Amazon Aurora Global Database for data storage.

- D. Use an Application Load Balancer for trac distribution and Amazon DynamoDB global tables for data storage.

**Correct:** B
**Why:** NLB supports UDP for game traffic; DynamoDB on‑demand scales automatically for storing scores/non‑relational data.

**Incorrect:**
- A: Aurora is relational and requires capacity planning.
- C: Aurora is relational and requires capacity planning.
- D: ALB does not support UDP.


---

---

### Question #408

A company runs an application that receives data from thousands of geographically dispersed remote devices that use UDP. The application processes the data immediately and sends a message back to the device if necessary. No data is stored. The company needs a solution that minimizes latency for the data transmission from the devices. The solution also must provide rapid failover to another AWS Region. Which solution will meet these requirements?

- A. Configure an Amazon Route 53 failover routing policy. Create a Network Load Balancer (NLB) in each of the two Regions. Configure the NLB to invoke an AWS Lambda function to process the data.

- B. Use AWS Global Accelerator. Create a Network Load Balancer (NLB) in each of the two Regions as an endpoint. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the NLProcess the data in Amazon ECS.

- C. Use AWS Global Accelerator. Create an Application Load Balancer (ALB) in each of the two Regions as an endpoint. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the ALB. Process the data in Amazon ECS.

- D. Configure an Amazon Route 53 failover routing policy. Create an Application Load Balancer (ALB) in each of the two Regions. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the ALB. Process the data in Amazon ECS.

**Correct:** B
**Why:** Global Accelerator reduces latency globally and provides fast failover; NLB supports UDP; ECS Fargate scales to process data.

**Incorrect:**
- A: Route 53 failover alone doesn’t optimize latency.
- C: ALB does not support UDP.
- D: Same as A plus ALB (no UDP).


---

---

### Question #434

A company hosts its application in the AWS Cloud. The application runs on Amazon EC2 instances behind an Elastic Load Balancer in an Auto Scaling group and with an Amazon DynamoDB table. The company wants to ensure the application can be made available in anotherAWS Region with minimal downtime. What should a solutions architect do to meet these requirements with the LEAST amount of downtime?

- A. Create an Auto Scaling group and a load balancer in the disaster recovery Region. Configure the DynamoDB table as a global table. Configure DNS failover to point to the new disaster recovery Region's load balancer.

- B. Create an AWS CloudFormation template to create EC2 instances, load balancers, and DynamoDB tables to be launched when needed Configure DNS failover to point to the new disaster recovery Region's load balancer.

- C. Create an AWS CloudFormation template to create EC2 instances and a load balancer to be launched when needed. Configure the DynamoDB table as a global table. Configure DNS failover to point to the new disaster recovery Region's load balancer.

- D. Create an Auto Scaling group and load balancer in the disaster recovery Region. Configure the DynamoDB table as a global table. Create an Amazon CloudWatch alarm to trigger an AWS Lambda function that updates Amazon Route 53 pointing to the disaster recovery load balancer.

**Correct:** A
**Why:** Pre‑provision ALB/ASG in DR Region and use DynamoDB global tables; DNS failover minimizes downtime.

**Incorrect:**
- B: Delayed provisioning increases downtime.
- C: Delayed provisioning increases downtime.
- D: Extra Lambda automation is unnecessary.


---

---

### Question #447

A company has a stateless web application that runs on AWS Lambda functions that are invoked by Amazon API Gateway. The company wants to deploy the application across multiple AWS Regions to provide Regional failover capabilities. What should a solutions architect do to route trac to multiple Regions?

- A. Create Amazon Route 53 health checks for each Region. Use an active-active failover configuration.

- B. Create an Amazon CloudFront distribution with an origin for each Region. Use CloudFront health checks to route trac.

- C. Create a transit gateway. Attach the transit gateway to the API Gateway endpoint in each Region. Configure the transit gateway to route requests.

- D. Create an Application Load Balancer in the primary Region. Set the target group to point to the API Gateway endpoint hostnames in each Region.

**Correct:** A
**Why:** Use Route 53 health checks with active‑active routing across Regions for serverless endpoints.

**Incorrect:**
- B: CloudFront is for HTTP caching, not API multi‑Region routing.
- C: Transit gateway/ALB cannot directly route to API Gateway across Regions as required.
- D: Transit gateway/ALB cannot directly route to API Gateway across Regions as required.


---

---

### Question #464

A company hosts an online shopping application that stores all orders in an Amazon RDS for PostgreSQL Single-AZ DB instance. Management wants to eliminate single points of failure and has asked a solutions architect to recommend an approach to minimize database downtime without requiring any changes to the application code. Which solution meets these requirements?

- A. Convert the existing database instance to a Multi-AZ deployment by modifying the database instance and specifying the Multi-AZ option.

- B. Create a new RDS Multi-AZ deployment. Take a snapshot of the current RDS instance and restore the new Multi-AZ deployment with the snapshot.

- C. Create a read-only replica of the PostgreSQL database in another Availability Zone. Use Amazon Route 53 weighted record sets to distribute requests across the databases.

- D. Place the RDS for PostgreSQL database in an Amazon EC2 Auto Scaling group with a minimum group size of two. Use Amazon Route 53 weighted record sets to distribute requests across instances.

**Correct:** A
**Why:** Modifying the RDS instance to Multi‑AZ provides automatic failover with minimal downtime and no app changes.

**Incorrect:**
- B: Spinning up a new deployment adds downtime.
- C: Read replicas/EC2 ASG do not provide transparent failover for writes.
- D: Read replicas/EC2 ASG do not provide transparent failover for writes.


---

---

### Question #466

A company designed a stateless two-tier application that uses Amazon EC2 in a single Availability Zone and an Amazon RDS Multi-AZ DB instance. New company management wants to ensure the application is highly available. What should a solutions architect do to meet this requirement?

- A. Configure the application to use Multi-AZ EC2 Auto Scaling and create an Application Load Balancer

- B. Configure the application to take snapshots of the EC2 instances and send them to a different AWS Region

- C. Configure the application to use Amazon Route 53 latency-based routing to feed requests to the application

- D. Configure Amazon Route 53 rules to handle incoming requests and create a Multi-AZ Application Load Balancer

**Correct:** A
**Why:** Make the stateless tier highly available with Multi‑AZ Auto Scaling and an ALB; DB is already Multi‑AZ.

**Incorrect:**
- B: Snapshots/Route 53 alone do not provide app‑tier HA.
- C: Snapshots/Route 53 alone do not provide app‑tier HA.
- D: Snapshots/Route 53 alone do not provide app‑tier HA.


---

## Amazon S3

### Question #252

A solutions architect needs to design a system to store client case files. The files are core company assets and are important. The number of files will grow over time. The files must be simultaneously accessible from multiple application servers that run on Amazon EC2 instances. The solution must have built-in redundancy. Which solution meets these requirements?

- A. Amazon Elastic File System (Amazon EFS)

- B. Amazon Elastic Block Store (Amazon EBS)

- C. Amazon S3 Glacier Deep Archive

- D. AWS Backup

**Correct:** A
**Why:** Amazon EFS provides shared, redundant NFS storage accessible concurrently by multiple EC2 instances.

**Incorrect:**
- B: EBS is block storage attached to a single AZ/instance at a time.
- C: Glacier Deep Archive is archival, not shared storage.
- D: AWS Backup is for backups, not primary shared storage.


---

---

### Question #256

A solutions architect is implementing a document review application using an Amazon S3 bucket for storage. The solution must prevent accidental deletion of the documents and ensure that all versions of the documents are available. Users must be able to download, modify, and upload documents. Which combination of actions should be taken to meet these requirements? (Choose two.)

- A. Enable a read-only bucket ACL.

- B. Enable versioning on the bucket.

- C. Attach an IAM policy to the bucket.

- D. Enable MFA Delete on the bucket.

E. Encrypt the bucket using AWS KMS.

**Correct:** B, D
**Why:** Versioning retains all versions; MFA Delete protects against accidental/unauthorized permanent deletions.

**Incorrect:**
- A: ACLs/policies/KMS do not ensure version retention or deletion protection by themselves.
- C: ACLs/policies/KMS do not ensure version retention or deletion protection by themselves.
- E: ACLs/policies/KMS do not ensure version retention or deletion protection by themselves.


---

---

### Question #257

A company is building a solution that will report Amazon EC2 Auto Scaling events across all the applications in an AWS account. The company needs to use a serverless solution to store the EC2 Auto Scaling status data in Amazon S3. The company then will use the data in Amazon S3 to provide near-real-time updates in a dashboard. The solution must not affect the speed of EC2 instance launches. How should the company move the data to Amazon S3 to meet these requirements?

- A. Use an Amazon CloudWatch metric stream to send the EC2 Auto Scaling status data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.

- B. Launch an Amazon EMR cluster to collect the EC2 Auto Scaling status data and send the data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.

- C. Create an Amazon EventBridge rule to invoke an AWS Lambda function on a schedule. Configure the Lambda function to send the EC2 Auto Scaling status data directly to Amazon S3.

- D. Use a bootstrap script during the launch of an EC2 instance to install Amazon Kinesis Agent. Configure Kinesis Agent to collect the EC2 Auto Scaling status data and send the data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.

**Correct:** A
**Why:** CloudWatch metric streams → Kinesis Data Firehose → S3 is serverless and near real time without impacting launches.

**Incorrect:**
- B: EMR/Kinesis Agent add unnecessary infrastructure.
- C: Scheduled polling adds latency and complexity.
- D: EMR/Kinesis Agent add unnecessary infrastructure.


---

---

### Question #258

A company has an application that places hundreds of .csv files into an Amazon S3 bucket every hour. The files are 1 GB in size. Each time a file is uploaded, the company needs to convert the file to Apache Parquet format and place the output file into an S3 bucket. Which solution will meet these requirements with the LEAST operational overhead?

- A. Create an AWS Lambda function to download the .csv files, convert the files to Parquet format, and place the output files in an S3 bucket. Invoke the Lambda function for each S3 PUT event.

- B. Create an Apache Spark job to read the .csv files, convert the files to Parquet format, and place the output files in an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the Spark job.

- C. Create an AWS Glue table and an AWS Glue crawler for the S3 bucket where the application places the .csv files. Schedule an AWS Lambda function to periodically use Amazon Athena to query the AWS Glue table, convert the query results into Parquet format, and place the output files into an S3 bucket.

- D. Create an AWS Glue extract, transform, and load (ETL) job to convert the .csv files to Parquet format and place the output files into an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the ETL job.

**Correct:** D
**Why:** AWS Glue ETL is managed and scalable; invoke job on S3 PUT via Lambda for timely conversion to Parquet.

**Incorrect:**
- A: Lambda alone is inefficient for 1‑GB files at scale.
- B: EMR or Athena‑based conversions add more ops/indirection.
- C: EMR or Athena‑based conversions add more ops/indirection.


---

---

### Question #259

A company is implementing new data retention policies for all databases that run on Amazon RDS DB instances. The company must retain daily backups for a minimum period of 2 years. The backups must be consistent and restorable. Which solution should a solutions architect recommend to meet these requirements?

- A. Create a backup vault in AWS Backup to retain RDS backups. Create a new backup plan with a daily schedule and an expiration period of 2 years after creation. Assign the RDS DB instances to the backup plan.

- B. Configure a backup window for the RDS DB instances for daily snapshots. Assign a snapshot retention policy of 2 years to each RDS DB instance. Use Amazon Data Lifecycle Manager (Amazon DLM) to schedule snapshot deletions.

- C. Configure database transaction logs to be automatically backed up to Amazon CloudWatch Logs with an expiration period of 2 years.

- D. Configure an AWS Database Migration Service (AWS DMS) replication task. Deploy a replication instance, and configure a change data capture (CDC) task to stream database changes to Amazon S3 as the target. Configure S3 Lifecycle policies to delete the snapshots after 2 years.

**Correct:** A
**Why:** AWS Backup can enforce daily backups and retain for 2 years with minimal ops.

**Incorrect:**
- B: DLM is for EBS; RDS snapshots retention isn’t managed that way.
- C: CloudWatch logs/DMS exports are not consistent RDS backups.
- D: CloudWatch logs/DMS exports are not consistent RDS backups.


---

---

### Question #266

A company has a popular gaming platform running on AWS. The application is sensitive to latency because latency can impact the user experience and introduce unfair advantages to some players. The application is deployed in every AWS Region. It runs on Amazon EC2 instances that are part of Auto Scaling groups congured behind Application Load Balancers (ALBs). A solutions architect needs to implement a mechanism to monitor the health of the application and redirect trac to healthy endpoints. Which solution meets these requirements?

- A. Configure an accelerator in AWS Global Accelerator. Add a listener for the port that the application listens on, and attach it to a Regional endpoint in each Region. Add the ALB as the endpoint.

- B. Create an Amazon CloudFront distribution and specify the ALB as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the trac.

- C. Create an Amazon CloudFront distribution and specify Amazon S3 as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the trac.

- D. Configure an Amazon DynamoDB database to serve as the data store for the application. Create a DynamoDB Accelerator (DAX) cluster to act as the in-memory cache for DynamoDB hosting the application data.

**Correct:** A
**Why:** Global Accelerator provides health checks and intelligent routing to the nearest healthy Regional endpoint (ALB).

**Incorrect:**
- B: CloudFront/S3 or DynamoDB/DAX don’t meet UDP/TCP app health redirect needs across Regions.
- C: CloudFront/S3 or DynamoDB/DAX don’t meet UDP/TCP app health redirect needs across Regions.
- D: CloudFront/S3 or DynamoDB/DAX don’t meet UDP/TCP app health redirect needs across Regions.


---

---

### Question #267

A company has one million users that use its mobile app. The company must analyze the data usage in near-real time. The company also must encrypt the data in near-real time and must store the data in a centralized location in Apache Parquet format for further processing. Which solution will meet these requirements with the LEAST operational overhead?

- A. Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon Kinesis Data Analytics application to analyze the data. Invoke an AWS Lambda function to send the data to the Kinesis Data Analytics application.

- B. Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon EMR cluster to analyze the data. Invoke an AWS Lambda function to send the data to the EMR cluster.

- C. Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create an Amazon EMR cluster to analyze the data.

- D. Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create an Amazon Kinesis Data Analytics application to analyze the data.

**Correct:** A
**Why:** Kinesis Data Streams → Kinesis Data Analytics for near real‑time analytics, Firehose to S3 with Parquet conversion; Athena for SQL.

**Incorrect:**
- B: Do not provide both real‑time analytics and Parquet delivery with least ops as cleanly.
- C: Do not provide both real‑time analytics and Parquet delivery with least ops as cleanly.
- D: Do not provide both real‑time analytics and Parquet delivery with least ops as cleanly.


---

---

### Question #270

A company is using a centralized AWS account to store log data in various Amazon S3 buckets. A solutions architect needs to ensure that the data is encrypted at rest before the data is uploaded to the S3 buckets. The data also must be encrypted in transit. Which solution meets these requirements?

- A. Use client-side encryption to encrypt the data that is being uploaded to the S3 buckets.

- B. Use server-side encryption to encrypt the data that is being uploaded to the S3 buckets.

- C. Create bucket policies that require the use of server-side encryption with S3 managed encryption keys (SSE-S3) for S3 uploads.

- D. Enable the security option to encrypt the S3 buckets through the use of a default AWS Key Management Service (AWS KMS) key.

**Correct:** C
**Why:** Bucket policies can require SSE (e.g., deny if x‑amz‑server‑side‑encryption is missing) to ensure encryption at rest; clients typically use HTTPS by default.

**Incorrect:**
- A: Client‑side encryption is heavier and not enforced server‑side.
- B: Do not enforce encryption upon upload by policy.
- D: Do not enforce encryption upon upload by policy.


---

---

### Question #272

A company serves a dynamic website from a eet of Amazon EC2 instances behind an Application Load Balancer (ALB). The website needs to support multiple languages to serve customers around the world. The website’s architecture is running in the us-west-1 Region and is exhibiting high request latency for users that are located in other parts of the world. The website needs to serve requests quickly and eciently regardless of a user’s location. However, the company does not want to recreate the existing architecture across multiple Regions. What should a solutions architect do to meet these requirements?

- A. Replace the existing architecture with a website that is served from an Amazon S3 bucket. Configure an Amazon CloudFront distribution with the S3 bucket as the origin. Set the cache behavior settings to cache based on the Accept-Language request header.

- B. Configure an Amazon CloudFront distribution with the ALB as the origin. Set the cache behavior settings to cache based on the Accept- Language request header.

- C. Create an Amazon API Gateway API that is integrated with the ALB. Configure the API to use the HTTP integration type. Set up an API Gateway stage to enable the API cache based on the Accept-Language request header.

- D. Launch an EC2 instance in each additional Region and configure NGINX to act as a cache server for that Region. Put all the EC2 instances and the ALB behind an Amazon Route 53 record set with a geolocation routing policy.

**Correct:** B
**Why:** CloudFront with ALB origin and cache based on Accept‑Language provides low latency globally without multi‑Region.

**Incorrect:**
- A: S3 static site or regional NGINX cache adds complexity or doesn’t fit dynamic origin.
- C: S3 static site or regional NGINX cache adds complexity or doesn’t fit dynamic origin.
- D: S3 static site or regional NGINX cache adds complexity or doesn’t fit dynamic origin.


---

---

### Question #277

A company provides an online service for posting video content and transcoding it for use by any mobile platform. The application architecture uses Amazon Elastic File System (Amazon EFS) Standard to collect and store the videos so that multiple Amazon EC2 Linux instances can access the video content for processing. As the popularity of the service has grown over time, the storage costs have become too expensive. Which storage solution is MOST cost-effective?

- A. Use AWS Storage Gateway for files to store and process the video content.

- B. Use AWS Storage Gateway for volumes to store and process the video content.

- C. Use Amazon EFS for storing the video content. Once processing is complete, transfer the files to Amazon Elastic Block Store (Amazon EBS).

- D. Use Amazon S3 for storing the video content. Move the files temporarily over to an Amazon Elastic Block Store (Amazon EBS) volume attached to the server for processing.

**Correct:** D
**Why:** Store content durably/cost‑effectively in S3 and move to EBS temporarily for processing on EC2.

**Incorrect:**
- A: Storage Gateway is not optimal for large‑scale processing cost.
- B: Storage Gateway is not optimal for large‑scale processing cost.
- C: Keeping in EFS Standard is costly for infrequently accessed media.


---

---

### Question #278

A company wants to create an application to store employee data in a hierarchical structured relationship. The company needs a minimum-latency response to high-trac queries for the employee data and must protect any sensitive data. The company also needs to receive monthly email messages if any nancial information is present in the employee data. Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)

- A. Use Amazon Redshift to store the employee data in hierarchies. Unload the data to Amazon S3 every month.

- B. Use Amazon DynamoDB to store the employee data in hierarchies. Export the data to Amazon S3 every month.

- C. Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly events to AWS Lambda.

- D. Use Amazon Athena to analyze the employee data in Amazon S3. Integrate Athena with Amazon QuickSight to publish analysis dashboards and share the dashboards with users.

E. Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly notications through an Amazon Simple Notification Service (Amazon SNS) subscription.

**Correct:** B, E
**Why:** DynamoDB suits hierarchical, low‑latency queries; Macie + EventBridge → SNS can notify monthly when financial data is detected.

**Incorrect:**
- A: Redshift/Athena aren’t optimized for low‑latency hierarchical queries.
- C: Lambda trigger pattern is unnecessary for monthly notifications.
- D: Redshift/Athena aren’t optimized for low‑latency hierarchical queries.


---

---

### Question #279

A company has an application that is backed by an Amazon DynamoDB table. The company’s compliance requirements specify that database backups must be taken every month, must be available for 6 months, and must be retained for 7 years. Which solution will meet these requirements?

- A. Create an AWS Backup plan to back up the DynamoDB table on the first day of each month. Specify a lifecycle policy that transitions the backup to cold storage after 6 months. Set the retention period for each backup to 7 years.

- B. Create a DynamoDB on-demand backup of the DynamoDB table on the first day of each month. Transition the backup to Amazon S3 Glacier Flexible Retrieval after 6 months. Create an S3 Lifecycle policy to delete backups that are older than 7 years.

- C. Use the AWS SDK to develop a script that creates an on-demand backup of the DynamoDB table. Set up an Amazon EventBridge rule that runs the script on the first day of each month. Create a second script that will run on the second day of each month to transition DynamoDB backups that are older than 6 months to cold storage and to delete backups that are older than 7 years.

- D. Use the AWS CLI to create an on-demand backup of the DynamoDB table. Set up an Amazon EventBridge rule that runs the command on the first day of each month with a cron expression. Specify in the command to transition the backups to cold storage after 6 months and to delete the backups after 7 years.

**Correct:** A
**Why:** AWS Backup monthly backup plan with lifecycle to cold storage after 6 months and 7‑year retention meets requirements.

**Incorrect:**
- B: Manual or partial solutions add ops and risk consistency.
- C: Manual or partial solutions add ops and risk consistency.
- D: Manual or partial solutions add ops and risk consistency.


---

---

### Question #280

A company is using Amazon CloudFront with its website. The company has enabled logging on the CloudFront distribution, and logs are saved in one of the company’s Amazon S3 buckets. The company needs to perform advanced analyses on the logs and build visualizations. What should a solutions architect do to meet these requirements?

- A. Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket. Visualize the results with AWS Glue.

- B. Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket. Visualize the results with Amazon QuickSight.

- C. Use standard SQL queries in Amazon DynamoDB to analyze the CloudFront logs in the S3 bucket. Visualize the results with AWS Glue.

- D. Use standard SQL queries in Amazon DynamoDB to analyze the CloudFront logs in the S3 bucket. Visualize the results with Amazon QuickSight.

**Correct:** B
**Why:** Use Athena to query CloudFront logs in S3 and QuickSight for visualizations.

**Incorrect:**
- A: Glue/DynamoDB are not needed for log analysis/visualization here.
- C: Glue/DynamoDB are not needed for log analysis/visualization here.
- D: Glue/DynamoDB are not needed for log analysis/visualization here.


---

---

### Question #283

A research company runs experiments that are powered by a simulation application and a visualization application. The simulation application runs on Linux and outputs intermediate data to an NFS share every 5 minutes. The visualization application is a Windows desktop application that displays the simulation output and requires an SMB file system. The company maintains two synchronized file systems. This strategy is causing data duplication and inecient resource usage. The company needs to migrate the applications to AWS without making code changes to either application. Which solution will meet these requirements?

- A. Migrate both applications to AWS Lambda. Create an Amazon S3 bucket to exchange data between the applications.

- B. Migrate both applications to Amazon Elastic Container Service (Amazon ECS). Configure Amazon FSx File Gateway for storage.

- C. Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization application to Windows EC2 instances. Configure Amazon Simple Queue Service (Amazon SQS) to exchange data between the applications.

- D. Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization application to Windows EC2 instances. Configure Amazon FSx for NetApp ONTAP for storage.

**Correct:** D
**Why:** FSx for NetApp ONTAP supports both NFS and SMB on the same data, removing duplicate storage and maintaining app compatibility.

**Incorrect:**
- A: Lambda/ECS + File Gateway or SQS don’t solve multi‑protocol file sharing.
- B: Lambda/ECS + File Gateway or SQS don’t solve multi‑protocol file sharing.
- C: Lambda/ECS + File Gateway or SQS don’t solve multi‑protocol file sharing.


---

---

### Question #285

A company hosts its static website by using Amazon S3. The company wants to add a contact form to its webpage. The contact form will have dynamic server-side components for users to input their name, email address, phone number, and user message. The company anticipates that there will be fewer than 100 site visits each month. Which solution will meet these requirements MOST cost-effectively?

- A. Host a dynamic contact form page in Amazon Elastic Container Service (Amazon ECS). Set up Amazon Simple Email Service (Amazon SES) to connect to any third-party email provider.

- B. Create an Amazon API Gateway endpoint with an AWS Lambda backend that makes a call to Amazon Simple Email Service (Amazon SES).

- C. Convert the static webpage to dynamic by deploying Amazon Lightsail. Use client-side scripting to build the contact form. Integrate the form with Amazon WorkMail.

- D. Create a t2.micro Amazon EC2 instance. Deploy a LAMP (Linux, Apache, MySQL, PHP/Perl/Python) stack to host the webpage. Use client- side scripting to build the contact form. Integrate the form with Amazon WorkMail.

**Correct:** B
**Why:** API Gateway + Lambda → SES adds minimal serverless backend to a static S3 site.

**Incorrect:**
- A: ECS/Lightsail/EC2 add cost and ops for very low traffic.
- C: ECS/Lightsail/EC2 add cost and ops for very low traffic.
- D: ECS/Lightsail/EC2 add cost and ops for very low traffic.


---

---

### Question #286

A company has a static website that is hosted on Amazon CloudFront in front of Amazon S3. The static website uses a database backend. The company notices that the website does not reect updates that have been made in the website’s Git repository. The company checks the continuous integration and continuous delivery (CI/CD) pipeline between the Git repository and Amazon S3. The company veries that the webhooks are congured properly and that the CI/CD pipeline is sending messages that indicate successful deployments. A solutions architect needs to implement a solution that displays the updates on the website. Which solution will meet these requirements?

- A. Add an Application Load Balancer.

- B. Add Amazon ElastiCache for Redis or Memcached to the database layer of the web application.

- C. Invalidate the CloudFront cache.

- D. Use AWS Certicate Manager (ACM) to validate the website’s SSL certicate.

**Correct:** C
**Why:** Invalidate the CloudFront cache so the CDN serves the latest content from S3.

**Incorrect:**
- A: ALB/ElastiCache/ACM do not address CDN cache staleness.
- B: ALB/ElastiCache/ACM do not address CDN cache staleness.
- D: ALB/ElastiCache/ACM do not address CDN cache staleness.


---

---

### Question #287

A company wants to migrate a Windows-based application from on premises to the AWS Cloud. The application has three tiers: an application tier, a business tier, and a database tier with Microsoft SQL Server. The company wants to use specic features of SQL Server such as native backups and Data Quality Services. The company also needs to share files for processing between the tiers. How should a solutions architect design the architecture to meet these requirements?

- A. Host all three tiers on Amazon EC2 instances. Use Amazon FSx File Gateway for file sharing between the tiers.

- B. Host all three tiers on Amazon EC2 instances. Use Amazon FSx for Windows File Server for file sharing between the tiers.

- C. Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon RDS. Use Amazon Elastic File System (Amazon EFS) for file sharing between the tiers.

- D. Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon RDS. Use a Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) volume for file sharing between the tiers.

**Correct:** B
**Why:** Keep SQL Server features by hosting DB on EC2; use FSx for Windows File Server for shared files; EC2 for app tiers.

**Incorrect:**
- C: RDS may not support required SQL Server features; EBS for file sharing is inappropriate.
- D: RDS may not support required SQL Server features; EBS for file sharing is inappropriate.


---

---

### Question #288

A company is migrating a Linux-based web server group to AWS. The web servers must access files in a shared file store for some content. The company must not make any changes to the application. What should a solutions architect do to meet these requirements?

- A. Create an Amazon S3 Standard bucket with access to the web servers.

- B. Configure an Amazon CloudFront distribution with an Amazon S3 bucket as the origin.

- C. Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on all web servers.

- D. Configure a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume to all web servers.

**Correct:** C
**Why:** EFS provides a POSIX shared file system for Linux without app changes.

**Incorrect:**
- A: S3/CloudFront are object/CDN; EBS is single‑attach.
- B: S3/CloudFront are object/CDN; EBS is single‑attach.
- D: S3/CloudFront are object/CDN; EBS is single‑attach.


---

---

### Question #289

A company has an AWS Lambda function that needs read access to an Amazon S3 bucket that is located in the same AWS account. Which solution will meet these requirements in the MOST secure manner?

- A. Apply an S3 bucket policy that grants read access to the S3 bucket.

- B. Apply an IAM role to the Lambda function. Apply an IAM policy to the role to grant read access to the S3 bucket.

- C. Embed an access key and a secret key in the Lambda function’s code to grant the required IAM permissions for read access to the S3 bucket.

- D. Apply an IAM role to the Lambda function. Apply an IAM policy to the role to grant read access to all S3 buckets in the account.

**Correct:** B
**Why:** Attach an IAM role to Lambda with least‑privilege S3 read access to the specific bucket.

**Incorrect:**
- A: Bucket‑wide or all‑buckets access is overbroad.
- C: Never embed keys in code.
- D: Bucket‑wide or all‑buckets access is overbroad.


---

---

### Question #291

A media company uses Amazon CloudFront for its publicly available streaming video content. The company wants to secure the video content that is hosted in Amazon S3 by controlling who has access. Some of the company’s users are using a custom HTTP client that does not support cookies. Some of the company’s users are unable to change the hardcoded URLs that they are using for access. Which services or methods will meet these requirements with the LEAST impact to the users? (Choose two.)

- A. Signed cookies

- B. Signed URLs

- C. AWS AppSync

- D. JSON Web Token (JWT)

E. AWS Secrets Manager

**Correct:** A, B
**Why:** Use signed cookies where clients support cookies and signed URLs where they don’t or URLs are hardcoded.

**Incorrect:**
- C: AppSync/JWT/Secrets Manager do not solve CloudFront authorization distribution.
- D: AppSync/JWT/Secrets Manager do not solve CloudFront authorization distribution.
- E: AppSync/JWT/Secrets Manager do not solve CloudFront authorization distribution.


---

---

### Question #292

A company is preparing a new data platform that will ingest real-time streaming data from multiple sources. The company needs to transform the data before writing the data to Amazon S3. The company needs the ability to use SQL to query the transformed data. Which solutions will meet these requirements? (Choose two.)

- A. Use Amazon Kinesis Data Streams to stream the data. Use Amazon Kinesis Data Analytics to transform the data. Use Amazon Kinesis Data Firehose to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.

- B. Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use AWS Glue to transform the data and to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.

- C. Use AWS Database Migration Service (AWS DMS) to ingest the data. Use Amazon EMR to transform the data and to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.

- D. Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use Amazon Kinesis Data Analytics to transform the data and to write the data to Amazon S3. Use the Amazon RDS query editor to query the transformed data from Amazon S3.

E. Use Amazon Kinesis Data Streams to stream the data. Use AWS Glue to transform the data. Use Amazon Kinesis Data Firehose to write the data to Amazon S3. Use the Amazon RDS query editor to query the transformed data from Amazon S3.

**Correct:** A, B
**Why:** Kinesis Data Streams + KDA + Firehose + S3 + Athena, or MSK + Glue + S3 + Athena both satisfy transform to S3 and SQL on data.

**Incorrect:**
- C: DMS/Incorrect query endpoints do not meet requirements fully.
- D: DMS/Incorrect query endpoints do not meet requirements fully.
- E: DMS/Incorrect query endpoints do not meet requirements fully.


---

---

### Question #293

A company has an on-premises volume backup solution that has reached its end of life. The company wants to use AWS as part of a new backup solution and wants to maintain local access to all the data while it is backed up on AWS. The company wants to ensure that the data backed up on AWS is automatically and securely transferred. Which solution meets these requirements?

- A. Use AWS Snowball to migrate data out of the on-premises solution to Amazon S3. Configure on-premises systems to mount the Snowball S3 endpoint to provide local access to the data.

- B. Use AWS Snowball Edge to migrate data out of the on-premises solution to Amazon S3. Use the Snowball Edge file interface to provide on- premises systems with local access to the data.

- C. Use AWS Storage Gateway and configure a cached volume gateway. Run the Storage Gateway software appliance on premises and configure a percentage of data to cache locally. Mount the gateway storage volumes to provide local access to the data.

- D. Use AWS Storage Gateway and configure a stored volume gateway. Run the Storage Gateway software appliance on premises and map the gateway storage volumes to on-premises storage. Mount the gateway storage volumes to provide local access to the data.

**Correct:** C
**Why:** Volume Gateway cached volumes keep recent data local, store full data in S3, and throttle bandwidth; provides local access.

**Incorrect:**
- A: Snow devices are one‑time migrations, not ongoing backups with local access.
- B: Snow devices are one‑time migrations, not ongoing backups with local access.
- D: Stored volumes keep full copy local, not desired here to minimize on‑prem scale.


---

---

### Question #294

An application that is hosted on Amazon EC2 instances needs to access an Amazon S3 bucket. Trac must not traverse the internet. How should a solutions architect configure access to meet these requirements?

- A. Create a private hosted zone by using Amazon Route 53.

- B. Set up a gateway VPC endpoint for Amazon S3 in the VPC.

- C. Configure the EC2 instances to use a NAT gateway to access the S3 bucket.

- D. Establish an AWS Site-to-Site VPN connection between the VPC and the S3 bucket.

**Correct:** B
**Why:** S3 gateway VPC endpoint keeps traffic private between EC2 and S3.

**Incorrect:**
- A: Private hosted zones/NAT/VPN are unnecessary for S3 private access.
- C: Private hosted zones/NAT/VPN are unnecessary for S3 private access.
- D: Private hosted zones/NAT/VPN are unnecessary for S3 private access.


---

---

### Question #295

An ecommerce company stores terabytes of customer data in the AWS Cloud. The data contains personally identiable information (PII). The company wants to use the data in three applications. Only one of the applications needs to process the PII. The PII must be removed before the other two applications process the data. Which solution will meet these requirements with the LEAST operational overhead?

- A. Store the data in an Amazon DynamoDB table. Create a proxy application layer to intercept and process the data that each application requests.

- B. Store the data in an Amazon S3 bucket. Process and transform the data by using S3 Object Lambda before returning the data to the requesting application.

- C. Process the data and store the transformed data in three separate Amazon S3 buckets so that each application has its own custom dataset. Point each application to its respective S3 bucket.

- D. Process the data and store the transformed data in three separate Amazon DynamoDB tables so that each application has its own custom dataset. Point each application to its respective DynamoDB table.

**Correct:** B
**Why:** S3 Object Lambda can redact or transform object content (e.g., remove PII) per requester while preserving original data.

**Incorrect:**
- A: Proxy/app‑level or separate datasets add ops and duplication.
- C: Proxy/app‑level or separate datasets add ops and duplication.
- D: Proxy/app‑level or separate datasets add ops and duplication.


---

---

### Question #299

A research laboratory needs to process approximately 8 TB of data. The laboratory requires sub-millisecond latencies and a minimum throughput of 6 GBps for the storage subsystem. Hundreds of Amazon EC2 instances that run Amazon Linux will distribute and process the data. Which solution will meet the performance requirements?

- A. Create an Amazon FSx for NetApp ONTAP file system. Sat each volume’ tiering policy to ALL. Import the raw data into the file system. Mount the la system on the EC2 instances.

- B. Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system that uses persistent SSD storage. Select the option to import data from and export data to Amazon S3. Mount the file system on the EC2 instances.

- C. Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system that uses persistent HDD storage. Select the option to import data from and export data to Amazon S3. Mount the file system on the EC2 instances.

- D. Create an Amazon FSx for NetApp ONTAP file system. Set each volume’s tiering policy to NONE. Import the raw data into the file system. Mount the file system on the EC2 instances.

**Correct:** B
**Why:** FSx for Lustre persistent SSD with S3 integration meets sub‑ms latency and high throughput across many EC2 instances.

**Incorrect:**
- A: NetApp ONTAP tiering/none policies don’t match performance requirements.
- C: HDD tier insufficient for performance.
- D: NetApp ONTAP tiering/none policies don’t match performance requirements.


---

---

### Question #300

A company needs to migrate a legacy application from an on-premises data center to the AWS Cloud because of hardware capacity constraints. The application runs 24 hours a day, 7 days a week. The application’s database storage continues to grow over time. What should a solutions architect do to meet these requirements MOST cost-effectively?

- A. Migrate the application layer to Amazon EC2 Spot Instances. Migrate the data storage layer to Amazon S3.

- B. Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer to Amazon RDS On-Demand Instances.

- C. Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer to Amazon Aurora Reserved Instances.

- D. Migrate the application layer to Amazon EC2 On-Demand Instances. Migrate the data storage layer to Amazon RDS Reserved Instances.

**Correct:** C
**Why:** 24/7 app → EC2 RIs; Aurora RIs for growing database with storage autoscaling; cost‑effective for continuous use.

**Incorrect:**
- A: Spot unsuitable for 24/7.
- B: On‑Demand for DB or app increases cost.
- D: On‑Demand for DB or app increases cost.


---

---

### Question #301

A university research laboratory needs to migrate 30 TB of data from an on-premises Windows file server to Amazon FSx for Windows File Server. The laboratory has a 1 Gbps network link that many other departments in the university share. The laboratory wants to implement a data migration service that will maximize the performance of the data transfer. However, the laboratory needs to be able to control the amount of bandwidth that the service uses to minimize the impact on other departments. The data migration must take place within the next 5 days. Which AWS solution will meet these requirements?

- A. AWS Snowcone

- B. Amazon FSx File Gateway

- C. AWS DataSync

- D. AWS Transfer Family

**Correct:** C
**Why:** AWS DataSync supports bandwidth throttling and high‑performance transfer to FSx Windows within the 5‑day window.

**Incorrect:**
- A: Snowcone/FSx File Gateway/Transfer Family are not optimal for bulk controlled‑bandwidth migration.
- B: Snowcone/FSx File Gateway/Transfer Family are not optimal for bulk controlled‑bandwidth migration.
- D: Snowcone/FSx File Gateway/Transfer Family are not optimal for bulk controlled‑bandwidth migration.


---

---

### Question #302

A company wants to create a mobile app that allows users to stream slow-motion video clips on their mobile devices. Currently, the app captures video clips and uploads the video clips in raw format into an Amazon S3 bucket. The app retrieves these video clips directly from the S3 bucket. However, the videos are large in their raw format. Users are experiencing issues with buffering and playback on mobile devices. The company wants to implement solutions to maximize the performance and scalability of the app while minimizing operational overhead. Which combination of solutions will meet these requirements? (Choose two.)

- A. Deploy Amazon CloudFront for content delivery and caching.

- B. Use AWS DataSync to replicate the video files across AW'S Regions in other S3 buckets.

- C. Use Amazon Elastic Transcoder to convert the video files to more appropriate formats.

- D. Deploy an Auto Sealing group of Amazon EC2 instances in Local Zones for content delivery and caching.

E. Deploy an Auto Scaling group of Amazon EC2 instances to convert the video files to more appropriate formats.

**Correct:** A, C
**Why:** CloudFront accelerates delivery and caching; Elastic Transcoder (or AWS Elemental MediaConvert) converts to mobile‑friendly formats.

**Incorrect:**
- B: Cross‑Region replication or EC2 fleets add ops without solving core playback/format issues.
- D: Cross‑Region replication or EC2 fleets add ops without solving core playback/format issues.
- E: Cross‑Region replication or EC2 fleets add ops without solving core playback/format issues.


---

---

### Question #305

A company is designing a shared storage solution for a gaming application that is hosted in the AWS Cloud. The company needs the ability to use SMB clients to access data. The solution must be fully managed. Which AWS solution meets these requirements?

- A. Create an AWS DataSync task that shares the data as a mountable file system. Mount the file system to the application server.

- B. Create an Amazon EC2 Windows instance. Install and configure a Windows file share role on the instance. Connect the application server to the file share.

- C. Create an Amazon FSx for Windows File Server file system. Attach the file system to the origin server. Connect the application server to the file system.

- D. Create an Amazon S3 bucket. Assign an IAM role to the application to grant access to the S3 bucket. Mount the S3 bucket to the application server.

**Correct:** C
**Why:** FSx for Windows File Server provides managed SMB file shares.

**Incorrect:**
- A: DataSync/Tape/Direct S3 are not SMB file servers.
- B: DataSync/Tape/Direct S3 are not SMB file servers.
- D: DataSync/Tape/Direct S3 are not SMB file servers.


---

---

### Question #307

A company that primarily runs its application servers on premises has decided to migrate to AWS. The company wants to minimize its need to scale its Internet Small Computer Systems Interface (iSCSI) storage on premises. The company wants only its recently accessed data to remain stored locally. Which AWS solution should the company use to meet these requirements?

- A. Amazon S3 File Gateway

- B. AWS Storage Gateway Tape Gateway

- C. AWS Storage Gateway Volume Gateway stored volumes

- D. AWS Storage Gateway Volume Gateway cached volumes

**Correct:** D
**Why:** Volume Gateway cached volumes keep only recently accessed data locally; bulk stored in AWS, reducing on‑prem scaling.

**Incorrect:**
- A: S3/ Tape/Stored volumes don’t fit “keep only recent local” requirement.
- B: S3/ Tape/Stored volumes don’t fit “keep only recent local” requirement.
- C: S3/ Tape/Stored volumes don’t fit “keep only recent local” requirement.


---

---

### Question #309

A solutions architect needs to optimize storage costs. The solutions architect must identify any Amazon S3 buckets that are no longer being accessed or are rarely accessed. Which solution will accomplish this goal with the LEAST operational overhead?

- A. Analyze bucket access patterns by using the S3 Storage Lens dashboard for advanced activity metrics.

- B. Analyze bucket access patterns by using the S3 dashboard in the AWS Management Console.

- C. Turn on the Amazon CloudWatch BucketSizeBytes metric for buckets. Analyze bucket access patterns by using the metrics data with Amazon Athena.

- D. Turn on AWS CloudTrail for S3 object monitoring. Analyze bucket access patterns by using CloudTrail logs that are integrated with Amazon CloudWatch Logs.

**Correct:** A
**Why:** S3 Storage Lens advanced metrics identify buckets with low/rare access with least ops.

**Incorrect:**
- B: Console metrics, CloudWatch, or CloudTrail require more work and are less precise.
- C: Console metrics, CloudWatch, or CloudTrail require more work and are less precise.
- D: Console metrics, CloudWatch, or CloudTrail require more work and are less precise.


---

---

### Question #310

A company sells datasets to customers who do research in articial intelligence and machine learning (AI/ML). The datasets are large, formatted files that are stored in an Amazon S3 bucket in the us-east-1 Region. The company hosts a web application that the customers use to purchase access to a given dataset. The web application is deployed on multiple Amazon EC2 instances behind an Application Load Balancer. After a purchase is made, customers receive an S3 signed URL that allows access to the files. The customers are distributed across North America and Europe. The company wants to reduce the cost that is associated with data transfers and wants to maintain or improve performance. What should a solutions architect do to meet these requirements?

- A. Configure S3 Transfer Acceleration on the existing S3 bucket. Direct customer requests to the S3 Transfer Acceleration endpoint. Continue to use S3 signed URLs for access control.

- B. Deploy an Amazon CloudFront distribution with the existing S3 bucket as the origin. Direct customer requests to the CloudFront URL. Switch to CloudFront signed URLs for access control.

- C. Set up a second S3 bucket in the eu-central-1 Region with S3 Cross-Region Replication between the buckets. Direct customer requests to the closest Region. Continue to use S3 signed URLs for access control.

- D. Modify the web application to enable streaming of the datasets to end users. Configure the web application to read the data from the existing S3 bucket. Implement access control directly in the application.

**Correct:** B
**Why:** CloudFront in front of S3 reduces egress cost (regional edge caches/edge), improves performance; use signed URLs for access control.

**Incorrect:**
- A: Transfer Acceleration/CRR/streaming app changes are less optimal or higher ops.
- C: Transfer Acceleration/CRR/streaming app changes are less optimal or higher ops.
- D: Transfer Acceleration/CRR/streaming app changes are less optimal or higher ops.


---

---

### Question #313

A company is building a mobile app on AWS. The company wants to expand its reach to millions of users. The company needs to build a platform so that authorized users can watch the company’s content on their mobile devices. What should a solutions architect recommend to meet these requirements?

- A. Publish content to a public Amazon S3 bucket. Use AWS Key Management Service (AWS KMS) keys to stream content.

- B. Set up IPsec VPN between the mobile app and the AWS environment to stream content.

- C. Use Amazon CloudFront. Provide signed URLs to stream content.

- D. Set up AWS Client VPN between the mobile app and the AWS environment to stream content.

**Correct:** C
**Why:** CloudFront with signed URLs securely streams to authorized users at scale on mobile devices.

**Incorrect:**
- A: S3 public/KMS, VPN/Client VPN are not suitable for mass content distribution.
- B: S3 public/KMS, VPN/Client VPN are not suitable for mass content distribution.
- D: S3 public/KMS, VPN/Client VPN are not suitable for mass content distribution.


---

---

### Question #317

A company uses a legacy application to produce data in CSV format. The legacy application stores the output data in Amazon S3. The company is deploying a new commercial off-the-shelf (COTS) application that can perform complex SQL queries to analyze data that is stored in Amazon Redshift and Amazon S3 only. However, the COTS application cannot process the .csv files that the legacy application produces. The company cannot update the legacy application to produce data in another format. The company needs to implement a solution so that the COTS application can use the data that the legacy application produces. Which solution will meet these requirements with the LEAST operational overhead?

- A. Create an AWS Glue extract, transform, and load (ETL) job that runs on a schedule. Configure the ETL job to process the .csv files and store the processed data in Amazon Redshift.

- B. Develop a Python script that runs on Amazon EC2 instances to convert the .csv files to .sql files. Invoke the Python script on a cron schedule to store the output files in Amazon S3.

- C. Create an AWS Lambda function and an Amazon DynamoDB table. Use an S3 event to invoke the Lambda function. Configure the Lambda function to perform an extract, transform, and load (ETL) job to process the .csv files and store the processed data in the DynamoDB table.

- D. Use Amazon EventBridge to launch an Amazon EMR cluster on a weekly schedule. Configure the EMR cluster to perform an extract, transform, and load (ETL) job to process the .csv files and store the processed data in an Amazon Redshift table.

**Correct:** A
**Why:** Glue ETL job on a schedule to process CSVs and load into Redshift with least ops.

**Incorrect:**
- B: EC2 scripts, DynamoDB, or EMR add ops for the use case.
- C: EC2 scripts, DynamoDB, or EMR add ops for the use case.
- D: EC2 scripts, DynamoDB, or EMR add ops for the use case.


---

---

### Question #320

A company is using a eet of Amazon EC2 instances to ingest data from on-premises data sources. The data is in JSON format and ingestion rates can be as high as 1 MB/s. When an EC2 instance is rebooted, the data in-ight is lost. The company’s data science team wants to query ingested data in near-real time. Which solution provides near-real-time data querying that is scalable with minimal data loss?

- A. Publish data to Amazon Kinesis Data Streams, Use Kinesis Data Analytics to query the data.

- B. Publish data to Amazon Kinesis Data Firehose with Amazon Redshift as the destination. Use Amazon Redshift to query the data.

- C. Store ingested data in an EC2 instance store. Publish data to Amazon Kinesis Data Firehose with Amazon S3 as the destination. Use Amazon Athena to query the data.

- D. Store ingested data in an Amazon Elastic Block Store (Amazon EBS) volume. Publish data to Amazon ElastiCache for Redis. Subscribe to the Redis channel to query the data.

**Correct:** A
**Why:** Kinesis Data Streams with Kinesis Data Analytics provides near‑real‑time querying with minimal loss and scalability.

**Incorrect:**
- B: Firehose→Redshift has more latency; instance store/EBS risk data loss; Redis pub/sub isn’t durable.
- C: Firehose→Redshift has more latency; instance store/EBS risk data loss; Redis pub/sub isn’t durable.
- D: Firehose→Redshift has more latency; instance store/EBS risk data loss; Redis pub/sub isn’t durable.


---

---

### Question #321

What should a solutions architect do to ensure that all objects uploaded to an Amazon S3 bucket are encrypted?

- A. Update the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set.

- B. Update the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set to private.

- C. Update the bucket policy to deny if the PutObject does not have an aws:SecureTransport header set to true.

- D. Update the bucket policy to deny if the PutObject does not have an x-amz-server-side-encryption header set.

**Correct:** D
**Why:** Deny PutObject that lacks x‑amz‑server‑side‑encryption enforces at‑rest encryption for all uploads.

**Incorrect:**
- A: ACL or SecureTransport alone don’t enforce SSE.
- B: ACL or SecureTransport alone don’t enforce SSE.
- C: ACL or SecureTransport alone don’t enforce SSE.


---

---

### Question #323

A company’s facility has badge readers at every entrance throughout the building. When badges are scanned, the readers send a message over HTTPS to indicate who attempted to access that particular entrance. A solutions architect must design a system to process these messages from the sensors. The solution must be highly available, and the results must be made available for the company’s security team to analyze. Which system architecture should the solutions architect recommend?

- A. Launch an Amazon EC2 instance to serve as the HTTPS endpoint and to process the messages. Configure the EC2 instance to save the results to an Amazon S3 bucket.

- B. Create an HTTPS endpoint in Amazon API Gateway. Configure the API Gateway endpoint to invoke an AWS Lambda function to process the messages and save the results to an Amazon DynamoDB table.

- C. Use Amazon Route 53 to direct incoming sensor messages to an AWS Lambda function. Configure the Lambda function to process the messages and save the results to an Amazon DynamoDB table.

- D. Create a gateway VPC endpoint for Amazon S3. Configure a Site-to-Site VPN connection from the facility network to the VPC so that sensor data can be written directly to an S3 bucket by way of the VPC endpoint.

**Correct:** B
**Why:** API Gateway HTTPS endpoint invoking Lambda, storing to DynamoDB, is highly available and serverless.

**Incorrect:**
- A: EC2 endpoint or direct S3 via VPN increases ops/complexity.
- C: Route 53 cannot directly invoke code.
- D: EC2 endpoint or direct S3 via VPN increases ops/complexity.


---

---

### Question #324

A company wants to implement a disaster recovery plan for its primary on-premises file storage volume. The file storage volume is mounted from an Internet Small Computer Systems Interface (iSCSI) device on a local storage server. The file storage volume holds hundreds of terabytes (TB) of data. The company wants to ensure that end users retain immediate access to all file types from the on-premises systems without experiencing latency. Which solution will meet these requirements with the LEAST amount of change to the company's existing infrastructure?

- A. Provision an Amazon S3 File Gateway as a virtual machine (VM) that is hosted on premises. Set the local cache to 10 TB. Modify existing applications to access the files through the NFS protocol. To recover from a disaster, provision an Amazon EC2 instance and mount the S3 bucket that contains the files.

- B. Provision an AWS Storage Gateway tape gateway. Use a data backup solution to back up all existing data to a virtual tape library. Configure the data backup solution to run nightly after the initial backup is complete. To recover from a disaster, provision an Amazon EC2 instance and restore the data to an Amazon Elastic Block Store (Amazon EBS) volume from the volumes in the virtual tape library.

- C. Provision an AWS Storage Gateway Volume Gateway cached volume. Set the local cache to 10 TB. Mount the Volume Gateway cached volume to the existing file server by using iSCSI, and copy all files to the storage volume. Configure scheduled snapshots of the storage volume. To recover from a disaster, restore a snapshot to an Amazon Elastic Block Store (Amazon EBS) volume and attach the EBS volume to an Amazon EC2 instance.

- D. Provision an AWS Storage Gateway Volume Gateway stored volume with the same amount of disk space as the existing file storage volume. Mount the Volume Gateway stored volume to the existing file server by using iSCSI, and copy all files to the storage volume. Configure scheduled snapshots of the storage volume. To recover from a disaster, restore a snapshot to an Amazon Elastic Block Store (Amazon EBS) volume and attach the EBS volume to an Amazon EC2 instance.

**Correct:** D
**Why:** Stored volumes keep the full dataset on‑prem for immediate access while asynchronously replicating to AWS for DR.

**Incorrect:**
- A: S3 File Gateway changes protocols and keeps only cached data.
- B: Tape Gateway is for backups only.
- C: Cached volumes keep only recent data local, not the full dataset.


---

---

### Question #325

A company is hosting a web application from an Amazon S3 bucket. The application uses Amazon Cognito as an identity provider to authenticate users and return a JSON Web Token (JWT) that provides access to protected resources that are stored in another S3 bucket. Upon deployment of the application, users report errors and are unable to access the protected content. A solutions architect must resolve this issue by providing proper permissions so that users can access the protected content. Which solution meets these requirements?

- A. Update the Amazon Cognito identity pool to assume the proper IAM role for access to the protected content.

- B. Update the S3 ACL to allow the application to access the protected content.

- C. Redeploy the application to Amazon S3 to prevent eventually consistent reads in the S3 bucket from affecting the ability of users to access the protected content.

- D. Update the Amazon Cognito pool to use custom attribute mappings within the identity pool and grant users the proper permissions to access the protected content.

**Correct:** A
**Why:** Configure Cognito identity pool roles to grant temporary S3 access for protected content.

**Incorrect:**
- B: S3 ACLs/custom attribute mapping do not address IAM role assumption.
- C: Redeploying to S3 doesn’t change permissions.
- D: S3 ACLs/custom attribute mapping do not address IAM role assumption.


---

---

### Question #326

An image hosting company uploads its large assets to Amazon S3 Standard buckets. The company uses multipart upload in parallel by using S3 APIs and overwrites if the same object is uploaded again. For the first 30 days after upload, the objects will be accessed frequently. The objects will be used less frequently after 30 days, but the access patterns for each object will be inconsistent. The company must optimize its S3 storage costs while maintaining high availability and resiliency of stored assets. Which combination of actions should a solutions architect recommend to meet these requirements? (Choose two.)

- A. Move assets to S3 Intelligent-Tiering after 30 days.

- B. Configure an S3 Lifecycle policy to clean up incomplete multipart uploads.

- C. Configure an S3 Lifecycle policy to clean up expired object delete markers.

- D. Move assets to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.

E. Move assets to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days.

**Correct:** A, B
**Why:** Move to Intelligent‑Tiering after 30 days for variable access and clean up incomplete multipart uploads to reduce cost.

**Incorrect:**
- C: Delete markers are separate; Standard‑IA/One Zone‑IA may be suboptimal or less resilient.
- D: Delete markers are separate; Standard‑IA/One Zone‑IA may be suboptimal or less resilient.
- E: Delete markers are separate; Standard‑IA/One Zone‑IA may be suboptimal or less resilient.


---

---

### Question #328

A company is hosting a three-tier ecommerce application in the AWS Cloud. The company hosts the website on Amazon S3 and integrates the website with an API that handles sales requests. The company hosts the API on three Amazon EC2 instances behind an Application Load Balancer (ALB). The API consists of static and dynamic front-end content along with backend workers that process sales requests asynchronously. The company is expecting a signicant and sudden increase in the number of sales requests during events for the launch of new products. What should a solutions architect recommend to ensure that all the requests are processed successfully?

- A. Add an Amazon CloudFront distribution for the dynamic content. Increase the number of EC2 instances to handle the increase in trac.

- B. Add an Amazon CloudFront distribution for the static content. Place the EC2 instances in an Auto Scaling group to launch new instances based on network trac.

- C. Add an Amazon CloudFront distribution for the dynamic content. Add an Amazon ElastiCache instance in front of the ALB to reduce trac for the API to handle.

- D. Add an Amazon CloudFront distribution for the static content. Add an Amazon Simple Queue Service (Amazon SQS) queue to receive requests from the website for later processing by the EC2 instances.

**Correct:** D
**Why:** Cache static content with CloudFront and use SQS to buffer backend requests to ensure all are processed.

**Incorrect:**
- A: Dynamic CloudFront or ElastiCache alone won’t handle sudden async workload spikes.
- B: Without queueing, backend can still be overwhelmed.
- C: Dynamic CloudFront or ElastiCache alone won’t handle sudden async workload spikes.


---

---

### Question #331

A company must migrate 20 TB of data from a data center to the AWS Cloud within 30 days. The company’s network bandwidth is limited to 15 Mbps and cannot exceed 70% utilization. What should a solutions architect do to meet these requirements?

- A. Use AWS Snowball.

- B. Use AWS DataSync.

- C. Use a secure VPN connection.

- D. Use Amazon S3 Transfer Acceleration.

**Correct:** A
**Why:** Snowball devices can move 20 TB within the time/bandwidth constraints cost‑effectively.

**Incorrect:**
- B: DataSync/VPN/Transfer Acceleration won’t meet time/bandwidth constraints.
- C: DataSync/VPN/Transfer Acceleration won’t meet time/bandwidth constraints.
- D: DataSync/VPN/Transfer Acceleration won’t meet time/bandwidth constraints.


---

---

### Question #332

A company needs to provide its employees with secure access to condential and sensitive files. The company wants to ensure that the files can be accessed only by authorized users. The files must be downloaded securely to the employees’ devices. The files are stored in an on-premises Windows file server. However, due to an increase in remote usage, the file server is running out of capacity. . Which solution will meet these requirements?

- A. Migrate the file server to an Amazon EC2 instance in a public subnet. Configure the security group to limit inbound trac to the employees’ IP addresses.

- B. Migrate the files to an Amazon FSx for Windows File Server file system. Integrate the Amazon FSx file system with the on-premises Active Directory. Configure AWS Client VPN.

- C. Migrate the files to Amazon S3, and create a private VPC endpoint. Create a signed URL to allow download.

- D. Migrate the files to Amazon S3, and create a public VPC endpoint. Allow employees to sign on with AWS IAM Identity Center (AWS Single Sign-On).

**Correct:** B
**Why:** FSx for Windows with on‑prem AD integration preserves permissions; Client VPN provides secure remote access and downloads.

**Incorrect:**
- A: Public EC2, S3 signed URLs, or public endpoints don’t integrate with AD or meet security requirements.
- C: Public EC2, S3 signed URLs, or public endpoints don’t integrate with AD or meet security requirements.
- D: Public EC2, S3 signed URLs, or public endpoints don’t integrate with AD or meet security requirements.


---

---

### Question #334

A company wants to give a customer the ability to use on-premises Microsoft Active Directory to download files that are stored in Amazon S3. The customer’s application uses an SFTP client to download the files. Which solution will meet these requirements with the LEAST operational overhead and no changes to the customer’s application?

- A. Set up AWS Transfer Family with SFTP for Amazon S3. Configure integrated Active Directory authentication.

- B. Set up AWS Database Migration Service (AWS DMS) to synchronize the on-premises client with Amazon S3. Configure integrated Active Directory authentication.

- C. Set up AWS DataSync to synchronize between the on-premises location and the S3 location by using AWS IAM Identity Center (AWS Single Sign-On).

- D. Set up a Windows Amazon EC2 instance with SFTP to connect the on-premises client with Amazon S3. Integrate AWS Identity and Access Management (IAM).

**Correct:** A
**Why:** AWS Transfer Family SFTP for S3 with AD auth meets two‑way SFTP needs without app changes.

**Incorrect:**
- B: DMS/DataSync/IAM‑backed EC2 SFTP add ops or don’t integrate AD SFTP properly.
- C: DMS/DataSync/IAM‑backed EC2 SFTP add ops or don’t integrate AD SFTP properly.
- D: DMS/DataSync/IAM‑backed EC2 SFTP add ops or don’t integrate AD SFTP properly.


---

---

### Question #336

A company hosts a multi-tier web application that uses an Amazon Aurora MySQL DB cluster for storage. The application tier is hosted on Amazon EC2 instances. The company’s IT security guidelines mandate that the database credentials be encrypted and rotated every 14 days. What should a solutions architect do to meet this requirement with the LEAST operational effort?

- A. Create a new AWS Key Management Service (AWS KMS) encryption key. Use AWS Secrets Manager to create a new secret that uses the KMS key with the appropriate credentials. Associate the secret with the Aurora DB cluster. Configure a custom rotation period of 14 days.

- B. Create two parameters in AWS Systems Manager Parameter Store: one for the user name as a string parameter and one that uses the SecureString type for the password. Select AWS Key Management Service (AWS KMS) encryption for the password parameter, and load these parameters in the application tier. Implement an AWS Lambda function that rotates the password every 14 days.

- C. Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system in all EC2 instances of the application tier. Restrict the access to the file on the file system so that the application can read the file and that only super users can modify the file. Implement an AWS Lambda function that rotates the key in Aurora every 14 days and writes new credentials into the file.

- D. Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon S3 bucket that the application uses to load the credentials. Download the file to the application regularly to ensure that the correct credentials are used. Implement an AWS Lambda function that rotates the Aurora credentials every 14 days and uploads these credentials to the file in the S3 bucket.

**Correct:** A
**Why:** Secrets Manager with KMS‑encrypted secret and built‑in rotation integrates with Aurora; set 14‑day rotation.

**Incorrect:**
- B: Parameter Store/EFS/S3 solutions require more glue code and ops.
- C: Parameter Store/EFS/S3 solutions require more glue code and ops.
- D: Parameter Store/EFS/S3 solutions require more glue code and ops.


---

---

### Question #341

A company has an Amazon S3 data lake that is governed by AWS Lake Formation. The company wants to create a visualization in Amazon QuickSight by joining the data in the data lake with operational data that is stored in an Amazon Aurora MySQL database. The company wants to enforce column-level authorization so that the company’s marketing team can access only a subset of columns in the database. Which solution will meet these requirements with the LEAST operational overhead?

- A. Use Amazon EMR to ingest the data directly from the database to the QuickSight SPICE engine. Include only the required columns.

- B. Use AWS Glue Studio to ingest the data from the database to the S3 data lake. Attach an IAM policy to the QuickSight users to enforce column-level access control. Use Amazon S3 as the data source in QuickSight.

- C. Use AWS Glue Elastic Views to create a materialized view for the database in Amazon S3. Create an S3 bucket policy to enforce column- level access control for the QuickSight users. Use Amazon S3 as the data source in QuickSight.

- D. Use a Lake Formation blueprint to ingest the data from the database to the S3 data lake. Use Lake Formation to enforce column-level access control for the QuickSight users. Use Amazon Athena as the data source in QuickSight.

**Correct:** D
**Why:** Lake Formation ingestion and column‑level permissions with Athena as the query engine integrate cleanly with QuickSight.

**Incorrect:**
- A: EMR/Glue Studio or Elastic Views/S3 policies don’t provide column‑level governance end‑to‑end as simply.
- B: EMR/Glue Studio or Elastic Views/S3 policies don’t provide column‑level governance end‑to‑end as simply.
- C: EMR/Glue Studio or Elastic Views/S3 policies don’t provide column‑level governance end‑to‑end as simply.


---

---

### Question #343

A solutions architect is designing a company’s disaster recovery (DR) architecture. The company has a MySQL database that runs on an Amazon EC2 instance in a private subnet with scheduled backup. The DR design needs to include multiple AWS Regions. Which solution will meet these requirements with the LEAST operational overhead?

- A. Migrate the MySQL database to multiple EC2 instances. Configure a standby EC2 instance in the DR Region. Turn on replication.

- B. Migrate the MySQL database to Amazon RDS. Use a Multi-AZ deployment. Turn on read replication for the primary DB instance in the different Availability Zones.

- C. Migrate the MySQL database to an Amazon Aurora global database. Host the primary DB cluster in the primary Region. Host the secondary DB cluster in the DR Region.

- D. Store the scheduled backup of the MySQL database in an Amazon S3 bucket that is congured for S3 Cross-Region Replication (CRR). Use the data backup to restore the database in the DR Region.

**Correct:** C
**Why:** Aurora Global Database provides multi‑Region design with low ops and built‑in replication; best for DR across Regions.

**Incorrect:**
- A: EC2 self‑managed, single‑Region RDS Multi‑AZ, or CRR of backups offer higher RTO/RPO or more ops.
- B: EC2 self‑managed, single‑Region RDS Multi‑AZ, or CRR of backups offer higher RTO/RPO or more ops.
- D: EC2 self‑managed, single‑Region RDS Multi‑AZ, or CRR of backups offer higher RTO/RPO or more ops.


---

---

### Question #344

A company has a Java application that uses Amazon Simple Queue Service (Amazon SQS) to parse messages. The application cannot parse messages that are larger than 256 KB in size. The company wants to implement a solution to give the application the ability to parse messages as large as 50 MB. Which solution will meet these requirements with the FEWEST changes to the code?

- A. Use the Amazon SQS Extended Client Library for Java to host messages that are larger than 256 KB in Amazon S3.

- B. Use Amazon EventBridge to post large messages from the application instead of Amazon SQS.

- C. Change the limit in Amazon SQS to handle messages that are larger than 256 KB.

- D. Store messages that are larger than 256 KB in Amazon Elastic File System (Amazon EFS). Configure Amazon SQS to reference this location in the messages.

**Correct:** A
**Why:** SQS Extended Client Library stores message payloads in S3 to overcome the 256 KB limit up to 2–50 MB.

**Incorrect:**
- B: EventBridge isn’t a queue; SQS cannot raise native limit; EFS indirection isn’t integrated.
- C: EventBridge isn’t a queue; SQS cannot raise native limit; EFS indirection isn’t integrated.
- D: EventBridge isn’t a queue; SQS cannot raise native limit; EFS indirection isn’t integrated.


---

---

### Question #345

A company wants to restrict access to the content of one of its main web applications and to protect the content by using authorization techniques available on AWS. The company wants to implement a serverless architecture and an authentication solution for fewer than 100 users. The solution needs to integrate with the main web application and serve web content globally. The solution must also scale as the company's user base grows while providing the lowest login latency possible. Which solution will meet these requirements MOST cost-effectively?

- A. Use Amazon Cognito for authentication. Use Lambda@Edge for authorization. Use Amazon CloudFront to serve the web application globally.

- B. Use AWS Directory Service for Microsoft Active Directory for authentication. Use AWS Lambda for authorization. Use an Application Load Balancer to serve the web application globally.

- C. Use Amazon Cognito for authentication. Use AWS Lambda for authorization. Use Amazon S3 Transfer Acceleration to serve the web application globally.

- D. Use AWS Directory Service for Microsoft Active Directory for authentication. Use Lambda@Edge for authorization. Use AWS Elastic Beanstalk to serve the web application globally.

**Correct:** A
**Why:** Cognito for auth, Lambda@Edge for authorization, and CloudFront for global delivery provide a serverless, low‑latency solution.

**Incorrect:**
- B: Directory Service and Beanstalk/ALB add ops;
- C: S3 Transfer Acceleration isn’t for auth.
- D: Directory Service and Beanstalk/ALB add ops;


---

---

### Question #346

A company has an aging network-attached storage (NAS) array in its data center. The NAS array presents SMB shares and NFS shares to client workstations. The company does not want to purchase a new NAS array. The company also does not want to incur the cost of renewing the NAS array’s support contract. Some of the data is accessed frequently, but much of the data is inactive. A solutions architect needs to implement a solution that migrates the data to Amazon S3, uses S3 Lifecycle policies, and maintains the same look and feel for the client workstations. The solutions architect has identied AWS Storage Gateway as part of the solution. Which type of storage gateway should the solutions architect provision to meet these requirements?

- A. Volume Gateway

- B. Tape Gateway

- C. Amazon FSx File Gateway

- D. Amazon S3 File Gateway

**Correct:** D
**Why:** S3 File Gateway presents SMB/NFS backed by S3, enables lifecycle policies, and preserves client experience.

**Incorrect:**
- A: Volume/Tape/FSx File Gateway (nonexistent) aren’t appropriate.
- B: Volume/Tape/FSx File Gateway (nonexistent) aren’t appropriate.
- C: Volume/Tape/FSx File Gateway (nonexistent) aren’t appropriate.


---

---

### Question #349

A company stores condential data in an Amazon Aurora PostgreSQL database in the ap-southeast-3 Region. The database is encrypted with an AWS Key Management Service (AWS KMS) customer managed key. The company was recently acquired and must securely share a backup of the database with the acquiring company’s AWS account in ap-southeast-3. What should a solutions architect do to meet these requirements?

- A. Create a database snapshot. Copy the snapshot to a new unencrypted snapshot. Share the new snapshot with the acquiring company’s AWS account.

- B. Create a database snapshot. Add the acquiring company’s AWS account to the KMS key policy. Share the snapshot with the acquiring company’s AWS account.

- C. Create a database snapshot that uses a different AWS managed KMS key. Add the acquiring company’s AWS account to the KMS key alias. Share the snapshot with the acquiring company's AWS account.

- D. Create a database snapshot. Download the database snapshot. Upload the database snapshot to an Amazon S3 bucket. Update the S3 bucket policy to allow access from the acquiring company’s AWS account.

**Correct:** B
**Why:** Share encrypted snapshot by adding the other account to the CMK key policy and sharing the snapshot.

**Incorrect:**
- A: Unencrypted copies, different KMS aliases, or S3 export are unnecessary/less secure.
- C: Unencrypted copies, different KMS aliases, or S3 export are unnecessary/less secure.
- D: Unencrypted copies, different KMS aliases, or S3 export are unnecessary/less secure.


---

---

### Question #353

A company hosts a three-tier web application on Amazon EC2 instances in a single Availability Zone. The web application uses a self-managed MySQL database that is hosted on an EC2 instance to store data in an Amazon Elastic Block Store (Amazon EBS) volume. The MySQL database currently uses a 1 TB Provisioned IOPS SSD (io2) EBS volume. The company expects trac of 1,000 IOPS for both reads and writes at peak trac. The company wants to minimize any disruptions, stabilize performance, and reduce costs while retaining the capacity for double the IOPS. The company wants to move the database tier to a fully managed solution that is highly available and fault tolerant. Which solution will meet these requirements MOST cost-effectively?

- A. Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with an io2 Block Express EBS volume.

- B. Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with a General Purpose SSD (gp2) EBS volume.

- C. Use Amazon S3 Intelligent-Tiering access tiers.

- D. Use two large EC2 instances to host the database in active-passive mode.

**Correct:** B
**Why:** Multi-AZ Amazon RDS for MySQL with General Purpose SSD (gp2) provides stable, managed, highly available performance and is cost‑effective for ~1–2K IOPS needs.

**Incorrect:**
- A: io2 Block Express is not a typical/necessary choice for RDS here and is more expensive.
- C: S3 Intelligent‑Tiering is unrelated to database storage.
- D: Self-managed EC2 databases increase ops and reduce availability compared to RDS.


---

---

### Question #356

A company stores its data objects in Amazon S3 Standard storage. A solutions architect has found that 75% of the data is rarely accessed after 30 days. The company needs all the data to remain immediately accessible with the same high availability and resiliency, but the company wants to minimize storage costs. Which storage solution will meet these requirements?

- A. Move the data objects to S3 Glacier Deep Archive after 30 days.

- B. Move the data objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.

- C. Move the data objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days.

- D. Move the data objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) immediately.

**Correct:** B
**Why:** S3 Standard‑IA keeps immediate access and the same durability/availability at lower cost for data rarely accessed after 30 days.

**Incorrect:**
- A: Glacier Deep Archive is not immediate access.
- C: One Zone classes reduce availability/resiliency versus requirement.
- D: One Zone classes reduce availability/resiliency versus requirement.


---

---

### Question #357

A gaming company is moving its public scoreboard from a data center to the AWS Cloud. The company uses Amazon EC2 Windows Server instances behind an Application Load Balancer to host its dynamic application. The company needs a highly available storage solution for the application. The application consists of static files and dynamic server-side code. Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)

- A. Store the static files on Amazon S3. Use Amazon CloudFront to cache objects at the edge.

- B. Store the static files on Amazon S3. Use Amazon ElastiCache to cache objects at the edge.

- C. Store the server-side code on Amazon Elastic File System (Amazon EFS). Mount the EFS volume on each EC2 instance to share the files.

- D. Store the server-side code on Amazon FSx for Windows File Server. Mount the FSx for Windows File Server volume on each EC2 instance to share the files.

E. Store the server-side code on a General Purpose SSD (gp2) Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume on each EC2 instance to share the files.

**Correct:** A, D
**Why:** Store static files in S3 and cache with CloudFront; use FSx for Windows File Server to share server‑side code across Windows EC2 instances.

**Incorrect:**
- B: ElastiCache is not an edge CDN for static files.
- C: EFS is POSIX/NFS (Linux), not ideal for Windows.
- E: EBS volumes are single‑instance; not shareable across instances.


---

---

### Question #358

A social media company runs its application on Amazon EC2 instances behind an Application Load Balancer (ALB). The ALB is the origin for an Amazon CloudFront distribution. The application has more than a billion images stored in an Amazon S3 bucket and processes thousands of images each second. The company wants to resize the images dynamically and serve appropriate formats to clients. Which solution will meet these requirements with the LEAST operational overhead?

- A. Install an external image management library on an EC2 instance. Use the image management library to process the images.

- B. Create a CloudFront origin request policy. Use the policy to automatically resize images and to serve the appropriate format based on the User-Agent HTTP header in the request.

- C. Use a Lambda@Edge function with an external image management library. Associate the Lambda@Edge function with the CloudFront behaviors that serve the images.

- D. Create a CloudFront response headers policy. Use the policy to automatically resize images and to serve the appropriate format based on the User-Agent HTTP header in the request.

**Correct:** C
**Why:** Lambda@Edge can dynamically transform/resize images at the edge with minimal ops and latency.

**Incorrect:**
- A: Managing EC2 image service increases ops and scale burden.
- B: CloudFront policies/headers cannot perform image processing by themselves.
- D: CloudFront policies/headers cannot perform image processing by themselves.


---

---

### Question #359

A hospital needs to store patient records in an Amazon S3 bucket. The hospital’s compliance team must ensure that all protected health information (PHI) is encrypted in transit and at rest. The compliance team must administer the encryption key for data at rest. Which solution will meet these requirements?

- A. Create a public SSL/TLS certicate in AWS Certicate Manager (ACM). Associate the certicate with Amazon S3. Configure default encryption for each S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS). Assign the compliance team to manage the KMS keys.

- B. Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Configure default encryption for each S3 bucket to use server-side encryption with S3 managed encryption keys (SSE-S3). Assign the compliance team to manage the SSE-S3 keys.

- C. Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Configure default encryption for each S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS). Assign the compliance team to manage the KMS keys.

- D. Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Use Amazon Macie to protect the sensitive data that is stored in Amazon S3. Assign the compliance team to manage Macie.

**Correct:** C
**Why:** Enforce TLS in transit with aws:SecureTransport on the S3 bucket policy; use SSE‑KMS so the compliance team administers the CMK for data at rest.

**Incorrect:**
- A: ACM public certs aren’t attached directly to S3; also KMS administration is needed, not SSE‑S3.
- B: SSE‑S3 keys are managed by AWS, not the compliance team.
- D: Macie discovers PII; it does not fulfill encryption requirements.


---

---

### Question #361

A company hosts a multiplayer gaming application on AWS. The company wants the application to read data with sub-millisecond latency and run one-time queries on historical data. Which solution will meet these requirements with the LEAST operational overhead?

- A. Use Amazon RDS for data that is frequently accessed. Run a periodic custom script to export the data to an Amazon S3 bucket.

- B. Store the data directly in an Amazon S3 bucket. Implement an S3 Lifecycle policy to move older data to S3 Glacier Deep Archive for long- term storage. Run one-time queries on the data in Amazon S3 by using Amazon Athena.

- C. Use Amazon DynamoDB with DynamoDB Accelerator (DAX) for data that is frequently accessed. Export the data to an Amazon S3 bucket by using DynamoDB table export. Run one-time queries on the data in Amazon S3 by using Amazon Athena.

- D. Use Amazon DynamoDB for data that is frequently accessed. Turn on streaming to Amazon Kinesis Data Streams. Use Amazon Kinesis Data Firehose to read the data from Kinesis Data Streams. Store the records in an Amazon S3 bucket.

**Correct:** C
**Why:** DynamoDB + DAX provides sub‑millisecond reads; export to S3 and query ad hoc with Athena for historical analysis with low ops.

**Incorrect:**
- A: Adds more moving parts and does not give sub‑ms reads as simply as DAX.
- B: S3 alone lacks sub‑ms latency for frequently accessed data.
- D: Adds more moving parts and does not give sub‑ms reads as simply as DAX.


---

---

### Question #372

A company wants to migrate an Oracle database to AWS. The database consists of a single table that contains millions of geographic information systems (GIS) images that are high resolution and are identied by a geographic code. When a natural disaster occurs, tens of thousands of images get updated every few minutes. Each geographic code has a single image or row that is associated with it. The company wants a solution that is highly available and scalable during such events. Which solution meets these requirements MOST cost-effectively?

- A. Store the images and geographic codes in a database table. Use Oracle running on an Amazon RDS Multi-AZ DB instance.

- B. Store the images in Amazon S3 buckets. Use Amazon DynamoDB with the geographic code as the key and the image S3 URL as the value.

- C. Store the images and geographic codes in an Amazon DynamoDB table. Configure DynamoDB Accelerator (DAX) during times of high load.

- D. Store the images in Amazon S3 buckets. Store geographic codes and image S3 URLs in a database table. Use Oracle running on an Amazon RDS Multi-AZ DB instance.

**Correct:** B
**Why:** Store images in S3 and use DynamoDB keyed by geographic code with S3 URLs; scales and is highly available during spikes.

**Incorrect:**
- A: RDS Oracle will struggle to scale writes/size cost‑effectively.
- C: Storing large images directly in DynamoDB is not appropriate.
- D: RDS Oracle will struggle to scale writes/size cost‑effectively.


---

---

### Question #373

A company has an application that collects data from IoT sensors on automobiles. The data is streamed and stored in Amazon S3 through Amazon Kinesis Data Firehose. The data produces trillions of S3 objects each year. Each morning, the company uses the data from the previous 30 days to retrain a suite of machine learning (ML) models. Four times each year, the company uses the data from the previous 12 months to perform analysis and train other ML models. The data must be available with minimal delay for up to 1 year. After 1 year, the data must be retained for archival purposes. Which storage solution meets these requirements MOST cost-effectively?

- A. Use the S3 Intelligent-Tiering storage class. Create an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 1 year.

- B. Use the S3 Intelligent-Tiering storage class. Configure S3 Intelligent-Tiering to automatically move objects to S3 Glacier Deep Archive after 1 year.

- C. Use the S3 Standard-Infrequent Access (S3 Standard-IA) storage class. Create an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 1 year.

- D. Use the S3 Standard storage class. Create an S3 Lifecycle policy to transition objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days, and then to S3 Glacier Deep Archive after 1 year.

**Correct:** A
**Why:** S3 Intelligent‑Tiering optimizes cost for unknown/variable access up to 1 year; lifecycle to Glacier Deep Archive after 1 year minimizes cost for archival.

**Incorrect:**
- B: Intelligent‑Tiering does not auto‑move to Deep Archive; use lifecycle.
- C: Less optimal than Intelligent‑Tiering for highly variable access with trillions of objects.
- D: Less optimal than Intelligent‑Tiering for highly variable access with trillions of objects.


---

---

### Question #379

A company hosts a frontend application that uses an Amazon API Gateway API backend that is integrated with AWS Lambda. When the API receives requests, the Lambda function loads many libraries. Then the Lambda function connects to an Amazon RDS database, processes the data, and returns the data to the frontend application. The company wants to ensure that response latency is as low as possible for all its users with the fewest number of changes to the company's operations. Which solution will meet these requirements?

- A. Establish a connection between the frontend application and the database to make queries faster by bypassing the API.

- B. Configure provisioned concurrency for the Lambda function that handles the requests.

- C. Cache the results of the queries in Amazon S3 for faster retrieval of similar datasets.

- D. Increase the size of the database to increase the number of connections Lambda can establish at one time.

**Correct:** B
**Why:** Provisioned concurrency keeps Lambda execution environments warm, reducing cold starts (library load time) and latency with minimal changes.

**Incorrect:**
- A: Direct DB access from frontend bypasses API and is insecure.
- C: Caching S3 results doesn’t address dynamic queries and connections.
- D: Upsizing DB does not remove Lambda cold starts.


---

---

### Question #381

A company hosts a three-tier web application that includes a PostgreSQL database. The database stores the metadata from documents. The company searches the metadata for key terms to retrieve documents that the company reviews in a report each month. The documents are stored in Amazon S3. The documents are usually written only once, but they are updated frequently. The reporting process takes a few hours with the use of relational queries. The reporting process must not prevent any document modications or the addition of new documents. A solutions architect needs to implement a solution to speed up the reporting process. Which solution will meet these requirements with the LEAST amount of change to the application code?

- A. Set up a new Amazon DocumentDB (with MongoDB compatibility) cluster that includes a read replica. Scale the read replica to generate the reports.

- B. Set up a new Amazon Aurora PostgreSQL DB cluster that includes an Aurora Replica. Issue queries to the Aurora Replica to generate the reports.

- C. Set up a new Amazon RDS for PostgreSQL Multi-AZ DB instance. Configure the reporting module to query the secondary RDS node so that the reporting module does not affect the primary node.

- D. Set up a new Amazon DynamoDB table to store the documents. Use a xed write capacity to support new document entries. Automatically scale the read capacity to support the reports.

**Correct:** B
**Why:** Aurora PostgreSQL with an Aurora Replica lets reports run against the replica without blocking writes; minimal code change (same engine).

**Incorrect:**
- A: DocumentDB or RDS Multi‑AZ do not solve read scaling as effectively.
- C: DocumentDB or RDS Multi‑AZ do not solve read scaling as effectively.
- D: DynamoDB would require a redesign.


---

---

### Question #384

A company runs an application on Amazon EC2 Linux instances across multiple Availability Zones. The application needs a storage layer that is highly available and Portable Operating System Interface (POSIX)-compliant. The storage layer must provide maximum data durability and must be shareable across the EC2 instances. The data in the storage layer will be accessed frequently for the first 30 days and will be accessed infrequently after that time. Which solution will meet these requirements MOST cost-effectively?

- A. Use the Amazon S3 Standard storage class. Create an S3 Lifecycle policy to move infrequently accessed data to S3 Glacier.

- B. Use the Amazon S3 Standard storage class. Create an S3 Lifecycle policy to move infrequently accessed data to S3 Standard-Infrequent Access (S3 Standard-IA).

- C. Use the Amazon Elastic File System (Amazon EFS) Standard storage class. Create a lifecycle management policy to move infrequently accessed data to EFS Standard-Infrequent Access (EFS Standard-IA).

- D. Use the Amazon Elastic File System (Amazon EFS) One Zone storage class. Create a lifecycle management policy to move infrequently accessed data to EFS One Zone-Infrequent Access (EFS One Zone-IA).

**Correct:** C
**Why:** EFS provides shared, POSIX‑compliant, highly available storage; lifecycle moves infrequently accessed data to EFS Standard‑IA to save cost.

**Incorrect:**
- A: S3 is not POSIX‑compliant and not a shared filesystem for EC2.
- B: S3 is not POSIX‑compliant and not a shared filesystem for EC2.
- D: One Zone is not as highly available as required.


---

---

### Question #393

A payment processing company records all voice communication with its customers and stores the audio files in an Amazon S3 bucket. The company needs to capture the text from the audio files. The company must remove from the text any personally identiable information (PII) that belongs to customers. What should a solutions architect do to meet these requirements?

- A. Process the audio files by using Amazon Kinesis Video Streams. Use an AWS Lambda function to scan for known PII patterns.

- B. When an audio file is uploaded to the S3 bucket, invoke an AWS Lambda function to start an Amazon Textract task to analyze the call recordings.

- C. Configure an Amazon Transcribe transcription job with PII redaction turned on. When an audio file is uploaded to the S3 bucket, invoke an AWS Lambda function to start the transcription job. Store the output in a separate S3 bucket.

- D. Create an Amazon Connect contact flow that ingests the audio files with transcription turned on. Embed an AWS Lambda function to scan for known PII patterns. Use Amazon EventBridge to start the contact flow when an audio file is uploaded to the S3 bucket.

**Correct:** C
**Why:** Amazon Transcribe supports PII redaction; invoke jobs on upload and store sanitized text output separately.

**Incorrect:**
- A: Not appropriate services for transcription and PII redaction here.
- B: Textract is for documents, not audio.
- D: Not appropriate services for transcription and PII redaction here.


---

---

### Question #397

An ecommerce company needs to run a scheduled daily job to aggregate and filter sales records for analytics. The company stores the sales records in an Amazon S3 bucket. Each object can be up to 10 GB in size. Based on the number of sales events, the job can take up to an hour to complete. The CPU and memory usage of the job are constant and are known in advance. A solutions architect needs to minimize the amount of operational effort that is needed for the job to run. Which solution meets these requirements?

- A. Create an AWS Lambda function that has an Amazon EventBridge notification. Schedule the EventBridge event to run once a day.

- B. Create an AWS Lambda function. Create an Amazon API Gateway HTTP API, and integrate the API with the function. Create an Amazon EventBridge scheduled event that calls the API and invokes the function.

- C. Create an Amazon Elastic Container Service (Amazon ECS) cluster with an AWS Fargate launch type. Create an Amazon EventBridge scheduled event that launches an ECS task on the cluster to run the job.

- D. Create an Amazon Elastic Container Service (Amazon ECS) cluster with an Amazon EC2 launch type and an Auto Scaling group with at least one EC2 instance. Create an Amazon EventBridge scheduled event that launches an ECS task on the cluster to run the job.

**Correct:** C
**Why:** ECS on Fargate with an EventBridge schedule runs the containerized job with known CPU/memory, minimizing ops (no servers to manage).

**Incorrect:**
- A: Lambda is not suitable for up to 10 GB objects and 1‑hour jobs.
- B: Lambda is not suitable for up to 10 GB objects and 1‑hour jobs.
- D: EC2 launch type requires managing instances/ASG.


---

---

### Question #398

A company needs to transfer 600 TB of data from its on-premises network-attached storage (NAS) system to the AWS Cloud. The data transfer must be complete within 2 weeks. The data is sensitive and must be encrypted in transit. The company’s internet connection can support an upload speed of 100 Mbps. Which solution meets these requirements MOST cost-effectively?

- A. Use Amazon S3 multi-part upload functionality to transfer the files over HTTPS.

- B. Create a VPN connection between the on-premises NAS system and the nearest AWS Region. Transfer the data over the VPN connection.

- C. Use the AWS Snow Family console to order several AWS Snowball Edge Storage Optimized devices. Use the devices to transfer the data to Amazon S3.

- D. Set up a 10 Gbps AWS Direct Connect connection between the company location and the nearest AWS Region. Transfer the data over a VPN connection into the Region to store the data in Amazon S3.

**Correct:** C
**Why:** Snowball Edge devices securely transfer large datasets (600 TB) within the required 2 weeks with encryption in transit and at rest.

**Incorrect:**
- A: 100 Mbps is far too slow for 600 TB in 2 weeks.
- B: 100 Mbps is far too slow for 600 TB in 2 weeks.
- D: Provisioning DX for a one‑off transfer is costly/time‑consuming.


---

---

### Question #402

A company needs to ingest and handle large amounts of streaming data that its application generates. The application runs on Amazon EC2 instances and sends data to Amazon Kinesis Data Streams, which is congured with default settings. Every other day, the application consumes the data and writes the data to an Amazon S3 bucket for business intelligence (BI) processing. The company observes that Amazon S3 is not receiving all the data that the application sends to Kinesis Data Streams. What should a solutions architect do to resolve this issue?

- A. Update the Kinesis Data Streams default settings by modifying the data retention period.

- B. Update the application to use the Kinesis Producer Library (KPL) to send the data to Kinesis Data Streams.

- C. Update the number of Kinesis shards to handle the throughput of the data that is sent to Kinesis Data Streams.

- D. Turn on S3 Versioning within the S3 bucket to preserve every version of every object that is ingested in the S3 bucket.

**Correct:** A
**Why:** Default Kinesis Data Streams retention is 24 hours; consuming every other day requires a longer retention period.

**Incorrect:**
- B: KPL helps with throughput, not expired records.
- C: More shards won’t recover expired data.
- D: S3 Versioning does not affect Kinesis delivery.


---

---

### Question #403

A developer has an application that uses an AWS Lambda function to upload files to Amazon S3 and needs the required permissions to perform the task. The developer already has an IAM user with valid IAM credentials required for Amazon S3. What should a solutions architect do to grant the permissions?

- A. Add required IAM permissions in the resource policy of the Lambda function.

- B. Create a signed request using the existing IAM credentials in the Lambda function.

- C. Create a new IAM user and use the existing IAM credentials in the Lambda function.

- D. Create an IAM execution role with the required permissions and attach the IAM role to the Lambda function.

**Correct:** D
**Why:** Grant permissions via an IAM execution role attached to the Lambda function (least privilege, no static creds).

**Incorrect:**
- A: Lambda resource policy controls who can invoke, not its data access.
- B: Do not embed or create user credentials in code.
- C: Do not embed or create user credentials in code.


---

---

### Question #404

A company has deployed a serverless application that invokes an AWS Lambda function when new documents are uploaded to an Amazon S3 bucket. The application uses the Lambda function to process the documents. After a recent marketing campaign, the company noticed that the application did not process many of the documents. What should a solutions architect do to improve the architecture of this application?

- A. Set the Lambda function's runtime timeout value to 15 minutes.

- B. Configure an S3 bucket replication policy. Stage the documents in the S3 bucket for later processing.

- C. Deploy an additional Lambda function. Load balance the processing of the documents across the two Lambda functions.

- D. Create an Amazon Simple Queue Service (Amazon SQS) queue. Send the requests to the queue. Configure the queue as an event source for Lambda.

**Correct:** D
**Why:** Decouple S3 events with SQS; Lambda polls the queue, improving durability and handling bursts.

**Incorrect:**
- A: Increasing timeout does not fix event loss or throttling.
- B: Replication doesn’t address processing backpressure.
- C: Load balancing Lambdas is not applicable.


---

---

### Question #407

A company is implementing a shared storage solution for a gaming application that is hosted in the AWS Cloud. The company needs the ability to use Lustre clients to access data. The solution must be fully managed. Which solution meets these requirements?

- A. Create an AWS DataSync task that shares the data as a mountable file system. Mount the file system to the application server.

- B. Create an AWS Storage Gateway file gateway. Create a file share that uses the required client protocol. Connect the application server to the file share.

- C. Create an Amazon Elastic File System (Amazon EFS) file system, and configure it to support Lustre. Attach the file system to the origin server. Connect the application server to the file system.

- D. Create an Amazon FSx for Lustre file system. Attach the file system to the origin server. Connect the application server to the file system.

**Correct:** D
**Why:** Amazon FSx for Lustre is the fully managed Lustre file system compatible with Lustre clients.

**Incorrect:**
- A: DataSync, File Gateway, and EFS are not Lustre file systems.
- B: DataSync, File Gateway, and EFS are not Lustre file systems.
- C: DataSync, File Gateway, and EFS are not Lustre file systems.


---

---

### Question #412

An image-hosting company stores its objects in Amazon S3 buckets. The company wants to avoid accidental exposure of the objects in the S3 buckets to the public. All S3 objects in the entire AWS account need to remain private. Which solution will meet these requirements?

- A. Use Amazon GuardDuty to monitor S3 bucket policies. Create an automatic remediation action rule that uses an AWS Lambda function to remediate any change that makes the objects public.

- B. Use AWS Trusted Advisor to nd publicly accessible S3 buckets. Configure email notications in Trusted Advisor when a change is detected. Manually change the S3 bucket policy if it allows public access.

- C. Use AWS Resource Access Manager to nd publicly accessible S3 buckets. Use Amazon Simple Notification Service (Amazon SNS) to invoke an AWS Lambda function when a change is detected. Deploy a Lambda function that programmatically remediates the change.

- D. Use the S3 Block Public Access feature on the account level. Use AWS Organizations to create a service control policy (SCP) that prevents IAM users from changing the setting. Apply the SCP to the account.

**Correct:** D
**Why:** Enable S3 Block Public Access at the account level and lock it with an SCP to prevent changes.

**Incorrect:**
- A: Detection and manual remediation have higher risk/overhead.
- B: Detection and manual remediation have higher risk/overhead.
- C: Detection and manual remediation have higher risk/overhead.


---

---

### Question #414

A company has a business system that generates hundreds of reports each day. The business system saves the reports to a network share in CSV format. The company needs to store this data in the AWS Cloud in near-real time for analysis. Which solution will meet these requirements with the LEAST administrative overhead?

- A. Use AWS DataSync to transfer the files to Amazon S3. Create a scheduled task that runs at the end of each day.

- B. Create an Amazon S3 File Gateway. Update the business system to use a new network share from the S3 File Gateway.

- C. Use AWS DataSync to transfer the files to Amazon S3. Create an application that uses the DataSync API in the automation workow.

- D. Deploy an AWS Transfer for SFTP endpoint. Create a script that checks for new files on the network share and uploads the new files by using SFTP.

**Correct:** B
**Why:** S3 File Gateway exposes an SMB/NFS share that writes files to S3 in near real time with minimal admin.

**Incorrect:**
- A: DataSync is great for transfers/syncs but not a live network share.
- C: DataSync is great for transfers/syncs but not a live network share.
- D: Transfer for SFTP adds scripting/ops and is not a drop‑in network share.


---

---

### Question #415

A company is storing petabytes of data in Amazon S3 Standard. The data is stored in multiple S3 buckets and is accessed with varying frequency. The company does not know access patterns for all the data. The company needs to implement a solution for each S3 bucket to optimize the cost of S3 usage. Which solution will meet these requirements with the MOST operational eciency?

- A. Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 Intelligent-Tiering.

- B. Use the S3 storage class analysis tool to determine the correct tier for each object in the S3 bucket. Move each object to the identied storage tier.

- C. Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 Glacier Instant Retrieval.

- D. Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 One Zone-Infrequent Access (S3 One Zone- IA).

**Correct:** A
**Why:** Transition objects to S3 Intelligent‑Tiering per bucket for unknown access patterns with minimal admin.

**Incorrect:**
- B: Manually analyzing/moving per object is heavy.
- C: Single target classes may not optimize for variable access.
- D: Single target classes may not optimize for variable access.


---

---

### Question #416

A rapidly growing global ecommerce company is hosting its web application on AWS. The web application includes static content and dynamic content. The website stores online transaction processing (OLTP) data in an Amazon RDS database The website’s users are experiencing slow page loads. Which combination of actions should a solutions architect take to resolve this issue? (Choose two.)

- A. Configure an Amazon Redshift cluster.

- B. Set up an Amazon CloudFront distribution.

- C. Host the dynamic web content in Amazon S3.

- D. Create a read replica for the RDS DB instance.

E. Configure a Multi-AZ deployment for the RDS DB instance.

**Correct:** B, D
**Why:** CloudFront caches static/dynamic content at edge; RDS read replica offloads reads, improving page load.

**Incorrect:**
- A: Redshift is for OLAP, not OLTP page rendering.
- C: Dynamic content doesn’t belong in S3.
- E: Multi‑AZ is HA, not performance.


---

---

### Question #418

A solutions architect needs to allow team members to access Amazon S3 buckets in two different AWS accounts: a development account and a production account. The team currently has access to S3 buckets in the development account by using unique IAM users that are assigned to an IAM group that has appropriate permissions in the account. The solutions architect has created an IAM role in the production account. The role has a policy that grants access to an S3 bucket in the production account. Which solution will meet these requirements while complying with the principle of least privilege?

- A. Attach the Administrator Access policy to the development account users.

- B. Add the development account as a principal in the trust policy of the role in the production account.

- C. Turn off the S3 Block Public Access feature on the S3 bucket in the production account.

- D. Create a user in the production account with unique credentials for each team member.

**Correct:** B
**Why:** Add the development account as a trusted principal in the production role’s trust policy so users can assume the role to access S3.

**Incorrect:**
- A: AdminAccess violates least privilege.
- C: Block Public Access is unrelated.
- D: Per‑user accounts in production add overhead and wider access.


---

---

### Question #421

A company runs a highly available SFTP service. The SFTP service uses two Amazon EC2 Linux instances that run with elastic IP addresses to accept trac from trusted IP sources on the internet. The SFTP service is backed by shared storage that is attached to the instances. User accounts are created and managed as Linux users in the SFTP servers. The company wants a serverless option that provides high IOPS performance and highly congurable security. The company also wants to maintain control over user permissions. Which solution will meet these requirements?

- A. Create an encrypted Amazon Elastic Block Store (Amazon EBS) volume. Create an AWS Transfer Family SFTP service with a public endpoint that allows only trusted IP addresses. Attach the EBS volume to the SFTP service endpoint. Grant users access to the SFTP service.

- B. Create an encrypted Amazon Elastic File System (Amazon EFS) volume. Create an AWS Transfer Family SFTP service with elastic IP addresses and a VPC endpoint that has internet-facing access. Attach a security group to the endpoint that allows only trusted IP addresses. Attach the EFS volume to the SFTP service endpoint. Grant users access to the SFTP service.

- C. Create an Amazon S3 bucket with default encryption enabled. Create an AWS Transfer Family SFTP service with a public endpoint that allows only trusted IP addresses. Attach the S3 bucket to the SFTP service endpoint. Grant users access to the SFTP service.

- D. Create an Amazon S3 bucket with default encryption enabled. Create an AWS Transfer Family SFTP service with a VPC endpoint that has internal access in a private subnet. Attach a security group that allows only trusted IP addresses. Attach the S3 bucket to the SFTP service endpoint. Grant users access to the SFTP service.

**Correct:** C
**Why:** Transfer Family with an S3 backend is serverless, scalable, and supports IP allow lists; S3 delivers high throughput/IOPS.

**Incorrect:**
- A: You cannot attach EBS/EFS directly to Transfer Family.
- B: You cannot attach EBS/EFS directly to Transfer Family.
- D: A private VPC endpoint is internal; trusted public IPs could not reach it.


---

---

### Question #422

A company is developing a new machine learning (ML) model solution on AWS. The models are developed as independent microservices that fetch approximately 1 GB of model data from Amazon S3 at startup and load the data into memory. Users access the models through an asynchronous API. Users can send a request or a batch of requests and specify where the results should be sent. The company provides models to hundreds of users. The usage patterns for the models are irregular. Some models could be unused for days or weeks. Other models could receive batches of thousands of requests at a time. Which design should a solutions architect recommend to meet these requirements?

- A. Direct the requests from the API to a Network Load Balancer (NLB). Deploy the models as AWS Lambda functions that are invoked by the NLB.

- B. Direct the requests from the API to an Application Load Balancer (ALB). Deploy the models as Amazon Elastic Container Service (Amazon ECS) services that read from an Amazon Simple Queue Service (Amazon SQS) queue. Use AWS App Mesh to scale the instances of the ECS cluster based on the SQS queue size.

- C. Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue. Deploy the models as AWS Lambda functions that are invoked by SQS events. Use AWS Auto Scaling to increase the number of vCPUs for the Lambda functions based on the SQS queue size.

- D. Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue. Deploy the models as Amazon Elastic Container Service (Amazon ECS) services that read from the queue. Enable AWS Auto Scaling on Amazon ECS for both the cluster and copies of the service based on the queue size.

**Correct:** D
**Why:** Queue requests in SQS and run models on ECS services that scale on queue depth; suitable for 1 GB model loads and bursty use.

**Incorrect:**
- A: NLB→Lambda is not a pattern; Lambda cold starts with 1 GB model pulls are costly.
- B: ALB→ECS without decoupling risks backpressure; App Mesh is unnecessary.
- C: Lambda with 1 GB model loads and vCPU scaling is not appropriate.


---

---

### Question #426

A company needs to store data from its healthcare application. The application’s data frequently changes. A new regulation requires audit access at all levels of the stored data. The company hosts the application on an on-premises infrastructure that is running out of storage capacity. A solutions architect must securely migrate the existing data to AWS while satisfying the new regulation. Which solution will meet these requirements?

- A. Use AWS DataSync to move the existing data to Amazon S3. Use AWS CloudTrail to log data events.

- B. Use AWS Snowcone to move the existing data to Amazon S3. Use AWS CloudTrail to log management events.

- C. Use Amazon S3 Transfer Acceleration to move the existing data to Amazon S3. Use AWS CloudTrail to log data events.

- D. Use AWS Storage Gateway to move the existing data to Amazon S3. Use AWS CloudTrail to log management events.

**Correct:** A
**Why:** DataSync migrates data securely to S3; CloudTrail data events provide audit access to object‑level operations.

**Incorrect:**
- B: Snowcone is for small/edge; also management events alone are insufficient.
- C: Transfer Acceleration is for uploads from clients, not bulk secure migration with auditing.
- D: Storage Gateway is hybrid access, not best for migration plus data‑event auditing.


---

---

### Question #430

A manufacturing company has machine sensors that upload .csv files to an Amazon S3 bucket. These .csv files must be converted into images and must be made available as soon as possible for the automatic generation of graphical reports. The images become irrelevant after 1 month, but the .csv files must be kept to train machine learning (ML) models twice a year. The ML trainings and audits are planned weeks in advance. Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)

- A. Launch an Amazon EC2 Spot Instance that downloads the .csv files every hour, generates the image files, and uploads the images to the S3 bucket.

- B. Design an AWS Lambda function that converts the .csv files into images and stores the images in the S3 bucket. Invoke the Lambda function when a .csv file is uploaded.

- C. Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files from S3 Standard to S3 Glacier 1 day after they are uploaded. Expire the image files after 30 days.

- D. Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) 1 day after they are uploaded. Expire the image files after 30 days.

E. Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 1 day after they are uploaded. Keep the image files in Reduced Redundancy Storage (RRS).

**Correct:** B, C
**Why:** Invoke Lambda on S3 put to convert CSV to images quickly; transition CSVs to Glacier and expire images after 30 days to cut cost.

**Incorrect:**
- A: Managing Spot EC2 adds ops and latency.
- D: One Zone‑IA/RRS are not optimal and RRS is deprecated.
- E: One Zone‑IA/RRS are not optimal and RRS is deprecated.


---

---

### Question #438

A company wants to share accounting data with an external auditor. The data is stored in an Amazon RDS DB instance that resides in a private subnet. The auditor has its own AWS account and requires its own copy of the database. What is the MOST secure way for the company to share the database with the auditor?

- A. Create a read replica of the database. Configure IAM standard database authentication to grant the auditor access.

- B. Export the database contents to text files. Store the files in an Amazon S3 bucket. Create a new IAM user for the auditor. Grant the user access to the S3 bucket.

- C. Copy a snapshot of the database to an Amazon S3 bucket. Create an IAM user. Share the user's keys with the auditor to grant access to the object in the S3 bucket.

- D. Create an encrypted snapshot of the database. Share the snapshot with the auditor. Allow access to the AWS Key Management Service (AWS KMS) encryption key.

**Correct:** D
**Why:** Share an encrypted snapshot with the auditor and permit access to the KMS key; the auditor restores into its own account.

**Incorrect:**
- A: Read replica does not cross accounts easily and IAM DB auth is separate.
- B: Text export or snapshot in S3 is less secure/more work.
- C: Text export or snapshot in S3 is less secure/more work.


---

---

### Question #440

A company used an Amazon RDS for MySQL DB instance during application testing. Before terminating the DB instance at the end of the test cycle, a solutions architect created two backups. The solutions architect created the first backup by using the mysqldump utility to create a database dump. The solutions architect created the second backup by enabling the nal DB snapshot option on RDS termination. The company is now planning for a new test cycle and wants to create a new DB instance from the most recent backup. The company has chosen a MySQL-compatible edition ofAmazon Aurora to host the DB instance. Which solutions will create the new DB instance? (Choose two.)

- A. Import the RDS snapshot directly into Aurora.

- B. Upload the RDS snapshot to Amazon S3. Then import the RDS snapshot into Aurora.

- C. Upload the database dump to Amazon S3. Then import the database dump into Aurora.

- D. Use AWS Database Migration Service (AWS DMS) to import the RDS snapshot into Aurora.

E. Upload the database dump to Amazon S3. Then use AWS Database Migration Service (AWS DMS) to import the database dump into Aurora.

**Correct:** C, E
**Why:** Import from mysqldump in S3 to Aurora or use DMS to load from the dump; you cannot import an RDS snapshot directly into Aurora.

**Incorrect:**
- A: Snapshots are not directly importable to Aurora; DMS ingests data, not snapshots.
- B: Snapshots are not directly importable to Aurora; DMS ingests data, not snapshots.
- D: Snapshots are not directly importable to Aurora; DMS ingests data, not snapshots.


---

---

### Question #441

A company hosts a multi-tier web application on Amazon Linux Amazon EC2 instances behind an Application Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The company observes that the Auto Scaling group launches more On-Demand Instances when the application's end users access high volumes of static web content. The company wants to optimize cost. What should a solutions architect do to redesign the application MOST cost-effectively?

- A. Update the Auto Scaling group to use Reserved Instances instead of On-Demand Instances.

- B. Update the Auto Scaling group to scale by launching Spot Instances instead of On-Demand Instances.

- C. Create an Amazon CloudFront distribution to host the static web contents from an Amazon S3 bucket.

- D. Create an AWS Lambda function behind an Amazon API Gateway API to host the static website contents.

**Correct:** C
**Why:** Serve static content from S3 via CloudFront to reduce load on EC2/ALB and lower cost.

**Incorrect:**
- A: Instance pricing choices don’t remove static load driver.
- B: Instance pricing choices don’t remove static load driver.
- D: Lambda+API Gateway is unnecessary for static hosting.


---

---

### Question #443

A company wants to host a scalable web application on AWS. The application will be accessed by users from different geographic regions of the world. Application users will be able to download and upload unique data up to gigabytes in size. The development team wants a cost-effective solution to minimize upload and download latency and maximize performance. What should a solutions architect do to accomplish this?

- A. Use Amazon S3 with Transfer Acceleration to host the application.

- B. Use Amazon S3 with CacheControl headers to host the application.

- C. Use Amazon EC2 with Auto Scaling and Amazon CloudFront to host the application.

- D. Use Amazon EC2 with Auto Scaling and Amazon ElastiCache to host the application.

**Correct:** A
**Why:** S3 Transfer Acceleration provides globally distributed edge ingress/egress to minimize latency for large uploads/downloads.

**Incorrect:**
- B: CacheControl headers do not improve upload latency.
- C: EC2 adds ops and may not minimize global transfer latency.
- D: EC2 adds ops and may not minimize global transfer latency.


---

---

### Question #445

A company is storing 700 terabytes of data on a large network-attached storage (NAS) system in its corporate data center. The company has a hybrid environment with a 10 Gbps AWS Direct Connect connection. After an audit from a regulator, the company has 90 days to move the data to the cloud. The company needs to move the data eciently and without disruption. The company still needs to be able to access and update the data during the transfer window. Which solution will meet these requirements?

- A. Create an AWS DataSync agent in the corporate data center. Create a data transfer task Start the transfer to an Amazon S3 bucket.

- B. Back up the data to AWS Snowball Edge Storage Optimized devices. Ship the devices to an AWS data center. Mount a target Amazon S3 bucket on the on-premises file system.

- C. Use rsync to copy the data directly from local storage to a designated Amazon S3 bucket over the Direct Connect connection.

- D. Back up the data on tapes. Ship the tapes to an AWS data center. Mount a target Amazon S3 bucket on the on-premises file system.

**Correct:** A
**Why:** DataSync over DX moves large NAS datasets efficiently while allowing ongoing access and updates during the window.

**Incorrect:**
- B: Snow/tapes disrupt access during transfer and add logistics.
- C: rsync over DX for 700 TB is error‑prone and operationally heavy.
- D: Snow/tapes disrupt access during transfer and add logistics.


---

---

### Question #446

A company stores data in PDF format in an Amazon S3 bucket. The company must follow a legal requirement to retain all new and existing data in Amazon S3 for 7 years. Which solution will meet these requirements with the LEAST operational overhead?

- A. Turn on the S3 Versioning feature for the S3 bucket. Configure S3 Lifecycle to delete the data after 7 years. Configure multi-factor authentication (MFA) delete for all S3 objects.

- B. Turn on S3 Object Lock with governance retention mode for the S3 bucket. Set the retention period to expire after 7 years. Recopy all existing objects to bring the existing data into compliance.

- C. Turn on S3 Object Lock with compliance retention mode for the S3 bucket. Set the retention period to expire after 7 years. Recopy all existing objects to bring the existing data into compliance.

- D. Turn on S3 Object Lock with compliance retention mode for the S3 bucket. Set the retention period to expire after 7 years. Use S3 Batch Operations to bring the existing data into compliance.

**Correct:** D
**Why:** Enable S3 Object Lock in compliance mode for 7 years and use S3 Batch Operations to apply retention to existing objects.

**Incorrect:**
- A: Versioning/MFA delete do not enforce legal hold/retention.
- B: Recopying all objects is less efficient than Batch Operations.
- C: Recopying all objects is less efficient than Batch Operations.


---

---

### Question #453

A company wants to implement a backup strategy for Amazon EC2 data and multiple Amazon S3 buckets. Because of regulatory requirements, the company must retain backup files for a specic time period. The company must not alter the files for the duration of the retention period. Which solution will meet these requirements?

- A. Use AWS Backup to create a backup vault that has a vault lock in governance mode. Create the required backup plan.

- B. Use Amazon Data Lifecycle Manager to create the required automated snapshot policy.

- C. Use Amazon S3 File Gateway to create the backup. Configure the appropriate S3 Lifecycle management.

- D. Use AWS Backup to create a backup vault that has a vault lock in compliance mode. Create the required backup plan.

**Correct:** D
**Why:** AWS Backup Vault Lock in compliance mode enforces WORM retention for EC2 and S3 backups to meet regulatory requirements.

**Incorrect:**
- A: Governance mode can be bypassed by privileged users.
- B: DLM and File Gateway do not provide WORM‑style immutability across all backups.
- C: DLM and File Gateway do not provide WORM‑style immutability across all backups.


---

---

### Question #460

A company wants to securely exchange data between its software as a service (SaaS) application Salesforce account and Amazon S3. The company must encrypt the data at rest by using AWS Key Management Service (AWS KMS) customer managed keys (CMKs). The company must also encrypt the data in transit. The company has enabled API access for the Salesforce account.

- A. Create AWS Lambda functions to transfer the data securely from Salesforce to Amazon S3.

- B. Create an AWS Step Functions workow. Dene the task to transfer the data securely from Salesforce to Amazon S3.

- C. Create Amazon AppFlow ows to transfer the data securely from Salesforce to Amazon S3.

- D. Create a custom connector for Salesforce to transfer the data securely from Salesforce to Amazon S3.

**Correct:** C
**Why:** Amazon AppFlow has a native Salesforce connector with TLS in transit and SSE‑KMS at rest to S3 with CMKs.

**Incorrect:**
- A: More custom work; AppFlow is purpose‑built for this.
- B: More custom work; AppFlow is purpose‑built for this.
- D: More custom work; AppFlow is purpose‑built for this.


---

---

### Question #463

An IoT company is releasing a mattress that has sensors to collect data about a user’s sleep. The sensors will send data to an Amazon S3 bucket. The sensors collect approximately 2 MB of data every night for each mattress. The company must process and summarize the data for each mattress. The results need to be available as soon as possible. Data processing will require 1 GB of memory and will nish within 30 seconds. Which solution will meet these requirements MOST cost-effectively?

- A. Use AWS Glue with a Scala job

- B. Use Amazon EMR with an Apache Spark script

- C. Use AWS Lambda with a Python script

- D. Use AWS Glue with a PySpark job

**Correct:** C
**Why:** Lambda with a Python function is cost‑effective for 30‑second, 1 GB jobs triggered on S3 object arrival.

**Incorrect:**
- A: Glue/EMR are heavier/more costly for this small batch.
- B: Glue/EMR are heavier/more costly for this small batch.
- D: Glue/EMR are heavier/more costly for this small batch.


---

---

### Question #469

A company stores raw collected data in an Amazon S3 bucket. The data is used for several types of analytics on behalf of the company's customers. The type of analytics requested determines the access pattern on the S3 objects. The company cannot predict or control the access pattern. The company wants to reduce its S3 costs. Which solution will meet these requirements?

- A. Use S3 replication to transition infrequently accessed objects to S3 Standard-Infrequent Access (S3 Standard-IA)

- B. Use S3 Lifecycle rules to transition objects from S3 Standard to Standard-Infrequent Access (S3 Standard-IA)

- C. Use S3 Lifecycle rules to transition objects from S3 Standard to S3 Intelligent-Tiering

- D. Use S3 Inventory to identify and transition objects that have not been accessed from S3 Standard to S3 Intelligent-Tiering

**Correct:** C
**Why:** Transition to S3 Intelligent‑Tiering to handle unpredictable access patterns automatically and reduce cost.

**Incorrect:**
- A: Replication or manual inventory adds overhead; Standard‑IA may be suboptimal.
- B: Replication or manual inventory adds overhead; Standard‑IA may be suboptimal.
- D: Replication or manual inventory adds overhead; Standard‑IA may be suboptimal.


---

---

### Question #471

A company is creating an application that runs on containers in a VPC. The application stores and accesses data in an Amazon S3 bucket. During the development phase, the application will store and access 1 TB of data in Amazon S3 each day. The company wants to minimize costs and wants to prevent trac from traversing the internet whenever possible. Which solution will meet these requirements?

- A. Enable S3 Intelligent-Tiering for the S3 bucket

- B. Enable S3 Transfer Acceleration for the S3 bucket

- C. Create a gateway VPC endpoint for Amazon S3. Associate this endpoint with all route tables in the VPC

- D. Create an interface endpoint for Amazon S3 in the VPC. Associate this endpoint with all route tables in the VPC

**Correct:** C
**Why:** A gateway VPC endpoint for S3 keeps traffic private and avoids NAT data processing charges.

**Incorrect:**
- A: Tiering affects storage cost, not network path.
- B: Transfer Acceleration sends traffic over the internet.
- D: S3 uses gateway endpoints, not interface endpoints.


---

---

### Question #477

A group requires permissions to list an Amazon S3 bucket and delete objects from that bucket. An administrator has created the following IAM policy to provide access to the bucket and applied that policy to the group. The group is not able to delete objects in the bucket. The company follows least-privilege access rules. Which statement should a solutions architect add to the policy to correct bucket access?

Existing policy
{
	"Version": "2012-10-17",
	"Statement": [
		{
			"Action": [
				"s3:ListBucket",
				"s3:DeleteObject"
			],
			"Resource": [
				"arn:aws:s3:::bucket-name"
			],
			"Effect": "Allow"
		}
	]
}

Which statement should be added?

A.
{
	"Action": [
		"s3:*Object"
	],
	"Resource": [
		"arn:aws:s3:::bucket-name/*"
	],
	"Effect": "Allow"
}

B.
{
	"Action": [
		"s3:*"
	],
	"Resource": [
		"arn:aws:s3:::bucket-name/*"
	],
	"Effect": "Allow"
}

C.
{
	"Action": [
		"s3:DeleteObject"
	],
	"Resource": [
		"arn:aws:s3:::bucket-name/*"
	],
	"Effect": "Allow"
}

D.
{
	"Action": [
		"s3:DeleteObject"
	],
	"Resource": [
		"arn:aws:s3:::bucket-name"
	],
	"Effect": "Allow"
}

**Correct:** C
**Why:** DeleteObject must be granted on object ARNs (arn:aws:s3:::bucket-name/*). The existing policy lists the bucket but does not grant object-level permissions on object resources. Option C adds only s3:DeleteObject on the object ARNs, satisfying least privilege.

**Incorrect:**
- A: s3:*Object is broader than required and violates least privilege.
- B: s3:* on objects is overly permissive.
- D: Uses the bucket ARN instead of object ARNs; DeleteObject requires object-level resource specification.


---

---

### Question #478

A law rm needs to share information with the public. The information includes hundreds of files that must be publicly readable. Modications or deletions of the files by anyone before a designated future date are prohibited. Which solution will meet these requirements in the MOST secure way?

- A. Upload all files to an Amazon S3 bucket that is congured for static website hosting. Grant read-only IAM permissions to any AWS principals that access the S3 bucket until the designated date.

- B. Create a new Amazon S3 bucket with S3 Versioning enabled. Use S3 Object Lock with a retention period in accordance with the designated date. Configure the S3 bucket for static website hosting. Set an S3 bucket policy to allow read-only access to the objects.

- C. Create a new Amazon S3 bucket with S3 Versioning enabled. Configure an event trigger to run an AWS Lambda function in case of object modication or deletion. Configure the Lambda function to replace the objects with the original versions from a private S3 bucket.

- D. Upload all files to an Amazon S3 bucket that is congured for static website hosting. Select the folder that contains the files. Use S3 Object Lock with a retention period in accordance with the designated date. Grant read-only IAM permissions to any AWS principals that access the S3 bucket.

**Correct:** B
**Why:** Enable Versioning and Object Lock (compliance) with a retention period; host as static website; allow public read via bucket policy.

**Incorrect:**
- A: Without Object Lock on the bucket, files could be altered/deleted.
- C: Lambda rollback is reactive and less secure.
- D: Without Object Lock on the bucket, files could be altered/deleted.


---

---

### Question #480

A business application is hosted on Amazon EC2 and uses Amazon S3 for encrypted object storage. The chief information security ocer has directed that no application trac between the two services should traverse the public internet. Which capability should the solutions architect use to meet the compliance requirements?

- A. AWS Key Management Service (AWS KMS)

- B. VPC endpoint

- C. Private subnet

- D. Virtual private gateway

**Correct:** B
**Why:** A VPC gateway endpoint for S3 keeps traffic on the AWS network, not the public internet.

**Incorrect:**
- A: KMS is for encryption, not network path.
- C: Subnet type/VPN don’t guarantee private S3 access.
- D: Subnet type/VPN don’t guarantee private S3 access.


---

---

### Question #482

A company wants to migrate 100 GB of historical data from an on-premises location to an Amazon S3 bucket. The company has a 100 megabits per second (Mbps) internet connection on premises. The company needs to encrypt the data in transit to the S3 bucket. The company will store new data directly in Amazon S3. Which solution will meet these requirements with the LEAST operational overhead?

- A. Use the s3 sync command in the AWS CLI to move the data directly to an S3 bucket

- B. Use AWS DataSync to migrate the data from the on-premises location to an S3 bucket

- C. Use AWS Snowball to move the data to an S3 bucket

- D. Set up an IPsec VPN from the on-premises location to AWS. Use the s3 cp command in the AWS CLI to move the data directly to an S3 bucket

**Correct:** B
**Why:** DataSync securely transfers and encrypts data in transit with minimal setup and monitoring.

**Incorrect:**
- A: CLI/VPN approaches are more manual.
- C: Snowball is overkill for 100 GB and adds logistics.
- D: CLI/VPN approaches are more manual.


---

---

### Question #485

A company is looking for a solution that can store video archives in AWS from old news footage. The company needs to minimize costs and will rarely need to restore these files. When the files are needed, they must be available in a maximum of ve minutes. What is the MOST cost-effective solution?

- A. Store the video archives in Amazon S3 Glacier and use Expedited retrievals.

- B. Store the video archives in Amazon S3 Glacier and use Standard retrievals.

- C. Store the video archives in Amazon S3 Standard-Infrequent Access (S3 Standard-IA).

- D. Store the video archives in Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA).

**Correct:** A
**Why:** S3 Glacier (Flexible Retrieval) with Expedited retrievals returns objects in minutes and is the lowest cost for rarely accessed archives.

**Incorrect:**
- B: Standard retrieval is slower.
- C: S3 IA classes cost more over time for rarely accessed archives.
- D: S3 IA classes cost more over time for rarely accessed archives.


---

---

### Question #486

A company is building a three-tier application on AWS. The presentation tier will serve a static website The logic tier is a containerized application. This application will store data in a relational database. The company wants to simplify deployment and to reduce operational costs. Which solution will meet these requirements?

- A. Use Amazon S3 to host static content. Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for compute power. Use a managed Amazon RDS cluster for the database.

- B. Use Amazon CloudFront to host static content. Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 for compute power. Use a managed Amazon RDS cluster for the database.

- C. Use Amazon S3 to host static content. Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for compute power. Use a managed Amazon RDS cluster for the database.

- D. Use Amazon EC2 Reserved Instances to host static content. Use Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 for compute power. Use a managed Amazon RDS cluster for the database.

**Correct:** A
**Why:** S3 static hosting + ECS Fargate for containers + managed RDS minimizes ops and simplifies deployment.

**Incorrect:**
- B: CloudFront alone doesn’t host; EKS/EC2 increase ops.
- C: CloudFront alone doesn’t host; EKS/EC2 increase ops.
- D: CloudFront alone doesn’t host; EKS/EC2 increase ops.


---

---

### Question #490

A gaming company uses Amazon DynamoDB to store user information such as geographic location, player data, and leaderboards. The company needs to configure continuous backups to an Amazon S3 bucket with a minimal amount of coding. The backups must not affect availability of the application and must not affect the read capacity units (RCUs) that are dened for the table. Which solution meets these requirements?

- A. Use an Amazon EMR cluster. Create an Apache Hive job to back up the data to Amazon S3.

- B. Export the data directly from DynamoDB to Amazon S3 with continuous backups. Turn on point-in-time recovery for the table.

- C. Configure Amazon DynamoDB Streams. Create an AWS Lambda function to consume the stream and export the data to an Amazon S3 bucket.

- D. Create an AWS Lambda function to export the data from the database tables to Amazon S3 on a regular basis. Turn on point-in-time recovery for the table.

**Correct:** B
**Why:** DynamoDB table export to S3 provides continuous, server‑side exports without consuming RCUs and with no downtime; enable PITR.

**Incorrect:**
- A: EMR/Lambda/Streams add code/ops and may affect capacity.
- C: EMR/Lambda/Streams add code/ops and may affect capacity.
- D: EMR/Lambda/Streams add code/ops and may affect capacity.


---

---

### Question #495

A company is conducting an internal audit. The company wants to ensure that the data in an Amazon S3 bucket that is associated with the company’s AWS Lake Formation data lake does not contain sensitive customer or employee data. The company wants to discover personally identiable information (PII) or nancial information, including passport numbers and credit card numbers. Which solution will meet these requirements?

- A. Configure AWS Audit Manager on the account. Select the Payment Card Industry Data Security Standards (PCI DSS) for auditing.

- B. Configure Amazon S3 Inventory on the S3 bucket Configure Amazon Athena to query the inventory.

- C. Configure Amazon Macie to run a data discovery job that uses managed identiers for the required data types.

- D. Use Amazon S3 Select to run a report across the S3 bucket.

**Correct:** C
**Why:** Amazon Macie can scan S3 for PII/financial data with managed identifiers and produce findings.

**Incorrect:**
- A: Audit Manager/S3 Inventory/S3 Select do not detect PII patterns as Macie does.
- B: Audit Manager/S3 Inventory/S3 Select do not detect PII patterns as Macie does.
- D: Audit Manager/S3 Inventory/S3 Select do not detect PII patterns as Macie does.


---

---

### Question #496

A company uses on-premises servers to host its applications. The company is running out of storage capacity. The applications use both block storage and NFS storage. The company needs a high-performing solution that supports local caching without re-architecting its existing applications. Which combination of actions should a solutions architect take to meet these requirements? (Choose two.)

- A. Mount Amazon S3 as a file system to the on-premises servers.

- B. Deploy an AWS Storage Gateway file gateway to replace NFS storage.

- C. Deploy AWS Snowball Edge to provision NFS mounts to on-premises servers.

- D. Deploy an AWS Storage Gateway volume gateway to replace the block storage.

E. Deploy Amazon Elastic File System (Amazon EFS) volumes and mount them to on-premises servers.

**Correct:** B, D
**Why:** File Gateway replaces NFS with local cache; Volume Gateway replaces block storage with cached volumes—no app re‑architecture.

**Incorrect:**
- A: S3 is not mounted natively.
- C: Snowball Edge is for migration, not ongoing storage.
- E: EFS is NFS in AWS, not a cached on‑prem replacement.


---

---

### Question #497

A company has a service that reads and writes large amounts of data from an Amazon S3 bucket in the same AWS Region. The service is deployed on Amazon EC2 instances within the private subnet of a VPC. The service communicates with Amazon S3 over a NAT gateway in the public subnet. However, the company wants a solution that will reduce the data output costs. Which solution will meet these requirements MOST cost-effectively?

- A. Provision a dedicated EC2 NAT instance in the public subnet. Configure the route table for the private subnet to use the elastic network interface of this instance as the destination for all S3 trac.

- B. Provision a dedicated EC2 NAT instance in the private subnet. Configure the route table for the public subnet to use the elastic network interface of this instance as the destination for all S3 trac.

- C. Provision a VPC gateway endpoint. Configure the route table for the private subnet to use the gateway endpoint as the route for all S3 trac.

- D. Provision a second NAT gateway. Configure the route table for the private subnet to use this NAT gateway as the destination for all S3 trac.

**Correct:** C
**Why:** An S3 gateway endpoint keeps S3 traffic on the AWS network and avoids NAT gateway data processing charges.

**Incorrect:**
- A: NAT instances increase ops/cost.
- B: NAT instances increase ops/cost.
- D: A second NAT gateway increases cost.


---

---

### Question #498

A company uses Amazon S3 to store high-resolution pictures in an S3 bucket. To minimize application changes, the company stores the pictures as the latest version of an S3 object. The company needs to retain only the two most recent versions of the pictures. The company wants to reduce costs. The company has identied the S3 bucket as a large expense. Which solution will reduce the S3 costs with the LEAST operational overhead?

- A. Use S3 Lifecycle to delete expired object versions and retain the two most recent versions.

- B. Use an AWS Lambda function to check for older versions and delete all but the two most recent versions.

- C. Use S3 Batch Operations to delete noncurrent object versions and retain only the two most recent versions.

- D. Deactivate versioning on the S3 bucket and retain the two most recent versions.

**Correct:** A
**Why:** S3 Lifecycle can retain a set number of noncurrent versions and expire older versions automatically—low overhead.

**Incorrect:**
- B: Lambda/Batch Operations add ongoing work.
- C: Lambda/Batch Operations add ongoing work.
- D: Disabling versioning loses protection and does not retain two versions.


---

---

### Question #500

A company has multiple Windows file servers on premises. The company wants to migrate and consolidate its files into an Amazon FSx for Windows File Server file system. File permissions must be preserved to ensure that access rights do not change. Which solutions will meet these requirements? (Choose two.)

- A. Deploy AWS DataSync agents on premises. Schedule DataSync tasks to transfer the data to the FSx for Windows File Server file system.

- B. Copy the shares on each file server into Amazon S3 buckets by using the AWS CLI. Schedule AWS DataSync tasks to transfer the data to the FSx for Windows File Server file system.

- C. Remove the drives from each file server. Ship the drives to AWS for import into Amazon S3. Schedule AWS DataSync tasks to transfer the data to the FSx for Windows File Server file system.

- D. Order an AWS Snowcone device. Connect the device to the on-premises network. Launch AWS DataSync agents on the device. Schedule DataSync tasks to transfer the data to the FSx for Windows File Server file system.

E. Order an AWS Snowball Edge Storage Optimized device. Connect the device to the on-premises network. Copy data to the device by using the AWS CLI. Ship the device back to AWS for import into Amazon S3. Schedule AWS DataSync tasks to transfer the data to the FSx for Windows File Server file system.

**Correct:** A, E
**Why:** DataSync agents preserve metadata/ACLs when migrating to FSx for Windows; for large data sets, Snowball Edge to S3 plus DataSync to FSx is efficient.

**Incorrect:**
- B: Copying to S3 with CLI loses Windows ACLs/metadata.
- C: Drives/Snowcone are impractical for large multi‑server migrations.
- D: Drives/Snowcone are impractical for large multi‑server migrations.


---

## Amazon SNS

### Question #255

A company has an ecommerce checkout workow that writes an order to a database and calls a service to process the payment. Users are experiencing timeouts during the checkout process. When users resubmit the checkout form, multiple unique orders are created for the same desired transaction. How should a solutions architect refactor this workow to prevent the creation of multiple orders?

- A. Configure the web application to send an order message to Amazon Kinesis Data Firehose. Set the payment service to retrieve the message from Kinesis Data Firehose and process the order.

- B. Create a rule in AWS CloudTrail to invoke an AWS Lambda function based on the logged application path request. Use Lambda to query the database, call the payment service, and pass in the order information.

- C. Store the order in the database. Send a message that includes the order number to Amazon Simple Notification Service (Amazon SNS). Set the payment service to poll Amazon SNS, retrieve the message, and process the order.

- D. Store the order in the database. Send a message that includes the order number to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the payment service to retrieve the message and process the order. Delete the message from the queue.

**Correct:** D
**Why:** Use an SQS FIFO queue with content-based deduplication to ensure exactly-once processing and preserve ordering.

**Incorrect:**
- A: Kinesis/SNS do not provide FIFO message de‑duplication semantics for this flow.
- B: Kinesis/SNS do not provide FIFO message de‑duplication semantics for this flow.
- C: Kinesis/SNS do not provide FIFO message de‑duplication semantics for this flow.


---

---

### Question #278

A company wants to create an application to store employee data in a hierarchical structured relationship. The company needs a minimum-latency response to high-trac queries for the employee data and must protect any sensitive data. The company also needs to receive monthly email messages if any nancial information is present in the employee data. Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)

- A. Use Amazon Redshift to store the employee data in hierarchies. Unload the data to Amazon S3 every month.

- B. Use Amazon DynamoDB to store the employee data in hierarchies. Export the data to Amazon S3 every month.

- C. Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly events to AWS Lambda.

- D. Use Amazon Athena to analyze the employee data in Amazon S3. Integrate Athena with Amazon QuickSight to publish analysis dashboards and share the dashboards with users.

E. Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly notications through an Amazon Simple Notification Service (Amazon SNS) subscription.

**Correct:** B, E
**Why:** DynamoDB suits hierarchical, low‑latency queries; Macie + EventBridge → SNS can notify monthly when financial data is detected.

**Incorrect:**
- A: Redshift/Athena aren’t optimized for low‑latency hierarchical queries.
- C: Lambda trigger pattern is unnecessary for monthly notifications.
- D: Redshift/Athena aren’t optimized for low‑latency hierarchical queries.


---

---

### Question #297

A company deploys an application on ve Amazon EC2 instances. An Application Load Balancer (ALB) distributes trac to the instances by using a target group. The average CPU usage on each of the instances is below 10% most of the time, with occasional surges to 65%. A solutions architect needs to implement a solution to automate the scalability of the application. The solution must optimize the cost of the architecture and must ensure that the application has enough CPU resources when surges occur. Which solution will meet these requirements?

- A. Create an Amazon CloudWatch alarm that enters the ALARM state when the CPUUtilization metric is less than 20%. Create an AWS Lambda function that the CloudWatch alarm invokes to terminate one of the EC2 instances in the ALB target group.

- B. Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing target group as the target group. Set a target tracking scaling policy that is based on the ASGAverageCPUUtilization metric. Set the minimum instances to 2, the desired capacity to 3, the maximum instances to 6, and the target value to 50%. Add the EC2 instances to the Auto Scaling group.

- C. Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing target group as the target group. Set the minimum instances to 2, the desired capacity to 3, and the maximum instances to 6. Add the EC2 instances to the Auto Scaling group.

- D. Create two Amazon CloudWatch alarms. Configure the first CloudWatch alarm to enter the ALARM state when the average CPUUtilization metric is below 20%. Configure the second CloudWatch alarm to enter the ALARM state when the average CPUUtilization matric is above 50%. Configure the alarms to publish to an Amazon Simple Notification Service (Amazon SNS) topic to send an email message. After receiving the message, log in to decrease or increase the number of EC2 instances that are running.

**Correct:** B
**Why:** Create an Auto Scaling group with target tracking on CPU and attach to existing ALB with appropriate min/desired/max.

**Incorrect:**
- A: Manual alarms/emails are not automation.
- C: Missing scaling policy.
- D: Manual alarms/emails are not automation.


---

---

### Question #311

A company is using AWS to design a web application that will process insurance quotes. Users will request quotes from the application. Quotes must be separated by quote type, must be responded to within 24 hours, and must not get lost. The solution must maximize operational eciency and must minimize maintenance. Which solution meets these requirements?

- A. Create multiple Amazon Kinesis data streams based on the quote type. Configure the web application to send messages to the proper data stream. Configure each backend group of application servers to use the Kinesis Client Library (KCL) to pool messages from its own data stream.

- B. Create an AWS Lambda function and an Amazon Simple Notification Service (Amazon SNS) topic for each quote type. Subscribe the Lambda function to its associated SNS topic. Configure the application to publish requests for quotes to the appropriate SNS topic.

- C. Create a single Amazon Simple Notification Service (Amazon SNS) topic. Subscribe Amazon Simple Queue Service (Amazon SQS) queues to the SNS topic. Configure SNS message filtering to publish messages to the proper SQS queue based on the quote type. Configure each backend application server to use its own SQS queue.

- D. Create multiple Amazon Kinesis Data Firehose delivery streams based on the quote type to deliver data streams to an Amazon OpenSearch Service cluster. Configure the application to send messages to the proper delivery stream. Configure each backend group of application servers to search for the messages from OpenSearch Service and process them accordingly.

**Correct:** C
**Why:** SNS to SQS with message filtering separates quote types; SQS ensures durability (no loss) and 24‑hour processing window.

**Incorrect:**
- A: Kinesis or Firehose/OpenSearch are not needed for durable request queues.
- B: SNS + Lambda per type adds code and lacks durable backlog.
- D: Kinesis or Firehose/OpenSearch are not needed for durable request queues.


---

---

### Question #322

A solutions architect is designing a multi-tier application for a company. The application's users upload images from a mobile device. The application generates a thumbnail of each image and returns a message to the user to conrm that the image was uploaded successfully. The thumbnail generation can take up to 60 seconds, but the company wants to provide a faster response time to its users to notify them that the original image was received. The solutions architect must design the application to asynchronously dispatch requests to the different application tiers. What should the solutions architect do to meet these requirements?

- A. Write a custom AWS Lambda function to generate the thumbnail and alert the user. Use the image upload process as an event source to invoke the Lambda function.

- B. Create an AWS Step Functions workow. Configure Step Functions to handle the orchestration between the application tiers and alert the user when thumbnail generation is complete.

- C. Create an Amazon Simple Queue Service (Amazon SQS) message queue. As images are uploaded, place a message on the SQS queue for thumbnail generation. Alert the user through an application message that the image was received.

- D. Create Amazon Simple Notification Service (Amazon SNS) notification topics and subscriptions. Use one subscription with the application to generate the thumbnail after the image upload is complete. Use a second subscription to message the user's mobile app by way of a push notification after thumbnail generation is complete.

**Correct:** C
**Why:** Queue requests in SQS for async thumbnail generation and immediately acknowledge upload to users.

**Incorrect:**
- A: Lambda direct or Step Functions add latency or over‑orchestrate; SNS lacks durable work queue.
- B: Lambda direct or Step Functions add latency or over‑orchestrate; SNS lacks durable work queue.
- D: Lambda direct or Step Functions add latency or over‑orchestrate; SNS lacks durable work queue.


---

---

### Question #363

A company is building a game system that needs to send unique events to separate leaderboard, matchmaking, and authentication services concurrently. The company needs an AWS event-driven system that guarantees the order of the events. Which solution will meet these requirements?

- A. Amazon EventBridge event bus

- B. Amazon Simple Notification Service (Amazon SNS) FIFO topics

- C. Amazon Simple Notification Service (Amazon SNS) standard topics

- D. Amazon Simple Queue Service (Amazon SQS) FIFO queues

**Correct:** B
**Why:** SNS FIFO topics provide ordered, exactly‑once message fanout to multiple subscribers, meeting concurrency with order guarantees.

**Incorrect:**
- A: EventBridge does not guarantee strict ordering across targets.
- C: SNS standard topics are at‑least‑once and unordered.
- D: A single SQS FIFO queue cannot fan out to multiple services concurrently without extra components.


---

---

### Question #364

A hospital is designing a new application that gathers symptoms from patients. The hospital has decided to use Amazon Simple Queue Service (Amazon SQS) and Amazon Simple Notification Service (Amazon SNS) in the architecture. A solutions architect is reviewing the infrastructure design. Data must be encrypted at rest and in transit. Only authorized personnel of the hospital should be able to access the data. Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)

- A. Turn on server-side encryption on the SQS components. Update the default key policy to restrict key usage to a set of authorized principals.

- B. Turn on server-side encryption on the SNS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply a key policy to restrict key usage to a set of authorized principals.

- C. Turn on encryption on the SNS components. Update the default key policy to restrict key usage to a set of authorized principals. Set a condition in the topic policy to allow only encrypted connections over TLS.

- D. Turn on server-side encryption on the SQS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply a key policy to restrict key usage to a set of authorized principals. Set a condition in the queue policy to allow only encrypted connections over TLS.

E. Turn on server-side encryption on the SQS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply an IAM policy to restrict key usage to a set of authorized principals. Set a condition in the queue policy to allow only encrypted connections over TLS.

**Correct:** B, D
**Why:** Use KMS CMKs for server‑side encryption on SNS and SQS; restrict key usage to authorized principals and enforce TLS in topic/queue policies.

**Incorrect:**
- A: Missing CMK use where required or rely on default key policy/IAM policy inappropriately.
- C: Missing CMK use where required or rely on default key policy/IAM policy inappropriately.
- E: Missing CMK use where required or rely on default key policy/IAM policy inappropriately.


---

---

### Question #386

An ecommerce company is running a multi-tier application on AWS. The front-end and backend tiers both run on Amazon EC2, and the database runs on Amazon RDS for MySQL. The backend tier communicates with the RDS instance. There are frequent calls to return identical datasets from the database that are causing performance slowdowns. Which action should be taken to improve the performance of the backend?

- A. Implement Amazon SNS to store the database calls.

- B. Implement Amazon ElastiCache to cache the large datasets.

- C. Implement an RDS for MySQL read replica to cache database calls.

- D. Implement Amazon Kinesis Data Firehose to stream the calls to the database.

**Correct:** B
**Why:** ElastiCache (Redis/Memcached) caches repeated reads to reduce database load and improve performance.

**Incorrect:**
- A: SNS is a pub/sub service, not a cache.
- C: Read replicas help, but cache is better for repetitive identical datasets and lower latency.
- D: Kinesis Firehose is for data streaming, not query caching.


---

---

### Question #400

A meteorological startup company has a custom web application to sell weather data to its users online. The company uses Amazon DynamoDB to store its data and wants to build a new service that sends an alert to the managers of four internal teams every time a new weather event is recorded. The company does not want this new service to affect the performance of the current application. What should a solutions architect do to meet these requirements with the LEAST amount of operational overhead?

- A. Use DynamoDB transactions to write new event data to the table. Configure the transactions to notify internal teams.

- B. Have the current application publish a message to four Amazon Simple Notification Service (Amazon SNS) topics. Have each team subscribe to one topic.

- C. Enable Amazon DynamoDB Streams on the table. Use triggers to write to a single Amazon Simple Notification Service (Amazon SNS) topic to which the teams can subscribe.

- D. Add a custom attribute to each record to ag new items. Write a cron job that scans the table every minute for items that are new and noties an Amazon Simple Queue Service (Amazon SQS) queue to which the teams can subscribe.

**Correct:** C
**Why:** DynamoDB Streams with a trigger can publish to a single SNS topic for fanout to the four teams without impacting the main app.

**Incorrect:**
- A: Transactions don’t notify; they ensure atomic writes.
- B: Having the app publish to four topics adds coupling.
- D: Cron scans are inefficient and add lag.


---

---

### Question #412

An image-hosting company stores its objects in Amazon S3 buckets. The company wants to avoid accidental exposure of the objects in the S3 buckets to the public. All S3 objects in the entire AWS account need to remain private. Which solution will meet these requirements?

- A. Use Amazon GuardDuty to monitor S3 bucket policies. Create an automatic remediation action rule that uses an AWS Lambda function to remediate any change that makes the objects public.

- B. Use AWS Trusted Advisor to nd publicly accessible S3 buckets. Configure email notications in Trusted Advisor when a change is detected. Manually change the S3 bucket policy if it allows public access.

- C. Use AWS Resource Access Manager to nd publicly accessible S3 buckets. Use Amazon Simple Notification Service (Amazon SNS) to invoke an AWS Lambda function when a change is detected. Deploy a Lambda function that programmatically remediates the change.

- D. Use the S3 Block Public Access feature on the account level. Use AWS Organizations to create a service control policy (SCP) that prevents IAM users from changing the setting. Apply the SCP to the account.

**Correct:** D
**Why:** Enable S3 Block Public Access at the account level and lock it with an SCP to prevent changes.

**Incorrect:**
- A: Detection and manual remediation have higher risk/overhead.
- B: Detection and manual remediation have higher risk/overhead.
- C: Detection and manual remediation have higher risk/overhead.


---

---

### Question #413

An ecommerce company is experiencing an increase in user trac. The company’s store is deployed on Amazon EC2 instances as a two-tier web application consisting of a web tier and a separate database tier. As trac increases, the company notices that the architecture is causing signicant delays in sending timely marketing and order conrmation email to users. The company wants to reduce the time it spends resolving complex email delivery issues and minimize operational overhead. What should a solutions architect do to meet these requirements?

- A. Create a separate application tier using EC2 instances dedicated to email processing.

- B. Configure the web instance to send email through Amazon Simple Email Service (Amazon SES).

- C. Configure the web instance to send email through Amazon Simple Notification Service (Amazon SNS).

- D. Create a separate application tier using EC2 instances dedicated to email processing. Place the instances in an Auto Scaling group.

**Correct:** B
**Why:** Use Amazon SES to handle outbound email at scale with minimal ops.

**Incorrect:**
- A: Additional EC2 tiers increase ops and still require mail handling.
- C: SNS is not an SMTP service.
- D: Additional EC2 tiers increase ops and still require mail handling.


---

---

### Question #462

A company has an application that processes customer orders. The company hosts the application on an Amazon EC2 instance that saves the orders to an Amazon Aurora database. Occasionally when trac is high the workload does not process orders fast enough. What should a solutions architect do to write the orders reliably to the database as quickly as possible?

- A. Increase the instance size of the EC2 instance when trac is high. Write orders to Amazon Simple Notification Service (Amazon SNS). Subscribe the database endpoint to the SNS topic.

- B. Write orders to an Amazon Simple Queue Service (Amazon SQS) queue. Use EC2 instances in an Auto Scaling group behind an Application Load Balancer to read from the SQS queue and process orders into the database.

- C. Write orders to Amazon Simple Notification Service (Amazon SNS). Subscribe the database endpoint to the SNS topic. Use EC2 instances in an Auto Scaling group behind an Application Load Balancer to read from the SNS topic.

- D. Write orders to an Amazon Simple Queue Service (Amazon SQS) queue when the EC2 instance reaches CPU threshold limits. Use scheduled scaling of EC2 instances in an Auto Scaling group behind an Application Load Balancer to read from the SQS queue and process orders into the database.

**Correct:** B
**Why:** Buffer orders to SQS and scale consumers behind an ALB/ASG to write quickly and reliably to Aurora.

**Incorrect:**
- A: SNS lacks durable queue semantics; scaling only on CPU misses spikes; D is reactive.
- C: SNS lacks durable queue semantics; scaling only on CPU misses spikes; D is reactive.
- D: SNS lacks durable queue semantics; scaling only on CPU misses spikes; D is reactive.


---

---

### Question #489

An ecommerce company runs an application in the AWS Cloud that is integrated with an on-premises warehouse solution. The company uses Amazon Simple Notification Service (Amazon SNS) to send order messages to an on-premises HTTPS endpoint so the warehouse application can process the orders. The local data center team has detected that some of the order messages were not received. A solutions architect needs to retain messages that are not delivered and analyze the messages for up to 14 days. Which solution will meet these requirements with the LEAST development effort?

- A. Configure an Amazon SNS dead letter queue that has an Amazon Kinesis Data Stream target with a retention period of 14 days.

- B. Add an Amazon Simple Queue Service (Amazon SQS) queue with a retention period of 14 days between the application and Amazon SNS.

- C. Configure an Amazon SNS dead letter queue that has an Amazon Simple Queue Service (Amazon SQS) target with a retention period of 14 days.

- D. Configure an Amazon SNS dead letter queue that has an Amazon DynamoDB target with a TTL attribute set for a retention period of 14 days.

**Correct:** C
**Why:** Configure an SNS dead‑letter queue with SQS target and 14‑day retention to retain and analyze undelivered messages with minimal dev.

**Incorrect:**
- A: Kinesis/DynamoDB targets add complexity.
- B: Adding SQS in front of SNS changes the design.
- D: Kinesis/DynamoDB targets add complexity.


---

## Amazon SQS

### Question #255

A company has an ecommerce checkout workow that writes an order to a database and calls a service to process the payment. Users are experiencing timeouts during the checkout process. When users resubmit the checkout form, multiple unique orders are created for the same desired transaction. How should a solutions architect refactor this workow to prevent the creation of multiple orders?

- A. Configure the web application to send an order message to Amazon Kinesis Data Firehose. Set the payment service to retrieve the message from Kinesis Data Firehose and process the order.

- B. Create a rule in AWS CloudTrail to invoke an AWS Lambda function based on the logged application path request. Use Lambda to query the database, call the payment service, and pass in the order information.

- C. Store the order in the database. Send a message that includes the order number to Amazon Simple Notification Service (Amazon SNS). Set the payment service to poll Amazon SNS, retrieve the message, and process the order.

- D. Store the order in the database. Send a message that includes the order number to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the payment service to retrieve the message and process the order. Delete the message from the queue.

**Correct:** D
**Why:** Use an SQS FIFO queue with content-based deduplication to ensure exactly-once processing and preserve ordering.

**Incorrect:**
- A: Kinesis/SNS do not provide FIFO message de‑duplication semantics for this flow.
- B: Kinesis/SNS do not provide FIFO message de‑duplication semantics for this flow.
- C: Kinesis/SNS do not provide FIFO message de‑duplication semantics for this flow.


---

---

### Question #283

A research company runs experiments that are powered by a simulation application and a visualization application. The simulation application runs on Linux and outputs intermediate data to an NFS share every 5 minutes. The visualization application is a Windows desktop application that displays the simulation output and requires an SMB file system. The company maintains two synchronized file systems. This strategy is causing data duplication and inecient resource usage. The company needs to migrate the applications to AWS without making code changes to either application. Which solution will meet these requirements?

- A. Migrate both applications to AWS Lambda. Create an Amazon S3 bucket to exchange data between the applications.

- B. Migrate both applications to Amazon Elastic Container Service (Amazon ECS). Configure Amazon FSx File Gateway for storage.

- C. Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization application to Windows EC2 instances. Configure Amazon Simple Queue Service (Amazon SQS) to exchange data between the applications.

- D. Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization application to Windows EC2 instances. Configure Amazon FSx for NetApp ONTAP for storage.

**Correct:** D
**Why:** FSx for NetApp ONTAP supports both NFS and SMB on the same data, removing duplicate storage and maintaining app compatibility.

**Incorrect:**
- A: Lambda/ECS + File Gateway or SQS don’t solve multi‑protocol file sharing.
- B: Lambda/ECS + File Gateway or SQS don’t solve multi‑protocol file sharing.
- C: Lambda/ECS + File Gateway or SQS don’t solve multi‑protocol file sharing.


---

---

### Question #311

A company is using AWS to design a web application that will process insurance quotes. Users will request quotes from the application. Quotes must be separated by quote type, must be responded to within 24 hours, and must not get lost. The solution must maximize operational eciency and must minimize maintenance. Which solution meets these requirements?

- A. Create multiple Amazon Kinesis data streams based on the quote type. Configure the web application to send messages to the proper data stream. Configure each backend group of application servers to use the Kinesis Client Library (KCL) to pool messages from its own data stream.

- B. Create an AWS Lambda function and an Amazon Simple Notification Service (Amazon SNS) topic for each quote type. Subscribe the Lambda function to its associated SNS topic. Configure the application to publish requests for quotes to the appropriate SNS topic.

- C. Create a single Amazon Simple Notification Service (Amazon SNS) topic. Subscribe Amazon Simple Queue Service (Amazon SQS) queues to the SNS topic. Configure SNS message filtering to publish messages to the proper SQS queue based on the quote type. Configure each backend application server to use its own SQS queue.

- D. Create multiple Amazon Kinesis Data Firehose delivery streams based on the quote type to deliver data streams to an Amazon OpenSearch Service cluster. Configure the application to send messages to the proper delivery stream. Configure each backend group of application servers to search for the messages from OpenSearch Service and process them accordingly.

**Correct:** C
**Why:** SNS to SQS with message filtering separates quote types; SQS ensures durability (no loss) and 24‑hour processing window.

**Incorrect:**
- A: Kinesis or Firehose/OpenSearch are not needed for durable request queues.
- B: SNS + Lambda per type adds code and lacks durable backlog.
- D: Kinesis or Firehose/OpenSearch are not needed for durable request queues.


---

---

### Question #316

A company uses an Amazon EC2 instance to run a script to poll for and process messages in an Amazon Simple Queue Service (Amazon SQS) queue. The company wants to reduce operational costs while maintaining its ability to process a growing number of messages that are added to the queue. What should a solutions architect recommend to meet these requirements?

- A. Increase the size of the EC2 instance to process messages faster.

- B. Use Amazon EventBridge to turn off the EC2 instance when the instance is underutilized.

- C. Migrate the script on the EC2 instance to an AWS Lambda function with the appropriate runtime.

- D. Use AWS Systems Manager Run Command to run the script on demand.

**Correct:** C
**Why:** Replace the EC2 poller with a Lambda function subscribed to SQS; scales automatically, lower cost.

**Incorrect:**
- A: Larger instance, turning off, or Run Command do not reduce ops or scale on demand.
- B: Larger instance, turning off, or Run Command do not reduce ops or scale on demand.
- D: Larger instance, turning off, or Run Command do not reduce ops or scale on demand.


---

---

### Question #322

A solutions architect is designing a multi-tier application for a company. The application's users upload images from a mobile device. The application generates a thumbnail of each image and returns a message to the user to conrm that the image was uploaded successfully. The thumbnail generation can take up to 60 seconds, but the company wants to provide a faster response time to its users to notify them that the original image was received. The solutions architect must design the application to asynchronously dispatch requests to the different application tiers. What should the solutions architect do to meet these requirements?

- A. Write a custom AWS Lambda function to generate the thumbnail and alert the user. Use the image upload process as an event source to invoke the Lambda function.

- B. Create an AWS Step Functions workow. Configure Step Functions to handle the orchestration between the application tiers and alert the user when thumbnail generation is complete.

- C. Create an Amazon Simple Queue Service (Amazon SQS) message queue. As images are uploaded, place a message on the SQS queue for thumbnail generation. Alert the user through an application message that the image was received.

- D. Create Amazon Simple Notification Service (Amazon SNS) notification topics and subscriptions. Use one subscription with the application to generate the thumbnail after the image upload is complete. Use a second subscription to message the user's mobile app by way of a push notification after thumbnail generation is complete.

**Correct:** C
**Why:** Queue requests in SQS for async thumbnail generation and immediately acknowledge upload to users.

**Incorrect:**
- A: Lambda direct or Step Functions add latency or over‑orchestrate; SNS lacks durable work queue.
- B: Lambda direct or Step Functions add latency or over‑orchestrate; SNS lacks durable work queue.
- D: Lambda direct or Step Functions add latency or over‑orchestrate; SNS lacks durable work queue.


---

---

### Question #328

A company is hosting a three-tier ecommerce application in the AWS Cloud. The company hosts the website on Amazon S3 and integrates the website with an API that handles sales requests. The company hosts the API on three Amazon EC2 instances behind an Application Load Balancer (ALB). The API consists of static and dynamic front-end content along with backend workers that process sales requests asynchronously. The company is expecting a signicant and sudden increase in the number of sales requests during events for the launch of new products. What should a solutions architect recommend to ensure that all the requests are processed successfully?

- A. Add an Amazon CloudFront distribution for the dynamic content. Increase the number of EC2 instances to handle the increase in trac.

- B. Add an Amazon CloudFront distribution for the static content. Place the EC2 instances in an Auto Scaling group to launch new instances based on network trac.

- C. Add an Amazon CloudFront distribution for the dynamic content. Add an Amazon ElastiCache instance in front of the ALB to reduce trac for the API to handle.

- D. Add an Amazon CloudFront distribution for the static content. Add an Amazon Simple Queue Service (Amazon SQS) queue to receive requests from the website for later processing by the EC2 instances.

**Correct:** D
**Why:** Cache static content with CloudFront and use SQS to buffer backend requests to ensure all are processed.

**Incorrect:**
- A: Dynamic CloudFront or ElastiCache alone won’t handle sudden async workload spikes.
- B: Without queueing, backend can still be overwhelmed.
- C: Dynamic CloudFront or ElastiCache alone won’t handle sudden async workload spikes.


---

---

### Question #344

A company has a Java application that uses Amazon Simple Queue Service (Amazon SQS) to parse messages. The application cannot parse messages that are larger than 256 KB in size. The company wants to implement a solution to give the application the ability to parse messages as large as 50 MB. Which solution will meet these requirements with the FEWEST changes to the code?

- A. Use the Amazon SQS Extended Client Library for Java to host messages that are larger than 256 KB in Amazon S3.

- B. Use Amazon EventBridge to post large messages from the application instead of Amazon SQS.

- C. Change the limit in Amazon SQS to handle messages that are larger than 256 KB.

- D. Store messages that are larger than 256 KB in Amazon Elastic File System (Amazon EFS). Configure Amazon SQS to reference this location in the messages.

**Correct:** A
**Why:** SQS Extended Client Library stores message payloads in S3 to overcome the 256 KB limit up to 2–50 MB.

**Incorrect:**
- B: EventBridge isn’t a queue; SQS cannot raise native limit; EFS indirection isn’t integrated.
- C: EventBridge isn’t a queue; SQS cannot raise native limit; EFS indirection isn’t integrated.
- D: EventBridge isn’t a queue; SQS cannot raise native limit; EFS indirection isn’t integrated.


---

---

### Question #360

A company uses Amazon API Gateway to run a private gateway with two REST APIs in the same VPC. The BuyStock RESTful web service calls the CheckFunds RESTful web service to ensure that enough funds are available before a stock can be purchased. The company has noticed in the VPC flow logs that the BuyStock RESTful web service calls the CheckFunds RESTful web service over the internet instead of through the VPC. A solutions architect must implement a solution so that the APIs communicate through the VPC. Which solution will meet these requirements with the FEWEST changes to the code?

- A. Add an X-API-Key header in the HTTP header for authorization.

- B. Use an interface endpoint.

- C. Use a gateway endpoint.

- D. Add an Amazon Simple Queue Service (Amazon SQS) queue between the two REST APIs.

**Correct:** B
**Why:** An interface VPC endpoint for API Gateway ensures private communication between private APIs through the VPC with minimal code changes.

**Incorrect:**
- A: API keys don’t change the network path.
- C: Gateway endpoints are for S3/DynamoDB, not API Gateway.
- D: Adding SQS changes the design and still doesn’t ensure private API‑to‑API calls.


---

---

### Question #362

A company uses a payment processing system that requires messages for a particular payment ID to be received in the same order that they were sent. Otherwise, the payments might be processed incorrectly. Which actions should a solutions architect take to meet this requirement? (Choose two.)

- A. Write the messages to an Amazon DynamoDB table with the payment ID as the partition key.

- B. Write the messages to an Amazon Kinesis data stream with the payment ID as the partition key.

- C. Write the messages to an Amazon ElastiCache for Memcached cluster with the payment ID as the key.

- D. Write the messages to an Amazon Simple Queue Service (Amazon SQS) queue. Set the message attribute to use the payment ID.

E. Write the messages to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the message group to use the payment ID.

**Correct:** B, E
**Why:** Kinesis shards preserve ordering by partition key; SQS FIFO preserves message order per message group.

**Incorrect:**
- A: DynamoDB does not enforce arrival/processing order.
- C: ElastiCache is not a message bus with ordering guarantees.
- D: Standard SQS queues do not guarantee ordering.


---

---

### Question #363

A company is building a game system that needs to send unique events to separate leaderboard, matchmaking, and authentication services concurrently. The company needs an AWS event-driven system that guarantees the order of the events. Which solution will meet these requirements?

- A. Amazon EventBridge event bus

- B. Amazon Simple Notification Service (Amazon SNS) FIFO topics

- C. Amazon Simple Notification Service (Amazon SNS) standard topics

- D. Amazon Simple Queue Service (Amazon SQS) FIFO queues

**Correct:** B
**Why:** SNS FIFO topics provide ordered, exactly‑once message fanout to multiple subscribers, meeting concurrency with order guarantees.

**Incorrect:**
- A: EventBridge does not guarantee strict ordering across targets.
- C: SNS standard topics are at‑least‑once and unordered.
- D: A single SQS FIFO queue cannot fan out to multiple services concurrently without extra components.


---

---

### Question #364

A hospital is designing a new application that gathers symptoms from patients. The hospital has decided to use Amazon Simple Queue Service (Amazon SQS) and Amazon Simple Notification Service (Amazon SNS) in the architecture. A solutions architect is reviewing the infrastructure design. Data must be encrypted at rest and in transit. Only authorized personnel of the hospital should be able to access the data. Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)

- A. Turn on server-side encryption on the SQS components. Update the default key policy to restrict key usage to a set of authorized principals.

- B. Turn on server-side encryption on the SNS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply a key policy to restrict key usage to a set of authorized principals.

- C. Turn on encryption on the SNS components. Update the default key policy to restrict key usage to a set of authorized principals. Set a condition in the topic policy to allow only encrypted connections over TLS.

- D. Turn on server-side encryption on the SQS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply a key policy to restrict key usage to a set of authorized principals. Set a condition in the queue policy to allow only encrypted connections over TLS.

E. Turn on server-side encryption on the SQS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply an IAM policy to restrict key usage to a set of authorized principals. Set a condition in the queue policy to allow only encrypted connections over TLS.

**Correct:** B, D
**Why:** Use KMS CMKs for server‑side encryption on SNS and SQS; restrict key usage to authorized principals and enforce TLS in topic/queue policies.

**Incorrect:**
- A: Missing CMK use where required or rely on default key policy/IAM policy inappropriately.
- C: Missing CMK use where required or rely on default key policy/IAM policy inappropriately.
- E: Missing CMK use where required or rely on default key policy/IAM policy inappropriately.


---

---

### Question #375

An ecommerce company is building a distributed application that involves several serverless functions and AWS services to complete order- processing tasks. These tasks require manual approvals as part of the workow. A solutions architect needs to design an architecture for the order-processing application. The solution must be able to combine multiple AWS Lambda functions into responsive serverless applications. The solution also must orchestrate data and services that run on Amazon EC2 instances, containers, or on-premises servers. Which solution will meet these requirements with the LEAST operational overhead?

- A. Use AWS Step Functions to build the application.

- B. Integrate all the application components in an AWS Glue job.

- C. Use Amazon Simple Queue Service (Amazon SQS) to build the application.

- D. Use AWS Lambda functions and Amazon EventBridge events to build the application.

**Correct:** A
**Why:** Step Functions orchestrates serverless and non‑serverless tasks, supports human approval steps, and simplifies complex workflows.

**Incorrect:**
- B: AWS Glue is for data ETL, not general orchestration with approvals.
- C: SQS/EventBridge alone do not provide orchestration/state management.
- D: SQS/EventBridge alone do not provide orchestration/state management.


---

---

### Question #400

A meteorological startup company has a custom web application to sell weather data to its users online. The company uses Amazon DynamoDB to store its data and wants to build a new service that sends an alert to the managers of four internal teams every time a new weather event is recorded. The company does not want this new service to affect the performance of the current application. What should a solutions architect do to meet these requirements with the LEAST amount of operational overhead?

- A. Use DynamoDB transactions to write new event data to the table. Configure the transactions to notify internal teams.

- B. Have the current application publish a message to four Amazon Simple Notification Service (Amazon SNS) topics. Have each team subscribe to one topic.

- C. Enable Amazon DynamoDB Streams on the table. Use triggers to write to a single Amazon Simple Notification Service (Amazon SNS) topic to which the teams can subscribe.

- D. Add a custom attribute to each record to ag new items. Write a cron job that scans the table every minute for items that are new and noties an Amazon Simple Queue Service (Amazon SQS) queue to which the teams can subscribe.

**Correct:** C
**Why:** DynamoDB Streams with a trigger can publish to a single SNS topic for fanout to the four teams without impacting the main app.

**Incorrect:**
- A: Transactions don’t notify; they ensure atomic writes.
- B: Having the app publish to four topics adds coupling.
- D: Cron scans are inefficient and add lag.


---

---

### Question #404

A company has deployed a serverless application that invokes an AWS Lambda function when new documents are uploaded to an Amazon S3 bucket. The application uses the Lambda function to process the documents. After a recent marketing campaign, the company noticed that the application did not process many of the documents. What should a solutions architect do to improve the architecture of this application?

- A. Set the Lambda function's runtime timeout value to 15 minutes.

- B. Configure an S3 bucket replication policy. Stage the documents in the S3 bucket for later processing.

- C. Deploy an additional Lambda function. Load balance the processing of the documents across the two Lambda functions.

- D. Create an Amazon Simple Queue Service (Amazon SQS) queue. Send the requests to the queue. Configure the queue as an event source for Lambda.

**Correct:** D
**Why:** Decouple S3 events with SQS; Lambda polls the queue, improving durability and handling bursts.

**Incorrect:**
- A: Increasing timeout does not fix event loss or throttling.
- B: Replication doesn’t address processing backpressure.
- C: Load balancing Lambdas is not applicable.


---

---

### Question #422

A company is developing a new machine learning (ML) model solution on AWS. The models are developed as independent microservices that fetch approximately 1 GB of model data from Amazon S3 at startup and load the data into memory. Users access the models through an asynchronous API. Users can send a request or a batch of requests and specify where the results should be sent. The company provides models to hundreds of users. The usage patterns for the models are irregular. Some models could be unused for days or weeks. Other models could receive batches of thousands of requests at a time. Which design should a solutions architect recommend to meet these requirements?

- A. Direct the requests from the API to a Network Load Balancer (NLB). Deploy the models as AWS Lambda functions that are invoked by the NLB.

- B. Direct the requests from the API to an Application Load Balancer (ALB). Deploy the models as Amazon Elastic Container Service (Amazon ECS) services that read from an Amazon Simple Queue Service (Amazon SQS) queue. Use AWS App Mesh to scale the instances of the ECS cluster based on the SQS queue size.

- C. Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue. Deploy the models as AWS Lambda functions that are invoked by SQS events. Use AWS Auto Scaling to increase the number of vCPUs for the Lambda functions based on the SQS queue size.

- D. Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue. Deploy the models as Amazon Elastic Container Service (Amazon ECS) services that read from the queue. Enable AWS Auto Scaling on Amazon ECS for both the cluster and copies of the service based on the queue size.

**Correct:** D
**Why:** Queue requests in SQS and run models on ECS services that scale on queue depth; suitable for 1 GB model loads and bursty use.

**Incorrect:**
- A: NLB→Lambda is not a pattern; Lambda cold starts with 1 GB model pulls are costly.
- B: ALB→ECS without decoupling risks backpressure; App Mesh is unnecessary.
- C: Lambda with 1 GB model loads and vCPU scaling is not appropriate.


---

---

### Question #462

A company has an application that processes customer orders. The company hosts the application on an Amazon EC2 instance that saves the orders to an Amazon Aurora database. Occasionally when trac is high the workload does not process orders fast enough. What should a solutions architect do to write the orders reliably to the database as quickly as possible?

- A. Increase the instance size of the EC2 instance when trac is high. Write orders to Amazon Simple Notification Service (Amazon SNS). Subscribe the database endpoint to the SNS topic.

- B. Write orders to an Amazon Simple Queue Service (Amazon SQS) queue. Use EC2 instances in an Auto Scaling group behind an Application Load Balancer to read from the SQS queue and process orders into the database.

- C. Write orders to Amazon Simple Notification Service (Amazon SNS). Subscribe the database endpoint to the SNS topic. Use EC2 instances in an Auto Scaling group behind an Application Load Balancer to read from the SNS topic.

- D. Write orders to an Amazon Simple Queue Service (Amazon SQS) queue when the EC2 instance reaches CPU threshold limits. Use scheduled scaling of EC2 instances in an Auto Scaling group behind an Application Load Balancer to read from the SQS queue and process orders into the database.

**Correct:** B
**Why:** Buffer orders to SQS and scale consumers behind an ALB/ASG to write quickly and reliably to Aurora.

**Incorrect:**
- A: SNS lacks durable queue semantics; scaling only on CPU misses spikes; D is reactive.
- C: SNS lacks durable queue semantics; scaling only on CPU misses spikes; D is reactive.
- D: SNS lacks durable queue semantics; scaling only on CPU misses spikes; D is reactive.


---

---

### Question #489

An ecommerce company runs an application in the AWS Cloud that is integrated with an on-premises warehouse solution. The company uses Amazon Simple Notification Service (Amazon SNS) to send order messages to an on-premises HTTPS endpoint so the warehouse application can process the orders. The local data center team has detected that some of the order messages were not received. A solutions architect needs to retain messages that are not delivered and analyze the messages for up to 14 days. Which solution will meet these requirements with the LEAST development effort?

- A. Configure an Amazon SNS dead letter queue that has an Amazon Kinesis Data Stream target with a retention period of 14 days.

- B. Add an Amazon Simple Queue Service (Amazon SQS) queue with a retention period of 14 days between the application and Amazon SNS.

- C. Configure an Amazon SNS dead letter queue that has an Amazon Simple Queue Service (Amazon SQS) target with a retention period of 14 days.

- D. Configure an Amazon SNS dead letter queue that has an Amazon DynamoDB target with a TTL attribute set for a retention period of 14 days.

**Correct:** C
**Why:** Configure an SNS dead‑letter queue with SQS target and 14‑day retention to retain and analyze undelivered messages with minimal dev.

**Incorrect:**
- A: Kinesis/DynamoDB targets add complexity.
- B: Adding SQS in front of SNS changes the design.
- D: Kinesis/DynamoDB targets add complexity.


---

---

### Question #491

A solutions architect is designing an asynchronous application to process credit card data validation requests for a bank. The application must be secure and be able to process each request at least once. Which solution will meet these requirements MOST cost-effectively?

- A. Use AWS Lambda event source mapping. Set Amazon Simple Queue Service (Amazon SQS) standard queues as the event source. Use AWS Key Management Service (SSE-KMS) for encryption. Add the kms:Decrypt permission for the Lambda execution role.

- B. Use AWS Lambda event source mapping. Use Amazon Simple Queue Service (Amazon SQS) FIFO queues as the event source. Use SQS managed encryption keys (SSE-SQS) for encryption. Add the encryption key invocation permission for the Lambda function.

- C. Use the AWS Lambda event source mapping. Set Amazon Simple Queue Service (Amazon SQS) FIFO queues as the event source. Use AWS KMS keys (SSE-KMS). Add the kms:Decrypt permission for the Lambda execution role.

- D. Use the AWS Lambda event source mapping. Set Amazon Simple Queue Service (Amazon SQS) standard queues as the event source. Use AWS KMS keys (SSE-KMS) for encryption. Add the encryption key invocation permission for the Lambda function.

**Correct:** A
**Why:** SQS standard queues provide at‑least‑once delivery; encrypt with SSE‑KMS and grant Lambda kms:Decrypt.

**Incorrect:**
- B: FIFO is not required and may cost more; managed keys choice varies.
- C: FIFO is not required and may cost more; managed keys choice varies.
- D: Similar to A but misstates key permissions pattern.


---

## Amazon VPC

### Question #251

An Amazon EC2 instance is located in a private subnet in a new VPC. This subnet does not have outbound internet access, but the EC2 instance needs the ability to download monthly security updates from an outside vendor. What should a solutions architect do to meet these requirements?

- A. Create an internet gateway, and attach it to the VPC. Configure the private subnet route table to use the internet gateway as the default route.

- B. Create a NAT gateway, and place it in a public subnet. Configure the private subnet route table to use the NAT gateway as the default route.

- C. Create a NAT instance, and place it in the same subnet where the EC2 instance is located. Configure the private subnet route table to use the NAT instance as the default route.

- D. Create an internet gateway, and attach it to the VPC. Create a NAT instance, and place it in the same subnet where the EC2 instance is located. Configure the private subnet route table to use the internet gateway as the default route.

**Correct:** B
**Why:** Place a NAT gateway in a public subnet and route the private subnet’s default route to it for secure outbound internet access.

**Incorrect:**
- A: An internet gateway alone does not provide egress for private subnets.
- C: NAT instance in same private subnet will not work and adds ops burden.
- D: An internet gateway alone does not provide egress for private subnets.


---

---

### Question #254

A company is reviewing a recent migration of a three-tier application to a VPC. The security team discovers that the principle of least privilege is not being applied to Amazon EC2 security group ingress and egress rules between the application tiers. What should a solutions architect do to correct this issue?

- A. Create security group rules using the instance ID as the source or destination.

- B. Create security group rules using the security group ID as the source or destination.

- C. Create security group rules using the VPC CIDR blocks as the source or destination.

- D. Create security group rules using the subnet CIDR blocks as the source or destination.

**Correct:** B
**Why:** Reference security group IDs in rules to tightly allow only the specific tier’s traffic (least privilege).

**Incorrect:**
- A: Instance IDs, VPC CIDRs, or subnet CIDRs are broader and less maintainable.
- C: Instance IDs, VPC CIDRs, or subnet CIDRs are broader and less maintainable.
- D: Instance IDs, VPC CIDRs, or subnet CIDRs are broader and less maintainable.


---

---

### Question #262

A company plans to use Amazon ElastiCache for its multi-tier web application. A solutions architect creates a Cache VPC for the ElastiCache cluster and an App VPC for the application’s Amazon EC2 instances. Both VPCs are in the us-east-1 Region. The solutions architect must implement a solution to provide the application’s EC2 instances with access to the ElastiCache cluster. Which solution will meet these requirements MOST cost-effectively?

- A. Create a peering connection between the VPCs. Add a route table entry for the peering connection in both VPCs. Configure an inbound rule for the ElastiCache cluster’s security group to allow inbound connection from the application’s security group.

- B. Create a Transit VPC. Update the VPC route tables in the Cache VPC and the App VPC to route trac through the Transit VPC. Configure an inbound rule for the ElastiCache cluster's security group to allow inbound connection from the application’s security group.

- C. Create a peering connection between the VPCs. Add a route table entry for the peering connection in both VPCs. Configure an inbound rule for the peering connection’s security group to allow inbound connection from the application’s security group.

- D. Create a Transit VPC. Update the VPC route tables in the Cache VPC and the App VPC to route trac through the Transit VPC. Configure an inbound rule for the Transit VPC’s security group to allow inbound connection from the application’s security group.

**Correct:** A
**Why:** VPC peering with route entries and SG inbound from the app’s SG is the most cost‑effective cross‑VPC access.

**Incorrect:**
- B: Transit VPC adds cost/complexity without need.
- C: Peering connections don’t have security groups.
- D: Transit VPC adds cost/complexity without need.


---

---

### Question #265

A solutions architect needs to design a highly available application consisting of web, application, and database tiers. HTTPS content delivery should be as close to the edge as possible, with the least delivery time. Which solution meets these requirements and is MOST secure?

- A. Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in public subnets. Configure Amazon CloudFront to deliver HTTPS content using the public ALB as the origin.

- B. Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in private subnets. Configure Amazon CloudFront to deliver HTTPS content using the EC2 instances as the origin.

- C. Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in private subnets. Configure Amazon CloudFront to deliver HTTPS content using the public ALB as the origin.

- D. Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in public subnets. Configure Amazon CloudFront to deliver HTTPS content using the EC2 instances as the origin.

**Correct:** C
**Why:** Place EC2 in private subnets behind a public ALB; put CloudFront in front for HTTPS at the edge.

**Incorrect:**
- A: Either expose EC2 publicly or skip CloudFront edge TLS offload.
- B: Either expose EC2 publicly or skip CloudFront edge TLS offload.
- D: Either expose EC2 publicly or skip CloudFront edge TLS offload.


---

---

### Question #282

A company runs a web application that is deployed on Amazon EC2 instances in the private subnet of a VPC. An Application Load Balancer (ALB) that extends across the public subnets directs web trac to the EC2 instances. The company wants to implement new security measures to restrict inbound trac from the ALB to the EC2 instances while preventing access from any other source inside or outside the private subnet of the EC2 instances. Which solution will meet these requirements?

- A. Configure a route in a route table to direct trac from the internet to the private IP addresses of the EC2 instances.

- B. Configure the security group for the EC2 instances to only allow trac that comes from the security group for the ALB.

- C. Move the EC2 instances into the public subnet. Give the EC2 instances a set of Elastic IP addresses.

- D. Configure the security group for the ALB to allow any TCP trac on any port.

**Correct:** B
**Why:** Allow only the ALB security group as a source to the EC2 instances’ security group.

**Incorrect:**
- A: Route table or broad ALB SG rules do not enforce least privilege inbound to EC2.
- C: Route table or broad ALB SG rules do not enforce least privilege inbound to EC2.
- D: Route table or broad ALB SG rules do not enforce least privilege inbound to EC2.


---

---

### Question #294

An application that is hosted on Amazon EC2 instances needs to access an Amazon S3 bucket. Trac must not traverse the internet. How should a solutions architect configure access to meet these requirements?

- A. Create a private hosted zone by using Amazon Route 53.

- B. Set up a gateway VPC endpoint for Amazon S3 in the VPC.

- C. Configure the EC2 instances to use a NAT gateway to access the S3 bucket.

- D. Establish an AWS Site-to-Site VPN connection between the VPC and the S3 bucket.

**Correct:** B
**Why:** S3 gateway VPC endpoint keeps traffic private between EC2 and S3.

**Incorrect:**
- A: Private hosted zones/NAT/VPN are unnecessary for S3 private access.
- C: Private hosted zones/NAT/VPN are unnecessary for S3 private access.
- D: Private hosted zones/NAT/VPN are unnecessary for S3 private access.


---

---

### Question #296

A development team has launched a new application that is hosted on Amazon EC2 instances inside a development VPC. A solutions architect needs to create a new VPC in the same account. The new VPC will be peered with the development VPC. The VPC CIDR block for the development VPC is 192.168.0.0/24. The solutions architect needs to create a CIDR block for the new VPC. The CIDR block must be valid for a VPC peering connection to the development VPC. What is the SMALLEST CIDR block that meets these requirements?

- A. 10.0.1.0/32

- B. 192.168.0.0/24

- C. 192.168.1.0/32

- D. 10.0.1.0/24

**Correct:** D
**Why:** VPC peering requires non‑overlapping CIDRs; /24 is the smallest VPC CIDR; 10.0.1.0/24 does not overlap.

**Incorrect:**
- A: /32 is invalid for VPC.
- B: Overlaps with 192.168.0.0/24.
- C: /32 is invalid for VPC.


---

---

### Question #298

A company is running a critical business application on Amazon EC2 instances behind an Application Load Balancer. The EC2 instances run in an Auto Scaling group and access an Amazon RDS DB instance. The design did not pass an operational review because the EC2 instances and the DB instance are all located in a single Availability Zone. A solutions architect must update the design to use a second Availability Zone. Which solution will make the application highly available?

- A. Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance with connections to each network.

- B. Provision two subnets that extend across both Availability Zones. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance with connections to each network.

- C. Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance for Multi-AZ deployment.

- D. Provision a subnet that extends across both Availability Zones. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance for Multi-AZ deployment.

**Correct:** C
**Why:** Distribute EC2 across AZs via ASG and enable RDS Multi‑AZ for database HA.

**Incorrect:**
- A: Incorrect subnet constructs or missing Multi‑AZ DB.
- B: Incorrect subnet constructs or missing Multi‑AZ DB.
- D: Incorrect subnet constructs or missing Multi‑AZ DB.


---

---

### Question #323

A company’s facility has badge readers at every entrance throughout the building. When badges are scanned, the readers send a message over HTTPS to indicate who attempted to access that particular entrance. A solutions architect must design a system to process these messages from the sensors. The solution must be highly available, and the results must be made available for the company’s security team to analyze. Which system architecture should the solutions architect recommend?

- A. Launch an Amazon EC2 instance to serve as the HTTPS endpoint and to process the messages. Configure the EC2 instance to save the results to an Amazon S3 bucket.

- B. Create an HTTPS endpoint in Amazon API Gateway. Configure the API Gateway endpoint to invoke an AWS Lambda function to process the messages and save the results to an Amazon DynamoDB table.

- C. Use Amazon Route 53 to direct incoming sensor messages to an AWS Lambda function. Configure the Lambda function to process the messages and save the results to an Amazon DynamoDB table.

- D. Create a gateway VPC endpoint for Amazon S3. Configure a Site-to-Site VPN connection from the facility network to the VPC so that sensor data can be written directly to an S3 bucket by way of the VPC endpoint.

**Correct:** B
**Why:** API Gateway HTTPS endpoint invoking Lambda, storing to DynamoDB, is highly available and serverless.

**Incorrect:**
- A: EC2 endpoint or direct S3 via VPN increases ops/complexity.
- C: Route 53 cannot directly invoke code.
- D: EC2 endpoint or direct S3 via VPN increases ops/complexity.


---

---

### Question #327

A solutions architect must secure a VPC network that hosts Amazon EC2 instances. The EC2 instances contain highly sensitive data and run in a private subnet. According to company policy, the EC2 instances that run in the VPC can access only approved third-party software repositories on the internet for software product updates that use the third party’s URL. Other internet trac must be blocked. Which solution meets these requirements?

- A. Update the route table for the private subnet to route the outbound trac to an AWS Network Firewall firewall. Configure domain list rule groups.

- B. Set up an AWS WAF web ACL. Create a custom set of rules that filter trac requests based on source and destination IP address range sets.

- C. Implement strict inbound security group rules. Configure an outbound rule that allows trac only to the authorized software repositories on the internet by specifying the URLs.

- D. Configure an Application Load Balancer (ALB) in front of the EC2 instances. Direct all outbound trac to the ALB. Use a URL-based rule listener in the ALB’s target group for outbound access to the internet.

**Correct:** A
**Why:** AWS Network Firewall with domain list rules provides egress filtering to approved repositories from private subnets.

**Incorrect:**
- B: WAF is for HTTP(S) to apps; SGs cannot filter by domain/URL; ALB is inbound.
- C: WAF is for HTTP(S) to apps; SGs cannot filter by domain/URL; ALB is inbound.
- D: WAF is for HTTP(S) to apps; SGs cannot filter by domain/URL; ALB is inbound.


---

---

### Question #332

A company needs to provide its employees with secure access to condential and sensitive files. The company wants to ensure that the files can be accessed only by authorized users. The files must be downloaded securely to the employees’ devices. The files are stored in an on-premises Windows file server. However, due to an increase in remote usage, the file server is running out of capacity. . Which solution will meet these requirements?

- A. Migrate the file server to an Amazon EC2 instance in a public subnet. Configure the security group to limit inbound trac to the employees’ IP addresses.

- B. Migrate the files to an Amazon FSx for Windows File Server file system. Integrate the Amazon FSx file system with the on-premises Active Directory. Configure AWS Client VPN.

- C. Migrate the files to Amazon S3, and create a private VPC endpoint. Create a signed URL to allow download.

- D. Migrate the files to Amazon S3, and create a public VPC endpoint. Allow employees to sign on with AWS IAM Identity Center (AWS Single Sign-On).

**Correct:** B
**Why:** FSx for Windows with on‑prem AD integration preserves permissions; Client VPN provides secure remote access and downloads.

**Incorrect:**
- A: Public EC2, S3 signed URLs, or public endpoints don’t integrate with AD or meet security requirements.
- C: Public EC2, S3 signed URLs, or public endpoints don’t integrate with AD or meet security requirements.
- D: Public EC2, S3 signed URLs, or public endpoints don’t integrate with AD or meet security requirements.


---

---

### Question #343

A solutions architect is designing a company’s disaster recovery (DR) architecture. The company has a MySQL database that runs on an Amazon EC2 instance in a private subnet with scheduled backup. The DR design needs to include multiple AWS Regions. Which solution will meet these requirements with the LEAST operational overhead?

- A. Migrate the MySQL database to multiple EC2 instances. Configure a standby EC2 instance in the DR Region. Turn on replication.

- B. Migrate the MySQL database to Amazon RDS. Use a Multi-AZ deployment. Turn on read replication for the primary DB instance in the different Availability Zones.

- C. Migrate the MySQL database to an Amazon Aurora global database. Host the primary DB cluster in the primary Region. Host the secondary DB cluster in the DR Region.

- D. Store the scheduled backup of the MySQL database in an Amazon S3 bucket that is congured for S3 Cross-Region Replication (CRR). Use the data backup to restore the database in the DR Region.

**Correct:** C
**Why:** Aurora Global Database provides multi‑Region design with low ops and built‑in replication; best for DR across Regions.

**Incorrect:**
- A: EC2 self‑managed, single‑Region RDS Multi‑AZ, or CRR of backups offer higher RTO/RPO or more ops.
- B: EC2 self‑managed, single‑Region RDS Multi‑AZ, or CRR of backups offer higher RTO/RPO or more ops.
- D: EC2 self‑managed, single‑Region RDS Multi‑AZ, or CRR of backups offer higher RTO/RPO or more ops.


---

---

### Question #352

A company is designing the network for an online multi-player game. The game uses the UDP networking protocol and will be deployed in eight AWS Regions. The network architecture needs to minimize latency and packet loss to give end users a high-quality gaming experience. Which solution will meet these requirements?

- A. Setup a transit gateway in each Region. Create inter-Region peering attachments between each transit gateway.

- B. Set up AWS Global Accelerator with UDP listeners and endpoint groups in each Region.

- C. Set up Amazon CloudFront with UDP turned on. Configure an origin in each Region.

- D. Set up a VPC peering mesh between each Region. Turn on UDP for each VPC.

**Correct:** B
**Why:** AWS Global Accelerator supports UDP and provides static anycast IPs with global edge network to reduce latency and packet loss across Regions.

**Incorrect:**
- A: Transit gateways and peering do not optimize internet edge-to-Region latency for end users.
- C: CloudFront does not proxy UDP.
- D: VPC peering is for inter-VPC traffic; end users still traverse the internet without edge acceleration.


---

---

### Question #360

A company uses Amazon API Gateway to run a private gateway with two REST APIs in the same VPC. The BuyStock RESTful web service calls the CheckFunds RESTful web service to ensure that enough funds are available before a stock can be purchased. The company has noticed in the VPC flow logs that the BuyStock RESTful web service calls the CheckFunds RESTful web service over the internet instead of through the VPC. A solutions architect must implement a solution so that the APIs communicate through the VPC. Which solution will meet these requirements with the FEWEST changes to the code?

- A. Add an X-API-Key header in the HTTP header for authorization.

- B. Use an interface endpoint.

- C. Use a gateway endpoint.

- D. Add an Amazon Simple Queue Service (Amazon SQS) queue between the two REST APIs.

**Correct:** B
**Why:** An interface VPC endpoint for API Gateway ensures private communication between private APIs through the VPC with minimal code changes.

**Incorrect:**
- A: API keys don’t change the network path.
- C: Gateway endpoints are for S3/DynamoDB, not API Gateway.
- D: Adding SQS changes the design and still doesn’t ensure private API‑to‑API calls.


---

---

### Question #370

A company runs a public three-tier web application in a VPC. The application runs on Amazon EC2 instances across multiple Availability Zones. The EC2 instances that run in private subnets need to communicate with a license server over the internet. The company needs a managed solution that minimizes operational maintenance. Which solution meets these requirements?

- A. Provision a NAT instance in a public subnet. Modify each private subnet's route table with a default route that points to the NAT instance.

- B. Provision a NAT instance in a private subnet. Modify each private subnet's route table with a default route that points to the NAT instance.

- C. Provision a NAT gateway in a public subnet. Modify each private subnet's route table with a default route that points to the NAT gateway.

- D. Provision a NAT gateway in a private subnet. Modify each private subnet's route table with a default route that points to the NAT gateway.

**Correct:** C
**Why:** NAT gateway in a public subnet is the managed solution for outbound internet from private subnets with minimal ops.

**Incorrect:**
- A: NAT instance adds ops; NAT gateway must be in a public subnet.
- B: NAT instance adds ops; NAT gateway must be in a public subnet.
- D: NAT instance adds ops; NAT gateway must be in a public subnet.


---

---

### Question #374

A company is running several business applications in three separate VPCs within the us-east-1 Region. The applications must be able to communicate between VPCs. The applications also must be able to consistently send hundreds of gigabytes of data each day to a latency- sensitive application that runs in a single on-premises data center. A solutions architect needs to design a network connectivity solution that maximizes cost-effectiveness. Which solution meets these requirements?

- A. Configure three AWS Site-to-Site VPN connections from the data center to AWS. Establish connectivity by conguring one VPN connection for each VPC.

- B. Launch a third-party virtual network appliance in each VPC. Establish an IPsec VPN tunnel between the data center and each virtual appliance.

- C. Set up three AWS Direct Connect connections from the data center to a Direct Connect gateway in us-east-1. Establish connectivity by conguring each VPC to use one of the Direct Connect connections.

- D. Set up one AWS Direct Connect connection from the data center to AWS. Create a transit gateway, and attach each VPC to the transit gateway. Establish connectivity between the Direct Connect connection and the transit gateway.

**Correct:** D
**Why:** A single Direct Connect with a transit gateway lets multiple VPCs share the link and provides cost‑effective, consistent, low‑latency connectivity to on‑premises.

**Incorrect:**
- A: Multiple VPNs won’t meet latency/throughput needs.
- B: Third‑party appliances add cost/ops and complexity.
- C: Separate DX per VPC is unnecessary and costly.


---

---

### Question #385

A solutions architect is creating a new VPC design. There are two public subnets for the load balancer, two private subnets for web servers, and two private subnets for MySQL. The web servers use only HTTPS. The solutions architect has already created a security group for the load balancer allowing port 443 from 0.0.0.0/0. Company policy requires that each resource has the least access required to still be able to perform its tasks. Which additional configuration strategy should the solutions architect use to meet these requirements?

- A. Create a security group for the web servers and allow port 443 from 0.0.0.0/0. Create a security group for the MySQL servers and allow port 3306 from the web servers security group.

- B. Create a network ACL for the web servers and allow port 443 from 0.0.0.0/0. Create a network ACL for the MySQL servers and allow port 3306 from the web servers security group.

- C. Create a security group for the web servers and allow port 443 from the load balancer. Create a security group for the MySQL servers and allow port 3306 from the web servers security group.

- D. Create a network ACL for the web servers and allow port 443 from the load balancer. Create a network ACL for the MySQL servers and allow port 3306 from the web servers security group.

**Correct:** C
**Why:** Least privilege: Web SG allows 443 only from LB SG; DB SG allows 3306 only from Web SG.

**Incorrect:**
- A: Allowing from 0.0.0.0/0 or using NACLs is broader than necessary.
- B: Allowing from 0.0.0.0/0 or using NACLs is broader than necessary.
- D: Allowing from 0.0.0.0/0 or using NACLs is broader than necessary.


---

---

### Question #388

A company is deploying a two-tier web application in a VPC. The web tier is using an Amazon EC2 Auto Scaling group with public subnets that span multiple Availability Zones. The database tier consists of an Amazon RDS for MySQL DB instance in separate private subnets. The web tier requires access to the database to retrieve product information. The web application is not working as intended. The web application reports that it cannot connect to the database. The database is conrmed to be up and running. All congurations for the network ACLs, security groups, and route tables are still in their default states. What should a solutions architect recommend to x the application?

- A. Add an explicit rule to the private subnet’s network ACL to allow trac from the web tier’s EC2 instances.

- B. Add a route in the VPC route table to allow trac between the web tier’s EC2 instances and the database tier.

- C. Deploy the web tier's EC2 instances and the database tier’s RDS instance into two separate VPCs, and configure VPC peering.

- D. Add an inbound rule to the security group of the database tier’s RDS instance to allow trac from the web tiers security group.

**Correct:** D
**Why:** By default, RDS SG needs an inbound rule from the web tier SG; this enables connectivity while keeping least privilege.

**Incorrect:**
- A: Default NACLs/route tables already allow VPC internal traffic appropriately.
- B: Default NACLs/route tables already allow VPC internal traffic appropriately.
- C: VPC peering is unnecessary for a single VPC.


---

---

### Question #405

A solutions architect is designing the architecture for a software demonstration environment. The environment will run on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). The system will experience signicant increases in trac during working hours but is not required to operate on weekends. Which combination of actions should the solutions architect take to ensure that the system can scale to meet demand? (Choose two.)

- A. Use AWS Auto Scaling to adjust the ALB capacity based on request rate.

- B. Use AWS Auto Scaling to scale the capacity of the VPC internet gateway.

- C. Launch the EC2 instances in multiple AWS Regions to distribute the load across Regions.

- D. Use a target tracking scaling policy to scale the Auto Scaling group based on instance CPU utilization.

E. Use scheduled scaling to change the Auto Scaling group minimum, maximum, and desired capacity to zero for weekends. Revert to the default values at the start of the week.

**Correct:** D, E
**Why:** Use target tracking for dynamic scaling and scheduled scaling to zero on weekends to save cost.

**Incorrect:**
- A: ALB and internet gateways are managed; you don’t scale them.
- B: ALB and internet gateways are managed; you don’t scale them.
- C: Multi-Region is unnecessary for this need.


---

---

### Question #406

A solutions architect is designing a two-tiered architecture that includes a public subnet and a database subnet. The web servers in the public subnet must be open to the internet on port 443. The Amazon RDS for MySQL DB instance in the database subnet must be accessible only to the web servers on port 3306. Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)

- A. Create a network ACL for the public subnet. Add a rule to deny outbound trac to 0.0.0.0/0 on port 3306.

- B. Create a security group for the DB instance. Add a rule to allow trac from the public subnet CIDR block on port 3306.

- C. Create a security group for the web servers in the public subnet. Add a rule to allow trac from 0.0.0.0/0 on port 443.

- D. Create a security group for the DB instance. Add a rule to allow trac from the web servers’ security group on port 3306.

E. Create a security group for the DB instance. Add a rule to deny all trac except trac from the web servers’ security group on port 3306.

**Correct:** C, D
**Why:** Allow 443 from the internet to web SG; allow 3306 to DB SG only from the web servers’ SG.

**Incorrect:**
- A: NACL outbound deny to 0.0.0.0/0 on 3306 is unnecessary and coarse.
- B: Allowing from subnet CIDR is broader than referencing the web SG.
- E: SGs are allow-lists; “deny” rules don’t apply.


---

---

### Question #417

A company uses Amazon EC2 instances and AWS Lambda functions to run its application. The company has VPCs with public subnets and private subnets in its AWS account. The EC2 instances run in a private subnet in one of the VPCs. The Lambda functions need direct network access to the EC2 instances for the application to work. The application will run for at least 1 year. The company expects the number of Lambda functions that the application uses to increase during that time. The company wants to maximize its savings on all application resources and to keep network latency between the services low. Which solution will meet these requirements?

- A. Purchase an EC2 Instance Savings Plan Optimize the Lambda functions’ duration and memory usage and the number of invocations. Connect the Lambda functions to the private subnet that contains the EC2 instances.

- B. Purchase an EC2 Instance Savings Plan Optimize the Lambda functions' duration and memory usage, the number of invocations, and the amount of data that is transferred. Connect the Lambda functions to a public subnet in the same VPC where the EC2 instances run.

- C. Purchase a Compute Savings Plan. Optimize the Lambda functions’ duration and memory usage, the number of invocations, and the amount of data that is transferred. Connect the Lambda functions to the private subnet that contains the EC2 instances.

- D. Purchase a Compute Savings Plan. Optimize the Lambda functions’ duration and memory usage, the number of invocations, and the amount of data that is transferred. Keep the Lambda functions in the Lambda service VPC.

**Correct:** C
**Why:** Compute Savings Plan covers EC2 and Lambda; attach Lambda to the private subnet for low‑latency access to EC2.

**Incorrect:**
- A: EC2 Instance Savings Plan does not apply to Lambda.
- B: EC2 Instance Savings Plan does not apply to Lambda.
- D: Keeping Lambda in the service VPC prevents direct access to private EC2.


---

---

### Question #421

A company runs a highly available SFTP service. The SFTP service uses two Amazon EC2 Linux instances that run with elastic IP addresses to accept trac from trusted IP sources on the internet. The SFTP service is backed by shared storage that is attached to the instances. User accounts are created and managed as Linux users in the SFTP servers. The company wants a serverless option that provides high IOPS performance and highly congurable security. The company also wants to maintain control over user permissions. Which solution will meet these requirements?

- A. Create an encrypted Amazon Elastic Block Store (Amazon EBS) volume. Create an AWS Transfer Family SFTP service with a public endpoint that allows only trusted IP addresses. Attach the EBS volume to the SFTP service endpoint. Grant users access to the SFTP service.

- B. Create an encrypted Amazon Elastic File System (Amazon EFS) volume. Create an AWS Transfer Family SFTP service with elastic IP addresses and a VPC endpoint that has internet-facing access. Attach a security group to the endpoint that allows only trusted IP addresses. Attach the EFS volume to the SFTP service endpoint. Grant users access to the SFTP service.

- C. Create an Amazon S3 bucket with default encryption enabled. Create an AWS Transfer Family SFTP service with a public endpoint that allows only trusted IP addresses. Attach the S3 bucket to the SFTP service endpoint. Grant users access to the SFTP service.

- D. Create an Amazon S3 bucket with default encryption enabled. Create an AWS Transfer Family SFTP service with a VPC endpoint that has internal access in a private subnet. Attach a security group that allows only trusted IP addresses. Attach the S3 bucket to the SFTP service endpoint. Grant users access to the SFTP service.

**Correct:** C
**Why:** Transfer Family with an S3 backend is serverless, scalable, and supports IP allow lists; S3 delivers high throughput/IOPS.

**Incorrect:**
- A: You cannot attach EBS/EFS directly to Transfer Family.
- B: You cannot attach EBS/EFS directly to Transfer Family.
- D: A private VPC endpoint is internal; trusted public IPs could not reach it.


---

---

### Question #431

A company has developed a new video game as a web application. The application is in a three-tier architecture in a VPC with Amazon RDS for MySQL in the database layer. Several players will compete concurrently online. The game’s developers want to display a top-10 scoreboard in near- real time and offer the ability to stop and restore the game while preserving the current scores. What should a solutions architect do to meet these requirements?

- A. Set up an Amazon ElastiCache for Memcached cluster to cache the scores for the web application to display.

- B. Set up an Amazon ElastiCache for Redis cluster to compute and cache the scores for the web application to display.

- C. Place an Amazon CloudFront distribution in front of the web application to cache the scoreboard in a section of the application.

- D. Create a read replica on Amazon RDS for MySQL to run queries to compute the scoreboard and serve the read trac to the web application.

**Correct:** B
**Why:** ElastiCache for Redis supports sorted sets for near‑real‑time leaderboards and can persist to preserve scores.

**Incorrect:**
- A: Memcached lacks persistence and sorted‑set operations.
- C: CloudFront caches static content, not compute leaderboards.
- D: RDS read replica queries add latency and load.


---

---

### Question #437

A company operates an ecommerce website on Amazon EC2 instances behind an Application Load Balancer (ALB) in an Auto Scaling group. The site is experiencing performance issues related to a high request rate from illegitimate external systems with changing IP addresses. The security team is worried about potential DDoS attacks against the website. The company must block the illegitimate incoming requests in a way that has a minimal impact on legitimate users. What should a solutions architect recommend?

- A. Deploy Amazon Inspector and associate it with the ALB.

- B. Deploy AWS WAF, associate it with the ALB, and configure a rate-limiting rule.

- C. Deploy rules to the network ACLs associated with the ALB to block the incomingtrac.

- D. Deploy Amazon GuardDuty and enable rate-limiting protection when conguring GuardDuty.

**Correct:** B
**Why:** Attach AWS WAF to the ALB and configure rate‑based rules to throttle illegitimate request floods while allowing legitimate users.

**Incorrect:**
- A: Inspector/GuardDuty do not mitigate at the edge.
- C: NACLs are coarse and static; IPs change.
- D: Inspector/GuardDuty do not mitigate at the edge.


---

---

### Question #438

A company wants to share accounting data with an external auditor. The data is stored in an Amazon RDS DB instance that resides in a private subnet. The auditor has its own AWS account and requires its own copy of the database. What is the MOST secure way for the company to share the database with the auditor?

- A. Create a read replica of the database. Configure IAM standard database authentication to grant the auditor access.

- B. Export the database contents to text files. Store the files in an Amazon S3 bucket. Create a new IAM user for the auditor. Grant the user access to the S3 bucket.

- C. Copy a snapshot of the database to an Amazon S3 bucket. Create an IAM user. Share the user's keys with the auditor to grant access to the object in the S3 bucket.

- D. Create an encrypted snapshot of the database. Share the snapshot with the auditor. Allow access to the AWS Key Management Service (AWS KMS) encryption key.

**Correct:** D
**Why:** Share an encrypted snapshot with the auditor and permit access to the KMS key; the auditor restores into its own account.

**Incorrect:**
- A: Read replica does not cross accounts easily and IAM DB auth is separate.
- B: Text export or snapshot in S3 is less secure/more work.
- C: Text export or snapshot in S3 is less secure/more work.


---

---

### Question #439

A solutions architect congured a VPC that has a small range of IP addresses. The number of Amazon EC2 instances that are in the VPC is increasing, and there is an insucient number of IP addresses for future workloads. Which solution resolves this issue with the LEAST operational overhead?

- A. Add an additional IPv4 CIDR block to increase the number of IP addresses and create additional subnets in the VPC. Create new resources in the new subnets by using the new CIDR.

- B. Create a second VPC with additional subnets. Use a peering connection to connect the second VPC with the first VPC Update the routes and create new resources in the subnets of the second VPC.

- C. Use AWS Transit Gateway to add a transit gateway and connect a second VPC with the first VPUpdate the routes of the transit gateway and VPCs. Create new resources in the subnets of the second VPC.

- D. Create a second VPC. Create a Site-to-Site VPN connection between the first VPC and the second VPC by using a VPN-hosted solution on Amazon EC2 and a virtual private gateway. Update the route between VPCs to the trac through the VPN. Create new resources in the subnets of the second VPC.

**Correct:** A
**Why:** Add an additional IPv4 CIDR to the VPC and create new subnets—simple and low‑ops.

**Incorrect:**
- B: Creating/connecting a second VPC adds complexity.
- C: Creating/connecting a second VPC adds complexity.
- D: Creating/connecting a second VPC adds complexity.


---

---

### Question #444

A company has hired a solutions architect to design a reliable architecture for its application. The application consists of one Amazon RDS DB instance and two manually provisioned Amazon EC2 instances that run web servers. The EC2 instances are located in a single Availability Zone. An employee recently deleted the DB instance, and the application was unavailable for 24 hours as a result. The company is concerned with the overall reliability of its environment. What should the solutions architect do to maximize reliability of the application's infrastructure?

- A. Delete one EC2 instance and enable termination protection on the other EC2 instance. Update the DB instance to be Multi-AZ, and enable deletion protection.

- B. Update the DB instance to be Multi-AZ, and enable deletion protection. Place the EC2 instances behind an Application Load Balancer, and run them in an EC2 Auto Scaling group across multiple Availability Zones.

- C. Create an additional DB instance along with an Amazon API Gateway and an AWS Lambda function. Configure the application to invoke the Lambda function through API Gateway. Have the Lambda function write the data to the two DB instances.

- D. Place the EC2 instances in an EC2 Auto Scaling group that has multiple subnets located in multiple Availability Zones. Use Spot Instances instead of On-Demand Instances. Set up Amazon CloudWatch alarms to monitor the health of the instances Update the DB instance to be Multi-AZ, and enable deletion protection.

**Correct:** B
**Why:** Protect RDS with Multi‑AZ and deletion protection; place EC2 in ALB+Auto Scaling across AZs to maximize reliability.

**Incorrect:**
- A: Either reduce availability or add unnecessary components/cost.
- C: Either reduce availability or add unnecessary components/cost.
- D: Either reduce availability or add unnecessary components/cost.


---

---

### Question #447

A company has a stateless web application that runs on AWS Lambda functions that are invoked by Amazon API Gateway. The company wants to deploy the application across multiple AWS Regions to provide Regional failover capabilities. What should a solutions architect do to route trac to multiple Regions?

- A. Create Amazon Route 53 health checks for each Region. Use an active-active failover configuration.

- B. Create an Amazon CloudFront distribution with an origin for each Region. Use CloudFront health checks to route trac.

- C. Create a transit gateway. Attach the transit gateway to the API Gateway endpoint in each Region. Configure the transit gateway to route requests.

- D. Create an Application Load Balancer in the primary Region. Set the target group to point to the API Gateway endpoint hostnames in each Region.

**Correct:** A
**Why:** Use Route 53 health checks with active‑active routing across Regions for serverless endpoints.

**Incorrect:**
- B: CloudFront is for HTTP caching, not API multi‑Region routing.
- C: Transit gateway/ALB cannot directly route to API Gateway across Regions as required.
- D: Transit gateway/ALB cannot directly route to API Gateway across Regions as required.


---

---

### Question #448

A company has two VPCs named Management and Production. The Management VPC uses VPNs through a customer gateway to connect to a single device in the data center. The Production VPC uses a virtual private gateway with two attached AWS Direct Connect connections. The Management and Production VPCs both use a single VPC peering connection to allow communication between the applications. What should a solutions architect do to mitigate any single point of failure in this architecture?

- A. Add a set of VPNs between the Management and Production VPCs.

- B. Add a second virtual private gateway and attach it to the Management VPC.

- C. Add a second set of VPNs to the Management VPC from a second customer gateway device.

- D. Add a second VPC peering connection between the Management VPC and the Production VPC.

**Correct:** C
**Why:** Add redundant customer gateway hardware and VPNs to remove the single on‑premises device SPOF for the Management VPC.

**Incorrect:**
- A: Peering redundancy doesn’t fix the on‑prem single device.
- B: A second virtual private gateway in Management VPC doesn’t add on‑prem redundancy.
- D: Peering redundancy doesn’t fix the on‑prem single device.


---

---

### Question #450

A company has a three-tier web application that is in a single server. The company wants to migrate the application to the AWS Cloud. The company also wants the application to align with the AWS Well-Architected Framework and to be consistent with AWS recommended best practices for security, scalability, and resiliency. Which combination of solutions will meet these requirements? (Choose three.)

- A. Create a VPC across two Availability Zones with the application's existing architecture. Host the application with existing architecture on an Amazon EC2 instance in a private subnet in each Availability Zone with EC2 Auto Scaling groups. Secure the EC2 instance with security groups and network access control lists (network ACLs).

- B. Set up security groups and network access control lists (network ACLs) to control access to the database layer. Set up a single Amazon RDS database in a private subnet.

- C. Create a VPC across two Availability Zones. Refactor the application to host the web tier, application tier, and database tier. Host each tier on its own private subnet with Auto Scaling groups for the web tier and application tier.

- D. Use a single Amazon RDS database. Allow database access only from the application tier security group.

E. Use Elastic Load Balancers in front of the web tier. Control access by using security groups containing references to each layer's security groups.

F. Use an Amazon RDS database Multi-AZ cluster deployment in private subnets. Allow database access only from application tier security groups.

**Correct:** C, E, F
**Why:** Refactor into three tiers across two AZs; use ELBs for the web tier; and use RDS Multi‑AZ in private subnets with SGs restricting access from the app tier.

**Incorrect:**
- A: Single RDS or single‑AZ design and flat architectures don’t meet best practices for resiliency/security.
- B: Single RDS or single‑AZ design and flat architectures don’t meet best practices for resiliency/security.
- D: Single RDS or single‑AZ design and flat architectures don’t meet best practices for resiliency/security.


---

---

### Question #468

A company is developing a microservices application that will provide a search catalog for customers. The company must use REST APIs to present the frontend of the application to users. The REST APIs must access the backend services that the company hosts in containers in private VPC subnets. Which solution will meet these requirements?

- A. Design a WebSocket API by using Amazon API Gateway. Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet. Create a private VPC link for API Gateway to access Amazon ECS.

- B. Design a REST API by using Amazon API Gateway. Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet. Create a private VPC link for API Gateway to access Amazon ECS.

- C. Design a WebSocket API by using Amazon API Gateway. Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet. Create a security group for API Gateway to access Amazon ECS.

- D. Design a REST API by using Amazon API Gateway. Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet. Create a security group for API Gateway to access Amazon ECS.

**Correct:** B
**Why:** API Gateway REST API with a VPC Link privately connects to ECS services in private subnets.

**Incorrect:**
- A: WebSocket API or security groups alone don’t provide private integration.
- C: WebSocket API or security groups alone don’t provide private integration.
- D: WebSocket API or security groups alone don’t provide private integration.


---

---

### Question #470

A company has applications hosted on Amazon EC2 instances with IPv6 addresses. The applications must initiate communications with other external applications using the internet. However the company’s security policy states that any external service cannot initiate a connection to the EC2 instances. What should a solutions architect recommend to resolve this issue?

- A. Create a NAT gateway and make it the destination of the subnet's route table

- B. Create an internet gateway and make it the destination of the subnet's route table

- C. Create a virtual private gateway and make it the destination of the subnet's route table

- D. Create an egress-only internet gateway and make it the destination of the subnet's route table

**Correct:** D
**Why:** For IPv6, use an egress‑only internet gateway to allow outbound‑only connections from instances.

**Incorrect:**
- A: NAT/IGW/VGW do not provide outbound‑only behavior for IPv6.
- B: NAT/IGW/VGW do not provide outbound‑only behavior for IPv6.
- C: NAT/IGW/VGW do not provide outbound‑only behavior for IPv6.


---

---

### Question #471

A company is creating an application that runs on containers in a VPC. The application stores and accesses data in an Amazon S3 bucket. During the development phase, the application will store and access 1 TB of data in Amazon S3 each day. The company wants to minimize costs and wants to prevent trac from traversing the internet whenever possible. Which solution will meet these requirements?

- A. Enable S3 Intelligent-Tiering for the S3 bucket

- B. Enable S3 Transfer Acceleration for the S3 bucket

- C. Create a gateway VPC endpoint for Amazon S3. Associate this endpoint with all route tables in the VPC

- D. Create an interface endpoint for Amazon S3 in the VPC. Associate this endpoint with all route tables in the VPC

**Correct:** C
**Why:** A gateway VPC endpoint for S3 keeps traffic private and avoids NAT data processing charges.

**Incorrect:**
- A: Tiering affects storage cost, not network path.
- B: Transfer Acceleration sends traffic over the internet.
- D: S3 uses gateway endpoints, not interface endpoints.


---

---

### Question #474

A company has multiple VPCs across AWS Regions to support and run workloads that are isolated from workloads in other Regions. Because of a recent application launch requirement, the company’s VPCs must communicate with all other VPCs across all Regions. Which solution will meet these requirements with the LEAST amount of administrative effort?

- A. Use VPC peering to manage VPC communication in a single Region. Use VPC peering across Regions to manage VPC communications.

- B. Use AWS Direct Connect gateways across all Regions to connect VPCs across regions and manage VPC communications.

- C. Use AWS Transit Gateway to manage VPC communication in a single Region and Transit Gateway peering across Regions to manage VPC communications.

- D. Use AWS PrivateLink across all Regions to connect VPCs across Regions and manage VPC communications

**Correct:** C
**Why:** Use Transit Gateway for intra‑Region VPC connectivity and TGW peering for cross‑Region mesh—least admin effort.

**Incorrect:**
- A: Peering, DX gateways, or PrivateLink are less scalable for any‑to‑any VPC connectivity.
- B: Peering, DX gateways, or PrivateLink are less scalable for any‑to‑any VPC connectivity.
- D: Peering, DX gateways, or PrivateLink are less scalable for any‑to‑any VPC connectivity.


---

---

### Question #480

A business application is hosted on Amazon EC2 and uses Amazon S3 for encrypted object storage. The chief information security ocer has directed that no application trac between the two services should traverse the public internet. Which capability should the solutions architect use to meet the compliance requirements?

- A. AWS Key Management Service (AWS KMS)

- B. VPC endpoint

- C. Private subnet

- D. Virtual private gateway

**Correct:** B
**Why:** A VPC gateway endpoint for S3 keeps traffic on the AWS network, not the public internet.

**Incorrect:**
- A: KMS is for encryption, not network path.
- C: Subnet type/VPN don’t guarantee private S3 access.
- D: Subnet type/VPN don’t guarantee private S3 access.


---

---

### Question #487

A company seeks a storage solution for its application. The solution must be highly available and scalable. The solution also must function as a file system be mountable by multiple Linux instances in AWS and on premises through native protocols, and have no minimum size requirements. The company has set up a Site-to-Site VPN for access from its on-premises network to its VPC. Which storage solution meets these requirements?

- A. Amazon FSx Multi-AZ deployments

- B. Amazon Elastic Block Store (Amazon EBS) Multi-Attach volumes

- C. Amazon Elastic File System (Amazon EFS) with multiple mount targets

- D. Amazon Elastic File System (Amazon EFS) with a single mount target and multiple access points

**Correct:** C
**Why:** EFS is a scalable, highly available NFS file system with multiple mount targets; accessible from on‑prem via VPN.

**Incorrect:**
- A: FSx options/EBS don’t match NFS multi‑attach across many clients.
- B: FSx options/EBS don’t match NFS multi‑attach across many clients.
- D: FSx options/EBS don’t match NFS multi‑attach across many clients.


---

---

### Question #497

A company has a service that reads and writes large amounts of data from an Amazon S3 bucket in the same AWS Region. The service is deployed on Amazon EC2 instances within the private subnet of a VPC. The service communicates with Amazon S3 over a NAT gateway in the public subnet. However, the company wants a solution that will reduce the data output costs. Which solution will meet these requirements MOST cost-effectively?

- A. Provision a dedicated EC2 NAT instance in the public subnet. Configure the route table for the private subnet to use the elastic network interface of this instance as the destination for all S3 trac.

- B. Provision a dedicated EC2 NAT instance in the private subnet. Configure the route table for the public subnet to use the elastic network interface of this instance as the destination for all S3 trac.

- C. Provision a VPC gateway endpoint. Configure the route table for the private subnet to use the gateway endpoint as the route for all S3 trac.

- D. Provision a second NAT gateway. Configure the route table for the private subnet to use this NAT gateway as the destination for all S3 trac.

**Correct:** C
**Why:** An S3 gateway endpoint keeps S3 traffic on the AWS network and avoids NAT gateway data processing charges.

**Incorrect:**
- A: NAT instances increase ops/cost.
- B: NAT instances increase ops/cost.
- D: A second NAT gateway increases cost.


---

## Elastic Load Balancing (ALB/NLB/GWLB)

### Question #261

A company recently announced the deployment of its retail website to a global audience. The website runs on multiple Amazon EC2 instances behind an Elastic Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The company wants to provide its customers with different versions of content based on the devices that the customers use to access the website. Which combination of actions should a solutions architect take to meet these requirements? (Choose two.)

- A. Configure Amazon CloudFront to cache multiple versions of the content.

- B. Configure a host header in a Network Load Balancer to forward trac to different instances.

- C. Configure a Lambda@Edge function to send specic objects to users based on the User-Agent header.

- D. Configure AWS Global Accelerator. Forward requests to a Network Load Balancer (NLB). Configure the NLB to set up host-based routing to different EC2 instances.

E. Configure AWS Global Accelerator. Forward requests to a Network Load Balancer (NLB). Configure the NLB to set up path-based routing to different EC2 instances.

**Correct:** A, C
**Why:** Use CloudFront for global caching and Lambda@Edge to vary content based on User‑Agent header.

**Incorrect:**
- B: NLB host/path routing or Global Accelerator aren’t needed for device‑specific content at the edge.
- D: NLB host/path routing or Global Accelerator aren’t needed for device‑specific content at the edge.
- E: NLB host/path routing or Global Accelerator aren’t needed for device‑specific content at the edge.


---

---

### Question #264

A company has a web application hosted over 10 Amazon EC2 instances with trac directed by Amazon Route 53. The company occasionally experiences a timeout error when attempting to browse the application. The networking team nds that some DNS queries return IP addresses of unhealthy instances, resulting in the timeout error. What should a solutions architect implement to overcome these timeout errors?

- A. Create a Route 53 simple routing policy record for each EC2 instance. Associate a health check with each record.

- B. Create a Route 53 failover routing policy record for each EC2 instance. Associate a health check with each record.

- C. Create an Amazon CloudFront distribution with EC2 instances as its origin. Associate a health check with the EC2 instances.

- D. Create an Application Load Balancer (ALB) with a health check in front of the EC2 instances. Route to the ALB from Route 53.

**Correct:** D
**Why:** Put an ALB with health checks in front of instances and route DNS to the ALB to avoid returning unhealthy instance IPs.

**Incorrect:**
- A: Route 53 alone still returns unhealthy A records without ALB health integration.
- B: Route 53 alone still returns unhealthy A records without ALB health integration.
- C: CloudFront is for CDN, not origin health for EC2 fleet.


---

---

### Question #265

A solutions architect needs to design a highly available application consisting of web, application, and database tiers. HTTPS content delivery should be as close to the edge as possible, with the least delivery time. Which solution meets these requirements and is MOST secure?

- A. Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in public subnets. Configure Amazon CloudFront to deliver HTTPS content using the public ALB as the origin.

- B. Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in private subnets. Configure Amazon CloudFront to deliver HTTPS content using the EC2 instances as the origin.

- C. Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in private subnets. Configure Amazon CloudFront to deliver HTTPS content using the public ALB as the origin.

- D. Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in public subnets. Configure Amazon CloudFront to deliver HTTPS content using the EC2 instances as the origin.

**Correct:** C
**Why:** Place EC2 in private subnets behind a public ALB; put CloudFront in front for HTTPS at the edge.

**Incorrect:**
- A: Either expose EC2 publicly or skip CloudFront edge TLS offload.
- B: Either expose EC2 publicly or skip CloudFront edge TLS offload.
- D: Either expose EC2 publicly or skip CloudFront edge TLS offload.


---

---

### Question #266

A company has a popular gaming platform running on AWS. The application is sensitive to latency because latency can impact the user experience and introduce unfair advantages to some players. The application is deployed in every AWS Region. It runs on Amazon EC2 instances that are part of Auto Scaling groups congured behind Application Load Balancers (ALBs). A solutions architect needs to implement a mechanism to monitor the health of the application and redirect trac to healthy endpoints. Which solution meets these requirements?

- A. Configure an accelerator in AWS Global Accelerator. Add a listener for the port that the application listens on, and attach it to a Regional endpoint in each Region. Add the ALB as the endpoint.

- B. Create an Amazon CloudFront distribution and specify the ALB as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the trac.

- C. Create an Amazon CloudFront distribution and specify Amazon S3 as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the trac.

- D. Configure an Amazon DynamoDB database to serve as the data store for the application. Create a DynamoDB Accelerator (DAX) cluster to act as the in-memory cache for DynamoDB hosting the application data.

**Correct:** A
**Why:** Global Accelerator provides health checks and intelligent routing to the nearest healthy Regional endpoint (ALB).

**Incorrect:**
- B: CloudFront/S3 or DynamoDB/DAX don’t meet UDP/TCP app health redirect needs across Regions.
- C: CloudFront/S3 or DynamoDB/DAX don’t meet UDP/TCP app health redirect needs across Regions.
- D: CloudFront/S3 or DynamoDB/DAX don’t meet UDP/TCP app health redirect needs across Regions.


---

---

### Question #268

A gaming company has a web application that displays scores. The application runs on Amazon EC2 instances behind an Application Load Balancer. The application stores data in an Amazon RDS for MySQL database. Users are starting to experience long delays and interruptions that are caused by database read performance. The company wants to improve the user experience while minimizing changes to the application’s architecture. What should a solutions architect do to meet these requirements?

- A. Use Amazon ElastiCache in front of the database.

- B. Use RDS Proxy between the application and the database.

- C. Migrate the application from EC2 instances to AWS Lambda.

- D. Migrate the database from Amazon RDS for MySQL to Amazon DynamoDB.

**Correct:** A
**Why:** Add ElastiCache in front of RDS to reduce read pressure without major architecture change.

**Incorrect:**
- B: RDS Proxy assists connections, not read latency.
- C: Lambda/DynamoDB are re‑architectures.
- D: Lambda/DynamoDB are re‑architectures.


---

---

### Question #272

A company serves a dynamic website from a eet of Amazon EC2 instances behind an Application Load Balancer (ALB). The website needs to support multiple languages to serve customers around the world. The website’s architecture is running in the us-west-1 Region and is exhibiting high request latency for users that are located in other parts of the world. The website needs to serve requests quickly and eciently regardless of a user’s location. However, the company does not want to recreate the existing architecture across multiple Regions. What should a solutions architect do to meet these requirements?

- A. Replace the existing architecture with a website that is served from an Amazon S3 bucket. Configure an Amazon CloudFront distribution with the S3 bucket as the origin. Set the cache behavior settings to cache based on the Accept-Language request header.

- B. Configure an Amazon CloudFront distribution with the ALB as the origin. Set the cache behavior settings to cache based on the Accept- Language request header.

- C. Create an Amazon API Gateway API that is integrated with the ALB. Configure the API to use the HTTP integration type. Set up an API Gateway stage to enable the API cache based on the Accept-Language request header.

- D. Launch an EC2 instance in each additional Region and configure NGINX to act as a cache server for that Region. Put all the EC2 instances and the ALB behind an Amazon Route 53 record set with a geolocation routing policy.

**Correct:** B
**Why:** CloudFront with ALB origin and cache based on Accept‑Language provides low latency globally without multi‑Region.

**Incorrect:**
- A: S3 static site or regional NGINX cache adds complexity or doesn’t fit dynamic origin.
- C: S3 static site or regional NGINX cache adds complexity or doesn’t fit dynamic origin.
- D: S3 static site or regional NGINX cache adds complexity or doesn’t fit dynamic origin.


---

---

### Question #275

A company runs an internal browser-based application. The application runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. The Auto Scaling group scales up to 20 instances during work hours, but scales down to 2 instances overnight. Staff are complaining that the application is very slow when the day begins, although it runs well by mid-morning. How should the scaling be changed to address the staff complaints and keep costs to a minimum?

- A. Implement a scheduled action that sets the desired capacity to 20 shortly before the oce opens.

- B. Implement a step scaling action triggered at a lower CPU threshold, and decrease the cooldown period.

- C. Implement a target tracking action triggered at a lower CPU threshold, and decrease the cooldown period.

- D. Implement a scheduled action that sets the minimum and maximum capacity to 20 shortly before the oce opens.

**Correct:** A
**Why:** Pre‑scale via scheduled action to desired capacity before office hours to avoid cold starts while minimizing costs.

**Incorrect:**
- B: Threshold policies or setting min/max to 20 waste cost or react too late.
- C: Threshold policies or setting min/max to 20 waste cost or react too late.
- D: Threshold policies or setting min/max to 20 waste cost or react too late.


---

---

### Question #282

A company runs a web application that is deployed on Amazon EC2 instances in the private subnet of a VPC. An Application Load Balancer (ALB) that extends across the public subnets directs web trac to the EC2 instances. The company wants to implement new security measures to restrict inbound trac from the ALB to the EC2 instances while preventing access from any other source inside or outside the private subnet of the EC2 instances. Which solution will meet these requirements?

- A. Configure a route in a route table to direct trac from the internet to the private IP addresses of the EC2 instances.

- B. Configure the security group for the EC2 instances to only allow trac that comes from the security group for the ALB.

- C. Move the EC2 instances into the public subnet. Give the EC2 instances a set of Elastic IP addresses.

- D. Configure the security group for the ALB to allow any TCP trac on any port.

**Correct:** B
**Why:** Allow only the ALB security group as a source to the EC2 instances’ security group.

**Incorrect:**
- A: Route table or broad ALB SG rules do not enforce least privilege inbound to EC2.
- C: Route table or broad ALB SG rules do not enforce least privilege inbound to EC2.
- D: Route table or broad ALB SG rules do not enforce least privilege inbound to EC2.


---

---

### Question #286

A company has a static website that is hosted on Amazon CloudFront in front of Amazon S3. The static website uses a database backend. The company notices that the website does not reect updates that have been made in the website’s Git repository. The company checks the continuous integration and continuous delivery (CI/CD) pipeline between the Git repository and Amazon S3. The company veries that the webhooks are congured properly and that the CI/CD pipeline is sending messages that indicate successful deployments. A solutions architect needs to implement a solution that displays the updates on the website. Which solution will meet these requirements?

- A. Add an Application Load Balancer.

- B. Add Amazon ElastiCache for Redis or Memcached to the database layer of the web application.

- C. Invalidate the CloudFront cache.

- D. Use AWS Certicate Manager (ACM) to validate the website’s SSL certicate.

**Correct:** C
**Why:** Invalidate the CloudFront cache so the CDN serves the latest content from S3.

**Incorrect:**
- A: ALB/ElastiCache/ACM do not address CDN cache staleness.
- B: ALB/ElastiCache/ACM do not address CDN cache staleness.
- D: ALB/ElastiCache/ACM do not address CDN cache staleness.


---

---

### Question #297

A company deploys an application on ve Amazon EC2 instances. An Application Load Balancer (ALB) distributes trac to the instances by using a target group. The average CPU usage on each of the instances is below 10% most of the time, with occasional surges to 65%. A solutions architect needs to implement a solution to automate the scalability of the application. The solution must optimize the cost of the architecture and must ensure that the application has enough CPU resources when surges occur. Which solution will meet these requirements?

- A. Create an Amazon CloudWatch alarm that enters the ALARM state when the CPUUtilization metric is less than 20%. Create an AWS Lambda function that the CloudWatch alarm invokes to terminate one of the EC2 instances in the ALB target group.

- B. Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing target group as the target group. Set a target tracking scaling policy that is based on the ASGAverageCPUUtilization metric. Set the minimum instances to 2, the desired capacity to 3, the maximum instances to 6, and the target value to 50%. Add the EC2 instances to the Auto Scaling group.

- C. Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing target group as the target group. Set the minimum instances to 2, the desired capacity to 3, and the maximum instances to 6. Add the EC2 instances to the Auto Scaling group.

- D. Create two Amazon CloudWatch alarms. Configure the first CloudWatch alarm to enter the ALARM state when the average CPUUtilization metric is below 20%. Configure the second CloudWatch alarm to enter the ALARM state when the average CPUUtilization matric is above 50%. Configure the alarms to publish to an Amazon Simple Notification Service (Amazon SNS) topic to send an email message. After receiving the message, log in to decrease or increase the number of EC2 instances that are running.

**Correct:** B
**Why:** Create an Auto Scaling group with target tracking on CPU and attach to existing ALB with appropriate min/desired/max.

**Incorrect:**
- A: Manual alarms/emails are not automation.
- C: Missing scaling policy.
- D: Manual alarms/emails are not automation.


---

---

### Question #298

A company is running a critical business application on Amazon EC2 instances behind an Application Load Balancer. The EC2 instances run in an Auto Scaling group and access an Amazon RDS DB instance. The design did not pass an operational review because the EC2 instances and the DB instance are all located in a single Availability Zone. A solutions architect must update the design to use a second Availability Zone. Which solution will make the application highly available?

- A. Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance with connections to each network.

- B. Provision two subnets that extend across both Availability Zones. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance with connections to each network.

- C. Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance for Multi-AZ deployment.

- D. Provision a subnet that extends across both Availability Zones. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance for Multi-AZ deployment.

**Correct:** C
**Why:** Distribute EC2 across AZs via ASG and enable RDS Multi‑AZ for database HA.

**Incorrect:**
- A: Incorrect subnet constructs or missing Multi‑AZ DB.
- B: Incorrect subnet constructs or missing Multi‑AZ DB.
- D: Incorrect subnet constructs or missing Multi‑AZ DB.


---

---

### Question #310

A company sells datasets to customers who do research in articial intelligence and machine learning (AI/ML). The datasets are large, formatted files that are stored in an Amazon S3 bucket in the us-east-1 Region. The company hosts a web application that the customers use to purchase access to a given dataset. The web application is deployed on multiple Amazon EC2 instances behind an Application Load Balancer. After a purchase is made, customers receive an S3 signed URL that allows access to the files. The customers are distributed across North America and Europe. The company wants to reduce the cost that is associated with data transfers and wants to maintain or improve performance. What should a solutions architect do to meet these requirements?

- A. Configure S3 Transfer Acceleration on the existing S3 bucket. Direct customer requests to the S3 Transfer Acceleration endpoint. Continue to use S3 signed URLs for access control.

- B. Deploy an Amazon CloudFront distribution with the existing S3 bucket as the origin. Direct customer requests to the CloudFront URL. Switch to CloudFront signed URLs for access control.

- C. Set up a second S3 bucket in the eu-central-1 Region with S3 Cross-Region Replication between the buckets. Direct customer requests to the closest Region. Continue to use S3 signed URLs for access control.

- D. Modify the web application to enable streaming of the datasets to end users. Configure the web application to read the data from the existing S3 bucket. Implement access control directly in the application.

**Correct:** B
**Why:** CloudFront in front of S3 reduces egress cost (regional edge caches/edge), improves performance; use signed URLs for access control.

**Incorrect:**
- A: Transfer Acceleration/CRR/streaming app changes are less optimal or higher ops.
- C: Transfer Acceleration/CRR/streaming app changes are less optimal or higher ops.
- D: Transfer Acceleration/CRR/streaming app changes are less optimal or higher ops.


---

---

### Question #327

A solutions architect must secure a VPC network that hosts Amazon EC2 instances. The EC2 instances contain highly sensitive data and run in a private subnet. According to company policy, the EC2 instances that run in the VPC can access only approved third-party software repositories on the internet for software product updates that use the third party’s URL. Other internet trac must be blocked. Which solution meets these requirements?

- A. Update the route table for the private subnet to route the outbound trac to an AWS Network Firewall firewall. Configure domain list rule groups.

- B. Set up an AWS WAF web ACL. Create a custom set of rules that filter trac requests based on source and destination IP address range sets.

- C. Implement strict inbound security group rules. Configure an outbound rule that allows trac only to the authorized software repositories on the internet by specifying the URLs.

- D. Configure an Application Load Balancer (ALB) in front of the EC2 instances. Direct all outbound trac to the ALB. Use a URL-based rule listener in the ALB’s target group for outbound access to the internet.

**Correct:** A
**Why:** AWS Network Firewall with domain list rules provides egress filtering to approved repositories from private subnets.

**Incorrect:**
- B: WAF is for HTTP(S) to apps; SGs cannot filter by domain/URL; ALB is inbound.
- C: WAF is for HTTP(S) to apps; SGs cannot filter by domain/URL; ALB is inbound.
- D: WAF is for HTTP(S) to apps; SGs cannot filter by domain/URL; ALB is inbound.


---

---

### Question #328

A company is hosting a three-tier ecommerce application in the AWS Cloud. The company hosts the website on Amazon S3 and integrates the website with an API that handles sales requests. The company hosts the API on three Amazon EC2 instances behind an Application Load Balancer (ALB). The API consists of static and dynamic front-end content along with backend workers that process sales requests asynchronously. The company is expecting a signicant and sudden increase in the number of sales requests during events for the launch of new products. What should a solutions architect recommend to ensure that all the requests are processed successfully?

- A. Add an Amazon CloudFront distribution for the dynamic content. Increase the number of EC2 instances to handle the increase in trac.

- B. Add an Amazon CloudFront distribution for the static content. Place the EC2 instances in an Auto Scaling group to launch new instances based on network trac.

- C. Add an Amazon CloudFront distribution for the dynamic content. Add an Amazon ElastiCache instance in front of the ALB to reduce trac for the API to handle.

- D. Add an Amazon CloudFront distribution for the static content. Add an Amazon Simple Queue Service (Amazon SQS) queue to receive requests from the website for later processing by the EC2 instances.

**Correct:** D
**Why:** Cache static content with CloudFront and use SQS to buffer backend requests to ensure all are processed.

**Incorrect:**
- A: Dynamic CloudFront or ElastiCache alone won’t handle sudden async workload spikes.
- B: Without queueing, backend can still be overwhelmed.
- C: Dynamic CloudFront or ElastiCache alone won’t handle sudden async workload spikes.


---

---

### Question #333

A company’s application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. On the first day of every month at midnight, the application becomes much slower when the month-end nancial calculation batch runs. This causes the CPU utilization of the EC2 instances to immediately peak to 100%, which disrupts the application. What should a solutions architect recommend to ensure the application is able to handle the workload and avoid downtime?

- A. Configure an Amazon CloudFront distribution in front of the ALB.

- B. Configure an EC2 Auto Scaling simple scaling policy based on CPU utilization.

- C. Configure an EC2 Auto Scaling scheduled scaling policy based on the monthly schedule.

- D. Configure Amazon ElastiCache to remove some of the workload from the EC2 instances.

**Correct:** C
**Why:** Scheduled scaling handles known monthly spikes at midnight without downtime.

**Incorrect:**
- A: CDN or reactive scaling alone may be too late; caching doesn’t fix CPU‑bound computation.
- B: CDN or reactive scaling alone may be too late; caching doesn’t fix CPU‑bound computation.
- D: CDN or reactive scaling alone may be too late; caching doesn’t fix CPU‑bound computation.


---

---

### Question #340

A media company hosts its website on AWS. The website application’s architecture includes a eet of Amazon EC2 instances behind an Application Load Balancer (ALB) and a database that is hosted on Amazon Aurora. The company’s cybersecurity team reports that the application is vulnerable to SQL injection. How should the company resolve this issue?

- A. Use AWS WAF in front of the ALB. Associate the appropriate web ACLs with AWS WAF.

- B. Create an ALB listener rule to reply to SQL injections with a xed response.

- C. Subscribe to AWS Shield Advanced to block all SQL injection attempts automatically.

- D. Set up Amazon Inspector to block all SQL injection attempts automatically.

**Correct:** A
**Why:** Put WAF in front of ALB and enable managed rules for SQLi to mitigate vulnerabilities quickly.

**Incorrect:**
- B: ALB listener responses or Shield/Inspector don’t address SQLi at the app layer.
- C: ALB listener responses or Shield/Inspector don’t address SQLi at the app layer.
- D: ALB listener responses or Shield/Inspector don’t address SQLi at the app layer.


---

---

### Question #345

A company wants to restrict access to the content of one of its main web applications and to protect the content by using authorization techniques available on AWS. The company wants to implement a serverless architecture and an authentication solution for fewer than 100 users. The solution needs to integrate with the main web application and serve web content globally. The solution must also scale as the company's user base grows while providing the lowest login latency possible. Which solution will meet these requirements MOST cost-effectively?

- A. Use Amazon Cognito for authentication. Use Lambda@Edge for authorization. Use Amazon CloudFront to serve the web application globally.

- B. Use AWS Directory Service for Microsoft Active Directory for authentication. Use AWS Lambda for authorization. Use an Application Load Balancer to serve the web application globally.

- C. Use Amazon Cognito for authentication. Use AWS Lambda for authorization. Use Amazon S3 Transfer Acceleration to serve the web application globally.

- D. Use AWS Directory Service for Microsoft Active Directory for authentication. Use Lambda@Edge for authorization. Use AWS Elastic Beanstalk to serve the web application globally.

**Correct:** A
**Why:** Cognito for auth, Lambda@Edge for authorization, and CloudFront for global delivery provide a serverless, low‑latency solution.

**Incorrect:**
- B: Directory Service and Beanstalk/ALB add ops;
- C: S3 Transfer Acceleration isn’t for auth.
- D: Directory Service and Beanstalk/ALB add ops;


---

---

### Question #357

A gaming company is moving its public scoreboard from a data center to the AWS Cloud. The company uses Amazon EC2 Windows Server instances behind an Application Load Balancer to host its dynamic application. The company needs a highly available storage solution for the application. The application consists of static files and dynamic server-side code. Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)

- A. Store the static files on Amazon S3. Use Amazon CloudFront to cache objects at the edge.

- B. Store the static files on Amazon S3. Use Amazon ElastiCache to cache objects at the edge.

- C. Store the server-side code on Amazon Elastic File System (Amazon EFS). Mount the EFS volume on each EC2 instance to share the files.

- D. Store the server-side code on Amazon FSx for Windows File Server. Mount the FSx for Windows File Server volume on each EC2 instance to share the files.

E. Store the server-side code on a General Purpose SSD (gp2) Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume on each EC2 instance to share the files.

**Correct:** A, D
**Why:** Store static files in S3 and cache with CloudFront; use FSx for Windows File Server to share server‑side code across Windows EC2 instances.

**Incorrect:**
- B: ElastiCache is not an edge CDN for static files.
- C: EFS is POSIX/NFS (Linux), not ideal for Windows.
- E: EBS volumes are single‑instance; not shareable across instances.


---

---

### Question #358

A social media company runs its application on Amazon EC2 instances behind an Application Load Balancer (ALB). The ALB is the origin for an Amazon CloudFront distribution. The application has more than a billion images stored in an Amazon S3 bucket and processes thousands of images each second. The company wants to resize the images dynamically and serve appropriate formats to clients. Which solution will meet these requirements with the LEAST operational overhead?

- A. Install an external image management library on an EC2 instance. Use the image management library to process the images.

- B. Create a CloudFront origin request policy. Use the policy to automatically resize images and to serve the appropriate format based on the User-Agent HTTP header in the request.

- C. Use a Lambda@Edge function with an external image management library. Associate the Lambda@Edge function with the CloudFront behaviors that serve the images.

- D. Create a CloudFront response headers policy. Use the policy to automatically resize images and to serve the appropriate format based on the User-Agent HTTP header in the request.

**Correct:** C
**Why:** Lambda@Edge can dynamically transform/resize images at the edge with minimal ops and latency.

**Incorrect:**
- A: Managing EC2 image service increases ops and scale burden.
- B: CloudFront policies/headers cannot perform image processing by themselves.
- D: CloudFront policies/headers cannot perform image processing by themselves.


---

---

### Question #367

A company is using Amazon Route 53 latency-based routing to route requests to its UDP-based application for users around the world. The application is hosted on redundant servers in the company's on-premises data centers in the United States, Asia, and Europe. The company’s compliance requirements state that the application must be hosted on premises. The company wants to improve the performance and availability of the application. What should a solutions architect do to meet these requirements?

- A. Configure three Network Load Balancers (NLBs) in the three AWS Regions to address the on-premises endpoints. Create an accelerator by using AWS Global Accelerator, and register the NLBs as its endpoints. Provide access to the application by using a CNAME that points to the accelerator DNS.

- B. Configure three Application Load Balancers (ALBs) in the three AWS Regions to address the on-premises endpoints. Create an accelerator by using AWS Global Accelerator, and register the ALBs as its endpoints. Provide access to the application by using a CNAME that points to the accelerator DNS.

- C. Configure three Network Load Balancers (NLBs) in the three AWS Regions to address the on-premises endpoints. In Route 53, create a latency-based record that points to the three NLBs, and use it as an origin for an Amazon CloudFront distribution. Provide access to the application by using a CNAME that points to the CloudFront DNS.

- D. Configure three Application Load Balancers (ALBs) in the three AWS Regions to address the on-premises endpoints. In Route 53, create a latency-based record that points to the three ALBs, and use it as an origin for an Amazon CloudFront distribution. Provide access to the application by using a CNAME that points to the CloudFront DNS.

**Correct:** A
**Why:** Place NLBs in three Regions that point to on‑prem endpoints; use Global Accelerator to improve performance/availability for UDP traffic while keeping hosting on‑prem.

**Incorrect:**
- B: ALB does not support UDP.
- C: CloudFront is HTTP/HTTPS and not for UDP; also adds unnecessary layers.
- D: CloudFront is HTTP/HTTPS and not for UDP; also adds unnecessary layers.


---

---

### Question #378

A company is developing a real-time multiplayer game that uses UDP for communications between the client and servers in an Auto Scaling group. Spikes in demand are anticipated during the day, so the game server platform must adapt accordingly. Developers want to store gamer scores and other non-relational data in a database solution that will scale without intervention. Which solution should a solutions architect recommend?

- A. Use Amazon Route 53 for trac distribution and Amazon Aurora Serverless for data storage.

- B. Use a Network Load Balancer for trac distribution and Amazon DynamoDB on-demand for data storage.

- C. Use a Network Load Balancer for trac distribution and Amazon Aurora Global Database for data storage.

- D. Use an Application Load Balancer for trac distribution and Amazon DynamoDB global tables for data storage.

**Correct:** B
**Why:** NLB supports UDP for game traffic; DynamoDB on‑demand scales automatically for storing scores/non‑relational data.

**Incorrect:**
- A: Aurora is relational and requires capacity planning.
- C: Aurora is relational and requires capacity planning.
- D: ALB does not support UDP.


---

---

### Question #382

A company has a three-tier application on AWS that ingests sensor data from its users’ devices. The trac ows through a Network Load Balancer (NLB), then to Amazon EC2 instances for the web tier, and nally to EC2 instances for the application tier. The application tier makes calls to a database. What should a solutions architect do to improve the security of the data in transit?

- A. Configure a TLS listener. Deploy the server certicate on the NLB.

- B. Configure AWS Shield Advanced. Enable AWS WAF on the NLB.

- C. Change the load balancer to an Application Load Balancer (ALB). Enable AWS WAF on the ALB.

- D. Encrypt the Amazon Elastic Block Store (Amazon EBS) volume on the EC2 instances by using AWS Key Management Service (AWS KMS).

**Correct:** A
**Why:** Add a TLS listener and deploy certs on the NLB to encrypt data in transit from clients to the load balancer and to targets as needed.

**Incorrect:**
- B: Shield/WAF protect against attacks but don’t encrypt traffic.
- C: ALB is fine for HTTP/HTTPS but switching is unnecessary; TLS on NLB meets the need.
- D: EBS encryption is at rest, not in transit.


---

---

### Question #385

A solutions architect is creating a new VPC design. There are two public subnets for the load balancer, two private subnets for web servers, and two private subnets for MySQL. The web servers use only HTTPS. The solutions architect has already created a security group for the load balancer allowing port 443 from 0.0.0.0/0. Company policy requires that each resource has the least access required to still be able to perform its tasks. Which additional configuration strategy should the solutions architect use to meet these requirements?

- A. Create a security group for the web servers and allow port 443 from 0.0.0.0/0. Create a security group for the MySQL servers and allow port 3306 from the web servers security group.

- B. Create a network ACL for the web servers and allow port 443 from 0.0.0.0/0. Create a network ACL for the MySQL servers and allow port 3306 from the web servers security group.

- C. Create a security group for the web servers and allow port 443 from the load balancer. Create a security group for the MySQL servers and allow port 3306 from the web servers security group.

- D. Create a network ACL for the web servers and allow port 443 from the load balancer. Create a network ACL for the MySQL servers and allow port 3306 from the web servers security group.

**Correct:** C
**Why:** Least privilege: Web SG allows 443 only from LB SG; DB SG allows 3306 only from Web SG.

**Incorrect:**
- A: Allowing from 0.0.0.0/0 or using NACLs is broader than necessary.
- B: Allowing from 0.0.0.0/0 or using NACLs is broader than necessary.
- D: Allowing from 0.0.0.0/0 or using NACLs is broader than necessary.


---

---

### Question #389

A company has a large dataset for its online advertising business stored in an Amazon RDS for MySQL DB instance in a single Availability Zone. The company wants business reporting queries to run without impacting the write operations to the production DB instance. Which solution meets these requirements?

- A. Deploy RDS read replicas to process the business reporting queries.

- B. Scale out the DB instance horizontally by placing it behind an Elastic Load Balancer.

- C. Scale up the DB instance to a larger instance type to handle write operations and queries.

- D. Deploy the DB instance in multiple Availability Zones to process the business reporting queries.

**Correct:** A
**Why:** RDS read replicas offload read/reporting queries from the writer to avoid impacting write performance.

**Incorrect:**
- B: RDS isn’t load balanced like this; ELB is not for databases.
- C: Scaling up increases costs and may still impact writes.
- D: Multi‑AZ improves HA, not read scaling.


---

---

### Question #390

A company hosts a three-tier ecommerce application on a eet of Amazon EC2 instances. The instances run in an Auto Scaling group behind an Application Load Balancer (ALB). All ecommerce data is stored in an Amazon RDS for MariaDB Multi-AZ DB instance. The company wants to optimize customer session management during transactions. The application must store session data durably. Which solutions will meet these requirements? (Choose two.)

- A. Turn on the sticky sessions feature (session anity) on the ALB.

- B. Use an Amazon DynamoDB table to store customer session information.

- C. Deploy an Amazon Cognito user pool to manage user session information.

- D. Deploy an Amazon ElastiCache for Redis cluster to store customer session information.

E. Use AWS Systems Manager Application Manager in the application to manage user session information.

**Correct:** A, B
**Why:** Sticky sessions reduce cross‑instance churn during transactions; store session data durably in DynamoDB to survive instance failure.

**Incorrect:**
- C: Cognito manages auth, not app transaction sessions.
- D: ElastiCache Redis alone is not durable.
- E: Systems Manager Application Manager is not for session storage.


---

---

### Question #405

A solutions architect is designing the architecture for a software demonstration environment. The environment will run on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). The system will experience signicant increases in trac during working hours but is not required to operate on weekends. Which combination of actions should the solutions architect take to ensure that the system can scale to meet demand? (Choose two.)

- A. Use AWS Auto Scaling to adjust the ALB capacity based on request rate.

- B. Use AWS Auto Scaling to scale the capacity of the VPC internet gateway.

- C. Launch the EC2 instances in multiple AWS Regions to distribute the load across Regions.

- D. Use a target tracking scaling policy to scale the Auto Scaling group based on instance CPU utilization.

E. Use scheduled scaling to change the Auto Scaling group minimum, maximum, and desired capacity to zero for weekends. Revert to the default values at the start of the week.

**Correct:** D, E
**Why:** Use target tracking for dynamic scaling and scheduled scaling to zero on weekends to save cost.

**Incorrect:**
- A: ALB and internet gateways are managed; you don’t scale them.
- B: ALB and internet gateways are managed; you don’t scale them.
- C: Multi-Region is unnecessary for this need.


---

---

### Question #408

A company runs an application that receives data from thousands of geographically dispersed remote devices that use UDP. The application processes the data immediately and sends a message back to the device if necessary. No data is stored. The company needs a solution that minimizes latency for the data transmission from the devices. The solution also must provide rapid failover to another AWS Region. Which solution will meet these requirements?

- A. Configure an Amazon Route 53 failover routing policy. Create a Network Load Balancer (NLB) in each of the two Regions. Configure the NLB to invoke an AWS Lambda function to process the data.

- B. Use AWS Global Accelerator. Create a Network Load Balancer (NLB) in each of the two Regions as an endpoint. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the NLProcess the data in Amazon ECS.

- C. Use AWS Global Accelerator. Create an Application Load Balancer (ALB) in each of the two Regions as an endpoint. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the ALB. Process the data in Amazon ECS.

- D. Configure an Amazon Route 53 failover routing policy. Create an Application Load Balancer (ALB) in each of the two Regions. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the ALB. Process the data in Amazon ECS.

**Correct:** B
**Why:** Global Accelerator reduces latency globally and provides fast failover; NLB supports UDP; ECS Fargate scales to process data.

**Incorrect:**
- A: Route 53 failover alone doesn’t optimize latency.
- C: ALB does not support UDP.
- D: Same as A plus ALB (no UDP).


---

---

### Question #409

A solutions architect must migrate a Windows Internet Information Services (IIS) web application to AWS. The application currently relies on a file share hosted in the user's on-premises network-attached storage (NAS). The solutions architect has proposed migrating the IIS web servers to Amazon EC2 instances in multiple Availability Zones that are connected to the storage solution, and conguring an Elastic Load Balancer attached to the instances. Which replacement to the on-premises file share is MOST resilient and durable?

- A. Migrate the file share to Amazon RDS.

- B. Migrate the file share to AWS Storage Gateway.

- C. Migrate the file share to Amazon FSx for Windows File Server.

- D. Migrate the file share to Amazon Elastic File System (Amazon EFS).

**Correct:** C
**Why:** FSx for Windows File Server provides fully managed SMB shares for Windows IIS with HA/durability.

**Incorrect:**
- A: RDS is a database, not a file share.
- B: Storage Gateway is for hybrid access, not the most resilient/durable for primary shared storage.
- D: EFS is NFS (Linux), not SMB.


---

---

### Question #422

A company is developing a new machine learning (ML) model solution on AWS. The models are developed as independent microservices that fetch approximately 1 GB of model data from Amazon S3 at startup and load the data into memory. Users access the models through an asynchronous API. Users can send a request or a batch of requests and specify where the results should be sent. The company provides models to hundreds of users. The usage patterns for the models are irregular. Some models could be unused for days or weeks. Other models could receive batches of thousands of requests at a time. Which design should a solutions architect recommend to meet these requirements?

- A. Direct the requests from the API to a Network Load Balancer (NLB). Deploy the models as AWS Lambda functions that are invoked by the NLB.

- B. Direct the requests from the API to an Application Load Balancer (ALB). Deploy the models as Amazon Elastic Container Service (Amazon ECS) services that read from an Amazon Simple Queue Service (Amazon SQS) queue. Use AWS App Mesh to scale the instances of the ECS cluster based on the SQS queue size.

- C. Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue. Deploy the models as AWS Lambda functions that are invoked by SQS events. Use AWS Auto Scaling to increase the number of vCPUs for the Lambda functions based on the SQS queue size.

- D. Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue. Deploy the models as Amazon Elastic Container Service (Amazon ECS) services that read from the queue. Enable AWS Auto Scaling on Amazon ECS for both the cluster and copies of the service based on the queue size.

**Correct:** D
**Why:** Queue requests in SQS and run models on ECS services that scale on queue depth; suitable for 1 GB model loads and bursty use.

**Incorrect:**
- A: NLB→Lambda is not a pattern; Lambda cold starts with 1 GB model pulls are costly.
- B: ALB→ECS without decoupling risks backpressure; App Mesh is unnecessary.
- C: Lambda with 1 GB model loads and vCPU scaling is not appropriate.


---

---

### Question #434

A company hosts its application in the AWS Cloud. The application runs on Amazon EC2 instances behind an Elastic Load Balancer in an Auto Scaling group and with an Amazon DynamoDB table. The company wants to ensure the application can be made available in anotherAWS Region with minimal downtime. What should a solutions architect do to meet these requirements with the LEAST amount of downtime?

- A. Create an Auto Scaling group and a load balancer in the disaster recovery Region. Configure the DynamoDB table as a global table. Configure DNS failover to point to the new disaster recovery Region's load balancer.

- B. Create an AWS CloudFormation template to create EC2 instances, load balancers, and DynamoDB tables to be launched when needed Configure DNS failover to point to the new disaster recovery Region's load balancer.

- C. Create an AWS CloudFormation template to create EC2 instances and a load balancer to be launched when needed. Configure the DynamoDB table as a global table. Configure DNS failover to point to the new disaster recovery Region's load balancer.

- D. Create an Auto Scaling group and load balancer in the disaster recovery Region. Configure the DynamoDB table as a global table. Create an Amazon CloudWatch alarm to trigger an AWS Lambda function that updates Amazon Route 53 pointing to the disaster recovery load balancer.

**Correct:** A
**Why:** Pre‑provision ALB/ASG in DR Region and use DynamoDB global tables; DNS failover minimizes downtime.

**Incorrect:**
- B: Delayed provisioning increases downtime.
- C: Delayed provisioning increases downtime.
- D: Extra Lambda automation is unnecessary.


---

---

### Question #437

A company operates an ecommerce website on Amazon EC2 instances behind an Application Load Balancer (ALB) in an Auto Scaling group. The site is experiencing performance issues related to a high request rate from illegitimate external systems with changing IP addresses. The security team is worried about potential DDoS attacks against the website. The company must block the illegitimate incoming requests in a way that has a minimal impact on legitimate users. What should a solutions architect recommend?

- A. Deploy Amazon Inspector and associate it with the ALB.

- B. Deploy AWS WAF, associate it with the ALB, and configure a rate-limiting rule.

- C. Deploy rules to the network ACLs associated with the ALB to block the incomingtrac.

- D. Deploy Amazon GuardDuty and enable rate-limiting protection when conguring GuardDuty.

**Correct:** B
**Why:** Attach AWS WAF to the ALB and configure rate‑based rules to throttle illegitimate request floods while allowing legitimate users.

**Incorrect:**
- A: Inspector/GuardDuty do not mitigate at the edge.
- C: NACLs are coarse and static; IPs change.
- D: Inspector/GuardDuty do not mitigate at the edge.


---

---

### Question #441

A company hosts a multi-tier web application on Amazon Linux Amazon EC2 instances behind an Application Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The company observes that the Auto Scaling group launches more On-Demand Instances when the application's end users access high volumes of static web content. The company wants to optimize cost. What should a solutions architect do to redesign the application MOST cost-effectively?

- A. Update the Auto Scaling group to use Reserved Instances instead of On-Demand Instances.

- B. Update the Auto Scaling group to scale by launching Spot Instances instead of On-Demand Instances.

- C. Create an Amazon CloudFront distribution to host the static web contents from an Amazon S3 bucket.

- D. Create an AWS Lambda function behind an Amazon API Gateway API to host the static website contents.

**Correct:** C
**Why:** Serve static content from S3 via CloudFront to reduce load on EC2/ALB and lower cost.

**Incorrect:**
- A: Instance pricing choices don’t remove static load driver.
- B: Instance pricing choices don’t remove static load driver.
- D: Lambda+API Gateway is unnecessary for static hosting.


---

---

### Question #444

A company has hired a solutions architect to design a reliable architecture for its application. The application consists of one Amazon RDS DB instance and two manually provisioned Amazon EC2 instances that run web servers. The EC2 instances are located in a single Availability Zone. An employee recently deleted the DB instance, and the application was unavailable for 24 hours as a result. The company is concerned with the overall reliability of its environment. What should the solutions architect do to maximize reliability of the application's infrastructure?

- A. Delete one EC2 instance and enable termination protection on the other EC2 instance. Update the DB instance to be Multi-AZ, and enable deletion protection.

- B. Update the DB instance to be Multi-AZ, and enable deletion protection. Place the EC2 instances behind an Application Load Balancer, and run them in an EC2 Auto Scaling group across multiple Availability Zones.

- C. Create an additional DB instance along with an Amazon API Gateway and an AWS Lambda function. Configure the application to invoke the Lambda function through API Gateway. Have the Lambda function write the data to the two DB instances.

- D. Place the EC2 instances in an EC2 Auto Scaling group that has multiple subnets located in multiple Availability Zones. Use Spot Instances instead of On-Demand Instances. Set up Amazon CloudWatch alarms to monitor the health of the instances Update the DB instance to be Multi-AZ, and enable deletion protection.

**Correct:** B
**Why:** Protect RDS with Multi‑AZ and deletion protection; place EC2 in ALB+Auto Scaling across AZs to maximize reliability.

**Incorrect:**
- A: Either reduce availability or add unnecessary components/cost.
- C: Either reduce availability or add unnecessary components/cost.
- D: Either reduce availability or add unnecessary components/cost.


---

---

### Question #447

A company has a stateless web application that runs on AWS Lambda functions that are invoked by Amazon API Gateway. The company wants to deploy the application across multiple AWS Regions to provide Regional failover capabilities. What should a solutions architect do to route trac to multiple Regions?

- A. Create Amazon Route 53 health checks for each Region. Use an active-active failover configuration.

- B. Create an Amazon CloudFront distribution with an origin for each Region. Use CloudFront health checks to route trac.

- C. Create a transit gateway. Attach the transit gateway to the API Gateway endpoint in each Region. Configure the transit gateway to route requests.

- D. Create an Application Load Balancer in the primary Region. Set the target group to point to the API Gateway endpoint hostnames in each Region.

**Correct:** A
**Why:** Use Route 53 health checks with active‑active routing across Regions for serverless endpoints.

**Incorrect:**
- B: CloudFront is for HTTP caching, not API multi‑Region routing.
- C: Transit gateway/ALB cannot directly route to API Gateway across Regions as required.
- D: Transit gateway/ALB cannot directly route to API Gateway across Regions as required.


---

---

### Question #450

A company has a three-tier web application that is in a single server. The company wants to migrate the application to the AWS Cloud. The company also wants the application to align with the AWS Well-Architected Framework and to be consistent with AWS recommended best practices for security, scalability, and resiliency. Which combination of solutions will meet these requirements? (Choose three.)

- A. Create a VPC across two Availability Zones with the application's existing architecture. Host the application with existing architecture on an Amazon EC2 instance in a private subnet in each Availability Zone with EC2 Auto Scaling groups. Secure the EC2 instance with security groups and network access control lists (network ACLs).

- B. Set up security groups and network access control lists (network ACLs) to control access to the database layer. Set up a single Amazon RDS database in a private subnet.

- C. Create a VPC across two Availability Zones. Refactor the application to host the web tier, application tier, and database tier. Host each tier on its own private subnet with Auto Scaling groups for the web tier and application tier.

- D. Use a single Amazon RDS database. Allow database access only from the application tier security group.

E. Use Elastic Load Balancers in front of the web tier. Control access by using security groups containing references to each layer's security groups.

F. Use an Amazon RDS database Multi-AZ cluster deployment in private subnets. Allow database access only from application tier security groups.

**Correct:** C, E, F
**Why:** Refactor into three tiers across two AZs; use ELBs for the web tier; and use RDS Multi‑AZ in private subnets with SGs restricting access from the app tier.

**Incorrect:**
- A: Single RDS or single‑AZ design and flat architectures don’t meet best practices for resiliency/security.
- B: Single RDS or single‑AZ design and flat architectures don’t meet best practices for resiliency/security.
- D: Single RDS or single‑AZ design and flat architectures don’t meet best practices for resiliency/security.


---

---

### Question #461

A company is developing a mobile gaming app in a single AWS Region. The app runs on multiple Amazon EC2 instances in an Auto Scaling group. The company stores the app data in Amazon DynamoDB. The app communicates by using TCP trac and UDP trac between the users and the servers. The application will be used globally. The company wants to ensure the lowest possible latency for all users. Which solution will meet these requirements?

- A. Use AWS Global Accelerator to create an accelerator. Create an Application Load Balancer (ALB) behind an accelerator endpoint that uses Global Accelerator integration and listening on the TCP and UDP ports. Update the Auto Scaling group to register instances on the ALB.

- B. Use AWS Global Accelerator to create an accelerator. Create a Network Load Balancer (NLB) behind an accelerator endpoint that uses Global Accelerator integration and listening on the TCP and UDP ports. Update the Auto Scaling group to register instances on the NLB.

- C. Create an Amazon CloudFront content delivery network (CDN) endpoint. Create a Network Load Balancer (NLB) behind the endpoint and listening on the TCP and UDP ports. Update the Auto Scaling group to register instances on the NLB. Update CloudFront to use the NLB as the origin.

- D. Create an Amazon CloudFront content delivery network (CDN) endpoint. Create an Application Load Balancer (ALB) behind the endpoint and listening on the TCP and UDP ports. Update the Auto Scaling group to register instances on the ALB. Update CloudFront to use the ALB as the origin.

**Correct:** B
**Why:** Global Accelerator with an NLB supports both TCP and UDP and provides the lowest latency globally via the AWS edge.

**Incorrect:**
- A: ALB doesn’t support UDP; CloudFront is not for arbitrary TCP/UDP.
- C: ALB doesn’t support UDP; CloudFront is not for arbitrary TCP/UDP.
- D: ALB doesn’t support UDP; CloudFront is not for arbitrary TCP/UDP.


---

---

### Question #462

A company has an application that processes customer orders. The company hosts the application on an Amazon EC2 instance that saves the orders to an Amazon Aurora database. Occasionally when trac is high the workload does not process orders fast enough. What should a solutions architect do to write the orders reliably to the database as quickly as possible?

- A. Increase the instance size of the EC2 instance when trac is high. Write orders to Amazon Simple Notification Service (Amazon SNS). Subscribe the database endpoint to the SNS topic.

- B. Write orders to an Amazon Simple Queue Service (Amazon SQS) queue. Use EC2 instances in an Auto Scaling group behind an Application Load Balancer to read from the SQS queue and process orders into the database.

- C. Write orders to Amazon Simple Notification Service (Amazon SNS). Subscribe the database endpoint to the SNS topic. Use EC2 instances in an Auto Scaling group behind an Application Load Balancer to read from the SNS topic.

- D. Write orders to an Amazon Simple Queue Service (Amazon SQS) queue when the EC2 instance reaches CPU threshold limits. Use scheduled scaling of EC2 instances in an Auto Scaling group behind an Application Load Balancer to read from the SQS queue and process orders into the database.

**Correct:** B
**Why:** Buffer orders to SQS and scale consumers behind an ALB/ASG to write quickly and reliably to Aurora.

**Incorrect:**
- A: SNS lacks durable queue semantics; scaling only on CPU misses spikes; D is reactive.
- C: SNS lacks durable queue semantics; scaling only on CPU misses spikes; D is reactive.
- D: SNS lacks durable queue semantics; scaling only on CPU misses spikes; D is reactive.


---

---

### Question #466

A company designed a stateless two-tier application that uses Amazon EC2 in a single Availability Zone and an Amazon RDS Multi-AZ DB instance. New company management wants to ensure the application is highly available. What should a solutions architect do to meet this requirement?

- A. Configure the application to use Multi-AZ EC2 Auto Scaling and create an Application Load Balancer

- B. Configure the application to take snapshots of the EC2 instances and send them to a different AWS Region

- C. Configure the application to use Amazon Route 53 latency-based routing to feed requests to the application

- D. Configure Amazon Route 53 rules to handle incoming requests and create a Multi-AZ Application Load Balancer

**Correct:** A
**Why:** Make the stateless tier highly available with Multi‑AZ Auto Scaling and an ALB; DB is already Multi‑AZ.

**Incorrect:**
- B: Snapshots/Route 53 alone do not provide app‑tier HA.
- C: Snapshots/Route 53 alone do not provide app‑tier HA.
- D: Snapshots/Route 53 alone do not provide app‑tier HA.


---

---

### Question #473

A company hosts a website on Amazon EC2 instances behind an Application Load Balancer (ALB). The website serves static content. Website trac is increasing, and the company is concerned about a potential increase in cost.

- A. Create an Amazon CloudFront distribution to cache state files at edge locations

- B. Create an Amazon ElastiCache cluster. Connect the ALB to the ElastiCache cluster to serve cached files

- C. Create an AWS WAF web ACL and associate it with the ALB. Add a rule to the web ACL to cache static files

- D. Create a second ALB in an alternative AWS Region. Route user trac to the closest Region to minimize data transfer costs

**Correct:** A
**Why:** CloudFront caches static files at edge locations, reducing ALB/EC2 load and cost.

**Incorrect:**
- B: ALB cannot connect directly to ElastiCache to serve files.
- C: WAF is for filtering, not caching.
- D: Adding a second ALB/Region doesn’t address caching/cost.


---

---

### Question #479

A company is making a prototype of the infrastructure for its new website by manually provisioning the necessary infrastructure. This infrastructure includes an Auto Scaling group, an Application Load Balancer and an Amazon RDS database. After the configuration has been thoroughly validated, the company wants the capability to immediately deploy the infrastructure for development and production use in two Availability Zones in an automated fashion. What should a solutions architect recommend to meet these requirements?

- A. Use AWS Systems Manager to replicate and provision the prototype infrastructure in two Availability Zones

- B. Dene the infrastructure as a template by using the prototype infrastructure as a guide. Deploy the infrastructure with AWS CloudFormation.

- C. Use AWS Cong to record the inventory of resources that are used in the prototype infrastructure. Use AWS Cong to deploy the prototype infrastructure into two Availability Zones.

- D. Use AWS Elastic Beanstalk and configure it to use an automated reference to the prototype infrastructure to automatically deploy new environments in two Availability Zones.

**Correct:** B
**Why:** Define the infra in CloudFormation templates and deploy to multiple AZs automatically.

**Incorrect:**
- A: Systems Manager/Config/Elastic Beanstalk are not ideal for full infra replication here.
- C: Systems Manager/Config/Elastic Beanstalk are not ideal for full infra replication here.
- D: Systems Manager/Config/Elastic Beanstalk are not ideal for full infra replication here.


---

## General / Architecture

### Question #387

A new employee has joined a company as a deployment engineer. The deployment engineer will be using AWS CloudFormation templates to create multiple AWS resources. A solutions architect wants the deployment engineer to perform job activities while following the principle of least privilege. Which combination of actions should the solutions architect take to accomplish this goal? (Choose two.)

- A. Have the deployment engineer use AWS account root user credentials for performing AWS CloudFormation stack operations.

- B. Create a new IAM user for the deployment engineer and add the IAM user to a group that has the PowerUsers IAM policy attached.

- C. Create a new IAM user for the deployment engineer and add the IAM user to a group that has the AdministratorAccess IAM policy attached.

- D. Create a new IAM user for the deployment engineer and add the IAM user to a group that has an IAM policy that allows AWS CloudFormation actions only.

E. Create an IAM role for the deployment engineer to explicitly dene the permissions specic to the AWS CloudFormation stack and launch stacks using that IAM role.

**Correct:** D, E
**Why:** Give the engineer only CloudFormation permissions and use an IAM role with explicit permissions for stacks—principle of least privilege.

**Incorrect:**
- A: Never use the root user.
- B: PowerUsers/Administrators exceed least privilege.
- C: PowerUsers/Administrators exceed least privilege.


---

---

### Question #476

A company is expecting rapid growth in the near future. A solutions architect needs to configure existing users and grant permissions to new users on AWS. The solutions architect has decided to create IAM groups. The solutions architect will add the new users to IAM groups based on department. Which additional action is the MOST secure way to grant permissions to the new users?

- A. Apply service control policies (SCPs) to manage access permissions

- B. Create IAM roles that have least privilege permission. Attach the roles to the IAM groups

- C. Create an IAM policy that grants least privilege permission. Attach the policy to the IAM groups

- D. Create IAM roles. Associate the roles with a permissions boundary that denes the maximum permissions

**Correct:** C
**Why:** Create least‑privilege IAM policies and attach them to IAM groups; add users to groups.

**Incorrect:**
- A: SCPs govern accounts, not per‑user permissions.
- B: Roles/permission boundaries are not needed for basic group‑based access.
- D: Roles/permission boundaries are not needed for basic group‑based access.


---
